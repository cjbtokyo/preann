#+TITLE:       
#+AUTHOR:      
#+EMAIL:       
#+KEYWORDS:    Redes neuronales, algoritmos genéticos, redes neuronales evolutivas, neuro-evolución, aprendizaje por refuerzo, SSE2, GPGPU, CUDA.
#+LANGUAGE:    es
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[margin=2.5cm,includefoot]{geometry}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{pict2e}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{chngcntr}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{import}
#+LATEX_HEADER: \hypersetup{colorlinks,citecolor=green,filecolor=black,linkcolor=blue,urlcolor=blue}
#+OPTIONS:     toc:nil H:5
#+BIND: org-export-latex-title-command ""

#+TODO: HACER MODIFICAR BORRAR | REVISAR

# definiciones propias
#+begin_latex

\setcounter{secnumdepth}{5}
\counterwithin{figure}{section}
\setcounter{tocdepth}{5}

\newcommand{\murl}[2]{\url{#1://#2}}

\newcommand{\mail}[1][jtimonmv@gmail.com]{%
     \href{mailto:#1} {#1}
}

\newcommand{\definicion}[1]{%
	\textbullet \bfseries{ #1 :}
}

\newenvironment{listaDefiniciones}%
%ordenes al inicio
{
\begin{list}{}%
     {  \setlength{\itemsep}{0.5ex}
	\setlength{\parsep}{0.5ex}
	\setlength{\partopsep}{0.5ex}
	\setlength{\topsep}{\dimexpr 2\itemsep}
	\setlength{\listparindent}{\dimexpr \parindent}
	\renewcommand*{\makelabel}[1]{\definicion{##1}}
	}
}
%ordenes al final
{
\end{list}
}%

#+end_latex

# Título, abstract e índice
#+begin_latex

\begin{titlepage}

\title{Aprendizaje por refuerzo mediante algoritmos genéticos de redes neuronales paralelizadas con GPGPU}

\author{
\\\\\\\\\\\\
Autor:\\\\
Jorge Timón Morillo-Velarde\\\\
\mail\\\\
\\\\\\\\\\\\
Tutores del proyecto:\\\\ 
\\
Rosa M. Pérez Utrero\\\\
\mail[rosapere@unex.es]\\\\
\\\\
Juan A. Gómez Pulido\\\\
\mail[jangomez@unex.es]\\\\
\\\\\\\\\\\\
}

\end{titlepage}

\maketitle

\newpage
\begin{abstract}

En este trabajo se estudia un método alternativo para el entrenamiento de redes neuronales. Se utiliza un algoritmo genético para ajustar los pesos de la red neuronal. Se evalúa el uso de diferentes tipos de neuronas (con salida real o binaria) para comparar sus rendimientos utilizando diferentes implementaciones paralelas (para el coprocesador XMM y para la arquitectura CUDA). Se prueban variaciones de los operadores genéticos y se mide su efectividad en el entrenamiento. Se enfrenta el algoritmo a diferentes tipos de problemas de aprendizaje por refuerzo y se reflexiona sobre la idoneidad del mismo para cada problema.
\\\\

\textbf{Palabras clave:} Redes neuronales, algoritmos genéticos, redes neuronales evolutivas, neuro-evolución, aprendizaje por refuerzo, SSE2, GPGPU, CUDA.

\end{abstract}

\newpage

\setcounter{tocdepth}{2}
\tableofcontents

\newpage
#+end_latex

* Correo Rosa 

- Más que no se conozca es que no se usa directamente
  He adaptado el texto para distinguir mejor entre el no supervisado en general y el por refuerzo. Pero en el aprendizaje por refuerzo no tiene por qué conocerse exactamente la salida deseada. Por ejemplo, yo no se a priori cómo de bueno es cada tablero para cada jugada posible (en el caso de Reversi).

- ¿Es estrictamente necesario que aparezca código en el diseño?
  No, al final lo he movido todo al manual del programador. En esta sección se verán las clases y métodos, pero sólo a través de diagramas y nunca viendo su implementación interna (eso último sólo se hace para la paralelización CUDA y SSE2).

- [ ] se nota corta pega de varios sitios distintos. Se podría añadir un parrafo al principio en el que se indique lo que viene después. O mejor hilar unos parrafos con otros. Yo creo que deberías organizar un poco esta sección tan pronto hablasTan pronto hablas 
	  Sí, hablaba primero de redes, luego genéticos luego otra vez redes...Lo he modificado un poco para intentar arreglarlo.

* REVISAR Introducción
#+LaTeX: \label{intro}

El /aprendizaje automático/ es la rama de la inteligencia artificial que trata de construir sistemas informáticos que optimicen un criterio de rendimiento utilizando datos o experiencia previa. Dentro del aprendizaje automático, las técnicas se clasifican en función de su entrenamiento, éste puede ser supervisado o no supervisado. En el primero, al sistema se le suministran ejemplos de entradas con sus correspondientes salidas deseadas. En el entrenamiento no supervisado, no se tienen a priori ejemplos de cómo debería comportarse el sistema (en aglunos casos si se conocen, pero no se usan directamente). El aprendizaje por refuerzo es un caso especial de aprendizaje supervisado en que la salida deseada exacta es desconocida. Se basa sólo en información sobre si la salida del agente en cada acción es correcta o no (o lo correctas que son en conjunto ua serie de acciones). Al agente que se quiere que aprenda se le provee información sobre lo bien o lo mal que está actuando. Esta señal de recompensa puede serle indicada bien cada vez que el agente actúa, bien al final de una prueba completa durante la que realiza varias acciones.

Los algoritmos que tienen su origen en la observación de la naturaleza se denominan bio-inspirados. Entre ellos se encuentran las redes neuronales y los algoritmos genéticos. Tras introducir a las dos técnicas en este orden, se hablará de cómo combinarlos, se explacará la motivación para hacer una implementación paralela de las redes neuronales, se hablará de los problemas que puede resolver y, finalmente, se decribe brevemente la organización del documento.

Las redes neuronales son modelos que intentan reproducir ciertas características de los sistemas neuronales biológicos \cite[Hilera y Martínez, 1995]{Hilera95}. Una red neuronal consiste en un conjunto de elementos de procesamiento, llamados neuronas, los cuales se conectan entre sí \cite[Koehn, 1994]{Koehn94}. Las conexiones entre las neuronas tienen pesos asociados cuyos valores determinarán el comportamiento de la red. Existen algoritmos para determinar el valor de los pesos de una red mediante un entrenamiento supervisado, cabe destacar el de retro-propagación del error.

Dada una topología de red fija, el entrenamiento de una red neuronal puede ser visto como un proceso de optimización cuyo objetivo es encontrar un conjunto de pesos que minimice el error que produce la red sobre el conjunto de ejemplos en el entrenamiento supervisado, o que maximice la recompensa en el aprendizaje por refuerzo.

Los algoritmos evolutivos, dentro de los cuales los algoritmos genéticos son los más conocidos, son una familia de modelos computacionales inspirados en la evolución y la supervivencia del más apto \cite[B\"ach, et. al.]{BackSchwefel93}. Se utilizan fundamentalmente en la resolución de problemas de búsqueda y de optimización \cite[Holland, 1975]{Holland75}. Buscan una solución del problema reproduciendo genéticamente una población de individuos a lo largo de una serie de generaciones \cite[Koza, 1992]{Koza92}. El aprendizaje es formulado como un problema de optimización, en el que cada individuo de la población es una posible solución.

En nuestro caso, el individuo o agente utilizará una red neuronal para decidir sus acciones (salida de la red) a partir de los datos que pueda recoger de su entorno (entrada a la red). Se utiliza un algoritmo genético para decidir los pesos adecuados para la red, utilizando una población de agentes con redes diferentes. Se utiliza la recompensa acumulada por el individuo en una o varias pruebas para medir su adaptación al medio. Se utilizan estas medidas y la población actual para construir la siguiente generación. Cuando algún individuo demuestra estar lo suficientemente adaptado al medio para cumplir con las expectativas del entrenamiento (o cuando se supera un límite prefijado de iteraciones), éste finaliza.

En todo este proceso, El algoritmo que más se ejecuta es el que calcula la salida de la red a partir de la entrada. Este algoritmo se puede llamar varias veces por cada prueba. Además, las pruebas pueden repetirse varias veces por individuo para disminuir el ruido producido por los factores aleatorios que pueda haber en las pruebas y, por último, esto debe multiplicarse por el número de individuos que se evaluen por generación. Por ello, esta función se ha optimizado mediante su paralelización en dos arquitecturas ampliamente extendidas y económicas: el coprocesador XMM, presente en todos los PCs de reciente fabricación y la arquitectura CUDA [fn:cudaGPGPU], compatible con la mayoría de tarjetas gráficas NVIDIA a partir de la serie 8000. Además, para incrementar aún más el rendimiento, se estudia la viabilidad de entrenar redes con una versión mínima de neurona con activación de tipo escalón y con pesos con tamaño de un byte (en la sección \ref{disenoParal} se describe con más detalle). Llamaremos a las redes que utilizan estas estructuras redes discretas.

Los problemas en los que aplicamos nuestro desarrollo, aunque no son realistas, se consideran interesantes para la robótica y/o para la inteligencia artificial. Utilizamos la librería implementada como parte del proyecto para que aprenda a resolver problemas de clasificación y a jugar a un juego de estrategia.
Otros problemas para los que se podría utilizar la libraría podrían ser el control automático, la toma de decisiones en tiempo real y la colaboración de múltiples agentes en tiempo real. Este último tipo de problema puede estar relacionado con la [[http://es.wikipedia.org/wiki/Vida_artificial][vida artificial]]. En la mayoría de los problemas, el agente se enfrenta a pruebas genéricas que pueden tener factores aleatorios y/o agentes directamente programados. En algunos problemas, sin embargo, los agentes de la población pueden enfrentarse entre ellos para obtener una valoración.

En el presente documento (que contiene la documentación asociada al proyecto fin de carrera titulado /<<Aprendizaje por refuerzo mediante algoritmos genéticos de redes neuronales paralelizadas con GPGPU>>/, desarrollado por Jorge Timón Morillo-Velarde para la consecución del título de ingeniería informática), primero comentaremos los fundamentos teóricos precisos para su comprensión y su correcta ubicación dentro del dominio de la neuro-evolución, diferenciándolo de otros desarrollos en éste área. Después, justificaremos las principales decisiones tomadas durante el desarrollo y explicaremos en detalle algunas partes de su implementación. Los rendimientos obtenidos con las diferentes implementaciones paralelas y opciones en el algortimo genético se muestran en el capítulo \ref{rendimiento}. En el capítulo \ref{experimentacion}, se describen los experimentos realizados y se justifica la elección de los mismos. El capítulo de resultados de aprendizaje \ref{aprendizaje} expone los datos empíricos recogidos en los experimentos y razona unas someras conclusiones que luego se completan en el capítulo \ref{conclusiones}.

\newpage
* MODIFICAR [0/1] Base teórica
#+LaTeX: \label{baseTeorica}

Tanto las redes neuronales como los algoritmos genéticos están inspirados en la naturaleza y han sido utilizados desde largo tiempo atrás. Las redes neuronales están inspiradas en el funcionamiento del cerebro, como un sistema de procesamiento de información distribuido. Por su parte, los algoritmos genéticos se basan en la teoría de la evolución de Darwin, por la que la evolución se produce a través de dos principios básicos: los individuos que no se adaptan suficientemente al medio perecen, mientras que los que sí lo hacen transmiten sus genes con cierta variabilidad. Esta variabilidad puede proceder de dos fuentes: el cruce de genes entre individuos ó la mutación directa de esos genes.

La combinación de estas dos técnicas es algo relativamente reciente. Algunos autores - principalmente anglosajones - han coincidido en llamar a esta síntesis [[http://en.wikipedia.org/wiki/Neuroevolution][neuro-evolución]], otros se refieren a ella como [[http://laral.istc.cnr.it/nolfi/papers/HBTNN-A.pdf][evolución de redes neuronales artificiales]], GANN, etc. En general, la mayoría de los que la usan recurren a nombres que definen con más precisión la técnica concreta que utilizan; dadas las múltiples posibilidades para combinar ambos métodos. Aceptaremos el término neuro-evolución para el resto del texto, aunque sin renunciar a los otros términos que hayan podido ser utilizados como redes neuronales evolutivas.

A continuación explicaremos más detalladamente las bases teóricas de las tres técnicas: redes neuronales, algoritmos genéticos y neuro-evolución. Nos centraremos principalmente en los algoritmos y estructuras que más se asemejan a los implementamos en nuestra librería.

\newpage
** MODIFICAR [0/2] Redes neuronales
#+LaTeX: \label{basTeoRedes}

Describiremos de forma suscinta el funcionamiento general de las redes neuronales y sus estructuras más comunes, para luego discutir las ventajas y las posibles limitaciones, centrándonos en los algoritmos de aprendizaje de gradiente descendente y, en concreto, en el de retropropagación del error.

*** MODIFICAR Conceptos generales

Las redes neuronales constan de un conjunto de elementos de procesamiento - conocidos como nodos o neuronas - interconectados entre sí. Pueden ser descritas mediante un grafo dirigido en el que cada neurona  \(i\) usa una función de activación de la forma:

\begin{equation}\label{eqSalidaNeu}
  y_i=f_i(\sum_{j=1}^n (w_{ij} \cdot x_j - \theta_i)).
\end{equation}

donde \(y_i\) es la salida de la neurona \(i\), \(x_j\) es la entrada número \(j\) a la misma, \(w_{ij}\) es el peso de la conexión entre los nodos \(i\) y \(j\), \(\theta_i\) es el umbral de activación (o Bias) y \(f_i\) es una función que puede ser continua o discreta.

#+CAPTION:    Red neuronal \emph{feed-forward}.
#+LABEL:      figFeedForward
#+ATTR_LaTeX: trim= 0.5cm 22cm 10cm 0cm, clip, width=15cm
[[./img/feed-forward.jpg]]

TODO NOTAS rosa 1: La conectividad entre los nodos de una red neuronal está relacionada con la forma en que las salidas de las neuronas están canalizadas para convertirse en entradas de otras neuronas. La señal de salida de un nodo puede ser una entrada de otro elemento de proceso, o incluso ser una entrada de su propia salida (conexión autorrecurrente).

Cuando ninguna salida de la neurona es entrada de neuronas del mismo nivel o de niveles precedente, la red se describe como de propagación hacia adelante. Cuando las salidas pueden ser conectadas como entradas de neuronas de niveles previos o del mismo nivel, incluyéndose ellas mismas, la red es de propagación hacia atrás. 
--fin notas

[nota1 Las redes neuronales artificiales pueden clasificarse como /feed-forward/ (con propagación hacia delante) o recurrentes dependiendo de su conectividad. Una red es /feed-forward/ (figura \ref{figFeedForward}) si existe un método de numeración de las neuronas que cumpla que no existan conexiones desde un nodo hacia otro nodo con un número más pequeño que el de nodo de origen.] Una red es recurrente (figura \ref{figRecurrente}) si no existe un método de numeración que cumpla tal condición. En el presente trabajo, nos centraremos en las redes /feed-forward/, aunque la librería implementada permite conexiones recurrentes.

TODO nota rosa 2:
Aprendizaje se puede definir como: La modificación del comportamiento inducido por la interacción con el entorno y como resultado de experiencias conducentes al establecimiento de nuevos modelos de respuestas ante estímulos externos.
Biológicamente, se suele aceptar que la información memorizada en el cerebro está más relacionada con los valores sinápticos de las conexiones entre las neuronas que con ellas mismas; es decir, el conocimiento se encuentra en las sinapsis. En el caso de las redes neuronales artificiales, se puede considerar que el conocimiento se encuentra representado en los pesos de las conexiones entre las neuronas. Todo proceso de aprendizaje implica cierto número de cambios en estas conexiones. En realidad se puede decir que se aprende modificando los pesos de la red.
--fin notas

[nota2 El aprendizaje de las redes neuronales se consigue habitualmente usando ejemplos: suelen tener un entrenamiento supervisado. Se basa en la comparación directa entre la salida de la red y la salida correcta o deseada.] Normalmente se formula el entrenamiento como la minimización de una función de error como el sumatorio del cuadrado del error de la salida respecto de la salida deseada para todos los datos disponibles (que constan de pares de entradas con sus correspondientes salidas deseadas). [nota 3: Un algoritmo de optimización basado en el descenso del gradiente como la regla delta generalizada (también conocido como algoritmo /backpropagation/) puede ser usado después iterativamente para ajustar los pesos y así minimizar el error.]

[TODO nota rosa 3
El aprendizaje supervisado se caracteriza porque el proceso de aprendizaje se realiza mediante un metódico entrenamiento controlado por un agente externo, conocido como supervisor, que determina la respuesta que debería generar la red a partir de una entrada determinada. El supervisor comprueba la salida de la red y si ésta no coincide con lo que se quiere, se procede a modificar los pesos de las conexiones para conseguir que la salida obtenida se aproxime a la deseada.

 Dentro de las redes con aprendizaje supervisado, el modelo BPN (Backpropagation) utiliza el aprendizaje por corrección del error. Una característica de este algoritmo es la representación interna del conocimiento que es capaz de organizar en la capa intermedia de las células para conseguir cualquier correspondencia entre la entrada y la salida de la red. Este método de aprendizaje está basado en la generalización de la regla delta,]

#+CAPTION:    Red neuronal recurrente.
#+LABEL:      figRecurrente
#+ATTR_LaTeX: scale=0.35
[[./img/recurrente.jpg]]

[NOTa rosa 4 No entiendo porque hablas del algoritmo BP y luego dices que no lo utilizas. Además hay una contradicción antes dices que para simplificar el trabajo van a ser feedforward y ahora dices que recurrentes, Por favor revisa bien este párrafo
Si estás hablando de conceptos generales no queda claro porqué defines unos y otros no]

El algoritmo /backpropagation/ no funciona con [nota4 rosa]topologías recurrentes. En nuestro caso, no necesitamos una conectividad hacia delante, la librería implementada como parte del proyecto sí puede utilizar conexiones recurrentes como se especifica en el capítulo de [[analisis][análisis]]. Además, utilizamos aprendizaje por refuerzo y no necesitamos una colección de ejemplos (aunque se puede utilizar el error total en un conjunto de ejemplos para calcular el refuerzo). Los pesos los ajustará un algoritmo genético. La estructura de la red se definirá de forma previa y manualmente para cada problema y sólo evolucionarán los pesos (y umbrales).

[nota rosa 5: 
A que te refieres a las redes neuronales en general o a la BP?????
En mi opinión no te debes meter en complicaciones en el tecto, está bien que tu conozcas los requisitos específicos pero yo no los pondría en el texto, porque te pueden hacer preguntas y ponerte en un serio compromiso.]

*** MODIFICAR [nota rosa 5: Fortalezas y deficiencias]
#+LaTeX: \label{baseTeoricaFortRedes}

[Nota rosa 6
Ojo que en la literatura se suele cuando se habla de perceptron multicapa se considera algoritmo BP. De todas formas sigo sin entender si vas a utilizar un aprendizaje por refuerzo por que insistes en BP.]

Las redes neuronales con conexión hacia delante en general son un importante método de aproximación de funciones \cite[Kim, 1992]{Kim92}. El perceptrón multicapa es un tipo de red neuronal con conexiones hacia delante. [nota rosa 6La topología de un perceptrón multicapa esta definida por un conjunto de capas ocultas, una capa de entrada y una de salida. No existen restricciones sobre la función de activación aunque en general se suelen utilizar funciones sigmoideas. Existen demostraciones teóricas \cite[Funahashi, 1989]{Funahashi89} de que un perceptrón multicapa cuya función de activación sea no constante, acotada y monótona creciente es un aproximador universal de funciones. En \cite[Hornik et alt, 1989]{Hornik89} se llega a un resultado similar utilizando funciones de activación sigmoideas, no necesariamente continuas. Esto es un punto muy fuerte de las redes neuronales. ]

Además, constituyen buena una herramienta para la construcción de agentes pues sólo hay que codificar las entradas y las salidas de la red como las del agente y el tiempo de ejecución de la red sólo depende de la topología de ésta (para una topología dada, es lineal con respecto al número de entradas consecutivas).

Algunas deficiencias del algoritmo back-propagation son su baja adaptabilidad, la alta dependencia de los parámetros del algoritmo, el estancamiento en mínimos locales, la posibilidad de parálisis y la alta dependencia de las condiciones iniciales.

+ *Adaptabilidad:* 
  El algoritmo tiene como premisa la utilización de una función de activación derivable \cite[Walker, 1995]{Walker95}. Al hacer uso de la derivada de la función de activación, es condición necesaria para la aplicación del algoritmo que la misma sea continua y derivable en todo el dominio de aplicación \cite[Wilson, 1994]{Wilson94}. Esto impide la utilización del método en otras topologías donde la función de activación presenta discontinuidades.

  Este problema suele encontrarse en varios métodos de entrenamiento, los cuales son desarrollados para una determinada topología y sus resultados, en general, no son extensibles directamente a otras topologías. Es necesario adaptar los métodos para aplicarlos a otras topologías.

+ *Mínimos locales:* 
  La superficie que define la función de error E en base a los parámetros de la red neuronal es compleja y esta llena de valles y colinas. Debido a la utilización del gradiente para encontrar el mínimo de dicha función de error se corre el riesgo de que el proceso de entrenamiento quede atrapado en un mínimo local \cite[Sutton, 1986]{Sutton86}. Esta situación no es deseable, fundamentalmente si dicho mínimo esta localizado lejos del mínimo global.

  Existen algunos mecanismos para evitar que esto suceda. Una posible solución para evitar que el entrenamiento quede atrapado en un mínimo local es aumentar el número de neuronas ocultas de la red. Este mecanismo puede ayudar en aquellos casos en los que la red tiene escaso poder de representación interna, y no es capaz de distinguir entre dos patrones diferentes, proporcionando una misma salida para ambos patrones. Al aumentar el número de neuronas ocultas la red posee mayor cantidad de parámetros libres y puede conseguir una mejor representación interna.

  Otros mecanismos que ayudan a disminuir los efectos de este problema son la adición de una tasa de momento al proceso de entrenamiento, utilizar una tasa de aprendizaje decreciente a lo largo del proceso, partir de otras configuraciones iniciales de la red, añadir ruido al método de gradiente, etc.

+ *Saturación:* 
  El fenómeno de saturación, también conocido como parálisis, se produce cuando la entrada total a una neurona de la red toma valores muy altos, ya sean positivos o negativos. Al utilizar funciones de activación sigmoidales, la función de activación posee dos asíntotas horizontales. Si la entrada de la neurona alcanza un valor alto, la función de activación se satura y alcanza un valor de activación máximo o mínimo.

  Cuando la función de activación se satura su derivada tiende a hacerse nula, haciendo que los parámetros de la red permanezcan invariables y, como consecuencia, la suma de los errores locales permanece constante por un largo periodo de tiempo \cite[Kröse y van der Smagt, 1993]{Krose93}. Aunque esta situación se suele confundir con un mínimo local, pues el error permanece invariable, en este caso es posible que después de un cierto tiempo el error comience nuevamente a decrecer.

  El fenómeno de parálisis del perceptrón multicapa ocurre fundamentalmente cuando los parámetros de la red toman valores muy altos. Un mecanismo para evitar esto consiste en partir de valores iniciales bajos.

+ *Condiciones iniciales:* 
  El conjunto de pesos iniciales de la red neuronal generalmente se selecciona de manera aleatoria. Sin embargo, el algoritmo /backpropagation/ es muy dependiente de las condiciones iniciales seleccionadas \cite[Kolen, 1991]{Kolen91}. Pequeñas variaciones realizadas sobre las condiciones iniciales pueden llevar a grandes diferencias en el tiempo de convergencia del algoritmo.

+ *Dependencia de parámetros del algoritmo:* 
  Los algoritmos de gradiente descendente hacen uso de una tasa de aprendizaje que idealmente debería ser infinitesimal. De esta manera, mediante pequeños ajustes de los pesos sinápticos el algoritmo converge hacia un mínimo. El uso de tasas de aprendizaje muy pequeñas hace que el algoritmo tenga una convergencia estable hacia un mínimo, aunque el tiempo necesario para alcanzarlo puede llegar a ser muy alto. Como consecuencia de lo dicho anteriormente, y con el objetivo de disminuir el tiempo de convergencia del algoritmo, en la práctica se suelen utilizar tasas de aprendizajes mayores a las teóricas. El aumento de la tasa de aprendizaje disminuye el tiempo de convergencia, pero tiene un efecto contraproducente: el algoritmo comienza a oscilar en torno a un mínimo, disminuyendo la probabilidad de alcanzarlo. El efecto de oscilación puede reducirse mediante la adición de una tasa de momento, pero no puede eliminarse.

  El algoritmo /backpropagation/ es muy dependiente de los parámetros mencionados previamente. Dependiendo de la selección de parámetros realizadas el resultado de la aplicación del algoritmo será exitosa o no \cite[Liu et alt, 2004]{Liu2004}. Pequeñas variaciones sobre los parámetros del algoritmo pueden conducir a resultados diferentes. El principal problema es que no existe un método general que permita establecer el valor de estos parámetros \cite[Branke, 1995]{Branke95}. Los parámetros que aseguran la convergencia para un determinado problema pueden no ser aplicables a otro problema. De esta manera, la selección de los parámetros del algoritmo se realiza en base a la experiencia del diseñador, y se realiza un refinamiento de los mismos mediante mecanismos de prueba y error. Esto produce un aumento en el tiempo total de diseño y entrenamiento de la red.

A esto hay que añadir que los algoritmos de gradiente requieren entrenamiento supervisado (normalmente, no funcionan para el aprendizaje por refuerzo, que es más general) y que las conexiones sean hacia delante (la retro-propagación del error no se puede aplicar en redes recurrentes). 

Usando un [[basTeoGenet][algoritmo genético]] como método de entrenamiento de la red, se solucionan algunos de estos problemas y otros se mitigan en cierto grado:

- Con el algoritmo genético, se puede usar el aprendizaje por refuerzo y se pueden entrenar redes recurrentes sin problema alguno.
- No se tienen requerimientos para la función de activación que como hemos visto tenía que ser continua y derivable para /backpropagation/, por lo que aumenta su adaptabilidad. 
- El algoritmo genético es mucho menos tendente a estancarse en mínimos locales porque no utiliza la información del gradiente y porque explora varios puntos (tantos como individuos tenga la población) del espacio de búsqueda simultáneamente.
- El fenómeno de saturación se produce cuando una neurona alcanza un máximo o un mínimo. En este caso, la derivada de la función de activación se hace nula, y los pesos de la red permanecen invariables. Como el método propuesto no hace uso de la derivada de la función de activación, el efecto de este fenómeno es completamente eliminado. 
- Los valores iniciales de los pesos también pueden afectar al algoritmo genético, en especial si son muy altos (ya sean positivos o negativos), pero existen experimentos que permiten afirmar que el método propuesto es menos dependiente de los valores iniciales que el algoritmo /backpropagation/ \cite[Bertona2005]{Bertona2005}.
- Se cambia la dependencia de los parámetros de ese algoritmo y ahora depende de los parámetros del algoritmo genético, estos parámetros son más flexibles que se pueden alterar en durante el entrenamiento y no provocan la oscilación comentada anteriormente.

\newpage
** Algoritmos genéticos
#+LaTeX: \label{basTeoGenet}

Los algoritmos genéticos son métodos sistemáticos para la resolución de problemas de búsqueda y optimización que aplican a éstos los principios de la evolución biológica: selección basada en la población, reproducción sexual y mutación.

Los algoritmos genéticos son métodos de optimización, que tratan de resolver el conjunto de problemas formulados como: hallar (x_1,..., x_n) tales que F(x_1,..., x_n) sea máximo. En un algoritmo genético, tras parametrizar el problema en una serie de variables (x_1,..., x_n), se codifican en un cromosoma. Todos los operadores utilizados por un algoritmo genético se aplicarán sobre estos cromosomas, o sobre poblaciones de ellos. En el algoritmo genético va implícito el método para resolver el problema; son sólo parámetros de tal método los que están codificados - a diferencia de otros algoritmos evolutivos como la programación genética. Hay que tener en cuenta que un algoritmo genético es independiente del problema, lo cual lo hace un algoritmo robusto, por ser útil para cualquier problema, pero a la vez débil, pues no está especializado en ninguno.

Las soluciones codificadas en un cromosoma compiten para ver cuál constituye la mejor solución (aunque no necesariamente la mejor de todas las soluciones posibles). El ambiente, constituido por las otras camaradas soluciones, ejercerá una presión selectiva sobre la población, de forma que sólo los mejor adaptados (aquellos que resuelvan mejor el problema) sobrevivan o leguen su material genético a las siguientes generaciones, igual que en la evolución de las especies. La diversidad genética se introduce mediante mutaciones y reproducción sexual. En la Naturaleza lo único que hay que optimizar es la supervivencia, y eso significa a su vez maximizar diversos factores y minimizar otros. Un algoritmo genético, sin embargo, se usará habitualmente para optimizar sólo una función, no diversas funciones relacionadas entre sí simultáneamente. Este tipo de optimización, denominada optimización multimodal, también se suele abordar con un algoritmo genético especializado.

Por lo tanto, un algoritmo genético consiste en lo siguiente: hallar de qué parámetros depende el problema, codificarlos en un cromosoma, y se aplican los métodos de la evolución: selección y reproducción sexual con intercambio de información y alteraciones que generan diversidad. En el capítulo \ref{disenoGene} se describen en más detalle los operadores genéticos por separado.

Mediante los operadores de [[disenoGeneSel][selección]], se eligen los individuos que serán progenitores de la siguiente generación (o directamente formarán parte de ella). Con los operadores de [[disenoGeneCruz][cruce]], se generan nuevos individuos mezclando los cromosomas de varios individuos (normalmente, dos). Por último, los operadores de [[disenoGeneMut][mutación]] añaden cambios aleatorios a los individuos. La función de fitness nos da una aproximación de la adaptación del individuo al medio y es utilizada por los operadores de selección.

En nuestro caso, el cromosoma de cada individuo lo forman los pesos de la red que utiliza ese individuo. Para calcular el fitness del individuo, se construirá la red con los pesos del cromosoma y se realizarán varias pruebas (para reducir el ruido generado por los posibles factores aleatorios de éstas) sobre el individuo, sumando las recompensas de todas y obteniendo el citado fitness.

*** Algoritmo genético estándar y variaciones
#+LaTeX: \label{basTeoGenetEstan}

Existen muchas variaciones del algoritmo genético original \cite[Holland, 1975]{Holland75}. Los genes no tienen por qué ser bits, también pueden ser números, por ejemplo. El operador de inversión rara vez se usa hoy en día \cite[Mitchell M., 1996]{Mitchell96}. Algunas modifican el operador de cruce o /crossover/ de forma que preduzca cruces entre individuos con fitness similares para obtener búsquedas más locales. Otros directamente implementan nuevos operadores como la recombinación (cambiar la posición de los genes en un mismo individuo). En general, no hay una definición universal de algoritmo genético que especifique los operadores concretos que debe tener.

También existen diferencias en la forma de gestionar los individuos de la población. No sólo en el número de individuos de ésta. Algunos algoritmos compartimentalizan poblaciones separadas denominadas islas, que no pueden cuzarse entre sí o lo hacen de forma más restringida. El algoritmo genético original adoptaba la política de reemplazo generacional, con el que la población completa es reemplazada en cada generación. En cambio, la política de estado estacionario, adoptada por varios algoritmos genéticos posteriores, reemplaza la población selectivamente. Es posible mantener un miembro de la población sin modificarlo por varias generaciones, siempre que estos mantengan un fiteness que esté por encima de otros individuos de la población. Esta es la aproximación de GENITOR \cite[Whitley, 1989]{Whitley89}, que combina el estado estacionario con una selección por [[disenoGeneSelRank][ranking]]. Incluso existen aproximaciones en que los genes bloques de genes que no conforman un indidividuo completo reciben un fitness y compiten como otra población por ser usados por los individuos de la población de individuos \cite[Mitchell A. and De Jong, 2000]{Mitchell2000}.

*** Fortalezas y deficiencias
#+LaTeX: \label{baseTeoricaFortGene}

Un algoritmo genético es independiente del problema, lo cual lo hace un algoritmo robusto, por ser útil para cualquier problema, pero a la vez débil, pues no está especializado en ninguno. Hay que elegir la codificación de los cromosomas para cada caso concreto. Esto puede requerir cierto grado de conocimiento acerca del dominio de aplicación concreto a la hora de definir esta codificación.

Sin embargo, con nuestro método siempre códificaremos los cromosomas de manera similar (con una red neuronal) y sólo será necesario definir la función de fitness usando  las entradas y las salidas de una red nueronal y elegir su topología (una librería suficientemente flexible constituye una herramienta ideal para tratar de automatizar este último proceso). Aunque la codificación de las entradas pueda admitir varias posibilidades (y algunas puedan ser más ventajosas que otras) la red debe aprender a interpretar las correctas relaciones entre entradas y salidas por sí misma. Así, podemos aprovechar el hecho de que las redes neuronales son aproximadores universales de funciones para ahorrarnos el esfuerzo de analizar cada problema por separado y en detalle.

\newpage
** Neuro-evolución
#+LaTeX: \label{basTeoNeuro}

La evolución se ha aplicado las redes neuronales artificiales en tres niveles muy diferentes: a los pesos de las conexiones, la arquitectura de la red y a las reglas de aprendizaje. La evolución de los pesos de las conexiones introduce una aproximación global y adaptable al entrenamiento, especialmente para el aprendizaje por refuerzo o para el entrenamiento de redes recursivas, donde los métodos basados en el gradiente experimentan grandes dificultades. La evolución de las arquitecturas permite a las redes neuronales adaptar su topología a diferentes problemas sin intervención humana y con esto se consigue un diseño automático de redes neuronales, dado que tanto la arquitectura como los pesos pueden ser evolucionados. La evolución de las reglas de aprendizaje puede ser considerada como un proceso de "aprender a aprender" en redes neuronales que luego aprenderan de forma autónoma utilizando esas reglas. Puede ser contemplada como un proceso de descubrimiento automático de nuevas reglas de aprendizaje. 

Nos centraremos en la evolución de los pesos de las conexiones, por ser la evolución que utilizaremos. Pero comentaremos algunas aproximaciones en el campo de la evolución de las topologías, para el que la [[manualProgrApi][librería]] implementada puede ser de utilidad.

La evolución de los pesos de las conexiones se puede realizar en el aprendizaje supervisado (con ejemplos) definiendo la función de fitness como el error global obtenido por la red (invirtiendo el signo), comparando las salidas de la red y la salida deseada para cada ejemplo. También se puede utilizar para el aprendizaje por refuerzo definiendo con una función de fitness (un problema) que no requiera ejemplos.

En general, los pasos a seguir son dos: decidir la codificación de los pesos de las conexiones (si se hará mediante cadenas binarias o no) y la ejecución del algoritmo genético propiamente dicho. Para el primer paso, las opciones más extendidas son la representación binaria y la representación con números reales.

El algoritmo genético canónico siempre usa cadenas de bits para codificar las diferentes soluciones. Por ello, algunos trabajos tempranos de evolución de los pesos de las conexiones siguen esta aproximación \cite[Yao99]{Yao99}. Las ventajas son la fácil aplicación de los operadores genéticos y su posible implementación digital. Habría que elegir la representación de los números reales. Aquí hay un compromiso para la precisión con que se quieran representar los números reales. Si se usan muy pocos bits para representar cada conexión, el entrenamiento puede fallar porque algunas combinaciones de pesos no se pueden aproximar con suficiente precisión por valores discretos. Por otra parte, si se usan demasiados bits, los cromosomas que representen a redes neuronales grandes se volverán demasiado largos y la evolución en proceso resultará muy ineficiente.

\begin{figure}[t]
\begin{minipage}{0.45\textwidth}
    \includegraphics [width=7.20cm]{./img/grafo1.jpg}
  \caption {Red neuronal y su codificación binaria (asumiendo que se usan 4 bits para representar cada número real).}\label{figGrafo1}
\end{minipage}
\begin{minipage}{0.10\textwidth}
\hfill
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \includegraphics [width=7.20cm]{./img/grafo2.jpg}
  \caption{Red equivalente con codificación alternativa.}\label{figGrafo2}
\end{minipage}
\end{figure}

Por su parte, en la representación con números reales, los cromosomas se codifican como vectores de números reales con tantos elementos como conexiones. Los operadores genéticos no se pueden aplicar directamente sobre los bits y han de ser diseñados de nuevo. Esto puede ser una ventaja, pues, por ejemplo, el operador de mutación podría tener una distribución gaussiana (u otra función) en lugar de mutar un bit cualquiera sin tener en cuenta su peso en la construcción del número.

Uno de los problemas a los que se enfrenta la evolución de redes neuronales es el problema de la permutación. Es causado por el mapeado "muchos-a-uno" desde la representación en el cromosoma a la red que es construida. Con dos cromosomas distintos se pueden generar redes equivalentes como se muestra en las figuras \ref{figGrafo1} y \ref{figGrafo2}. Se puede solucionar dando más importancia al operador de mutación que al de cruce (que es el que sufre con este problema) o con otros métodos matemáticos \cite[Gomez, Miikkulainen 2003]{GomezMiikkulainen2003}. La gravedad de la redundancia en algoritmos de optimización es discutible y en ocasiones se introduce deliberadamente \cite[T. Weise, 2009]{Weise2009}.

Sin tener el problema de la redundancia en cuenta Se pueden conseguir mejores resultados al evolucionar pesos de las redes neuronales con algoritmos genéticos, que utilizando /backpropagation/ \cite[D. Montana, L. Davis, 1989]{Montana89}. Aunque otros métodos mejorados de ajuste e pesos (por ejemplo, "quickprop") no son superados por los algoritmos genéticos en tareas de aprendizaje supervisado, los últimos pueden ser muy útiles en tareas en las que /backpropagation/ y similares no pueden ser usados como en tareas de aprendizaje no supervisado, en las que el error de cada unidad de salida no está disponible para el sistema de aprendizaje o en situaciones en las que se tienen refuerzos dispersos \cite[Schaffer et al., 1992]{Schaffer92}. 

Un caso frecuente son las tareas de "neurocontrol", en las que redes neuronales se usan para controlar sistemas complejos como robots navegando en ambientes desconocidos. Algunas de esas redes de control u otras de reconocimiento de escitura continua, por ejemplo, requieren que la red pueda tener un estado interno por medio de la recursiividad en la topología \cite[S. Hochreiter and J. Schmidhuber, 1997]{Schmidhuber97}.

Una aproximación existosa para la evolución de la topología de la red además de los pesos consiste en empezar con una red pequeña e ir aumentando su tamaño cuando el fitness se estanca, añadiendo nuevas conexiones aleatorias en tiempo de entrenamiento \cite[Stanley, Miikkulainen, 2002]{Stanley2002}.

Si las variaciones en el uso de los algoritmos genéticos ya eran abundantes, la diversidad de combinaciones de esta técnica con las redes neuronales es aún mayor. No es el objetivo del presente documento el cubrirlas todas ni clasificarlas y con esta idea general estamos ya preparados para definir los objetivos de nuestro proyecto.

\newpage
* Análisis del problema
#+LaTeX: \label{analisis}

En este capítulo se deciden los objetivos específicos del proyecto. 

Se tratan las especificaciones formales de la librería a implementar. Se especifican los [[experimentacion][problemas]] que el algoritmo debe resolver utilizando la librería, pero no se describen y discuten en esta sección. Se resolverán tareas con entrenamiento [[experimentacionClasif][supervisado]] y [[experimentacionJuegos][por refuerzo]]. 

También se analizan las cuestiones técnicas y teóricas que se pretenden resolver en la sección de [[anaObjetivos][objetivos]]. Estas cuestiones determinarán la naturaleza de las empíricas realizadas, cuyos resultados se muestran por separado en un capitulo para el [[rendimiento]] y otro para el [[aprendizaje]]; y se discuten en las [[conclusiones][conclusiones]].

** Especificaciones de la librería a implementar
#+LaTeX: \label{anaEspecLib}

En el presente proyecto se pretende construir una librería de programación para el lenguaje C++ en un entorno GNU/Linux que permita el entrenamiento de redes neuronales utilizando algoritmos genéticos. La librería tendrá una licencia de software libre.

Se quiere que sea lo más flexible posible en cuanto a la estructura de la red, para poder, en un futuro, determinar la topología también de forma genética. Como los entrenamientos pueden ser costosos en tiempo de ejecución, la librería debe estar paralelizada internamente al menos para la ejecución de redes neuronales. Esta paralelización debe poder aprovecharse por los sistemas más extendidos para que pueda ser utilizada en proyectos que aprovechen la computación voluntaria. El compromiso entre flexibilidad y rendimiento computacional obedece a las siguientes exigencias:

1) El bloque de consrucción básico de redes serán *capas de neuronas* que comparten las mismas conexiones de entrada y las optimizaciones paralelizadas podrán aprovechar la paralelización de los datos a este nivel.

2) Una *capa* puede tomar como entrada cualquier número de capas, estableciendo una conexión con cada una. Deben ser posibles conexiones recurrentes y una *capa* debe incluso poder tomar su propia salida como entrada.

3) Debe existir un enumerado o entidad *tipo de implementación*. No debe existir acoplamiento entre la interfaz externa de la librería y las implementaciones optizadas. Incluir una nueva implementación optimizada debe poder ser suficientemente simple. Se implementarán al menos dos optimizaciones diferentes para garantizar esto. Ambas optimizaciones deben superar en rendimento a la implementación no optimizada de referencia descrita en el punto siguiente. 

4) La librería debe incorporar una implementación no optimizada de referencia con la que comparar el resto de *tipos de implementación* para comprobar la corrección de sus cálculos. También debe incorporar herramientas para que esta comprobación pueda hacerse de forma automática.

5) Se desecha la representación binaria de los genes en favor de usar números. Sin embargo, estos números no tienen por qué ser siempre floats, si no que podrían ser de otros tipos más grandes (double) o incluso números enteros y con rangos más pequeños (short, byte) para aproximaciones más cercanas a la lógica difusa. Aunque no se implementen todos los "/tipos de pesos/", la arquitectura debe ser tal que la librería pueda extenderse fácilmente para incorporar un nuevo tipo. Sin embargo, cada implementación optimizada puede requerir un tratamiento especial para el nuevo tipo o directamente no soportarlo. 

6) Cada *capa* de poder utilizar una función de activación diferente. Se define un *tipo de activación*.

7) Para poder explorar las posibilidades de incremento de rendimiento para funciones de activación concretas, En lugar de definirse un "/tipo de pesos/" para el punto 5, se define un *tipo de neurona*, en el que va implicito el tipo de peso y puede ir o no también implicito el tipo de activación. Es decir, un tipo de neurona en la *capa* de entrada tiene siempre un tipo de peso asociado, pero ese tipo de neuronas puede soportar sólo un *tipo de activación* (o un subbconjunto de todas las definidas a partir del punto anterior).

8) Una *capa* puede conectarse con capas cuyo *tipo de neurona* es diferente al suyo. Incluso debe poder tomar como entrada varias capas con *tipos de neurona* diferentes. Cuando un *tipo de implementación* no soporte conexiones entre un par de *tipos de neurona* concretos, debe lanzarse una excepción en el mismo momento en que se trata de crear dicha conexión, no cuando se vaya a activar.

9) Las *capas* no tienen que ser compatibles con otras capas de diferente *tipo de implementación*, sin embargo, las entradas y salidas de una *red neuronal* completa deben ser accedidas y manipuladas de forma completamente transparente con respecto al *tipo de implementación*.

10) Las diferentes implementaciones de las *capas* deben implementar también la gestión genética de los pesos y umbrales, pero se debe definir una interfaz lo suficientemente genérica para que puedan implementarse distintos esquemas de *cruza y mutación* sin que estos tengan que ser implementados una vez por cada *tipo de implementación*. Por tanto los diferentes tipos de operadores genéticos definidos en el punto 14 no deben ser necesarios desde la implementación de la *capa*.

11) Una /red neuronal/ debe poder incorporar nuevas capas y conexiones una vez creada, incluso en medio de un entrenamiento.

12) La entidad que se ocupa de la evolución es la *población*, que contiene una lista ordenada de *individuos*, que a su vez contienen una *red neuronal* completa.

13) La *población* debe poder ser gestionada de forma generacional o con el estado estacionario mencionado en la sección \ref{basTeoGenetEstan}. 

14) Para garantizar cierta flexibilidad en el algoritmo genético se definen varios enumerados con el fin de poder variar y extender los diferentes aspectos del algoritmos genético. Se debe implementar más de una opción para cada uno de los tipos listados a continuación:

	1) *Tipo de selección*: determina los *individuos* que serán utilizados para generar nuevos genotipos.

	2) *Tipo de cruce*: determina qué genes de los padres serán utilizados para generar un nuevo *individuo*.

	3) *Nivel de cruce*: determina qué partes del genotipo representan una unidad indivisible (un gen) para el cruce.

	4) *Tipo de mutación*: determina qué individuos cambiarán sus genes aleatoriamente y cómo.
 
15) Todas las combinaciones de los enumerados descritos en el punto anterior deben ser permitidas. Se debe permitir también que la población combine varios esquemas del mismo tipo simultáneamente.

16) Las *poblaciones* evolucionan a sus *individuos* para realizar una determinada *tarea*. La *tarea* toma a un individuo y puede presentarle cualquier número de entradas y activar la red para tomar las salidas cualquier número de veces y en cualquier orden (esto puede influir en las salidas si hay conexiones recurrentes) dependiendo de las características concretas del problema a optimizar y al final debe evaluar al individuo estableciendo el valor del campo *fitness* de la entidad *individuo*, que será un número real cuanto más positivo mejor.

17) La *población* debe tratar a todas las tareas de forma similar para que crear nuevas tareas sea una tarea relativamente sencilla. Por supuesto, sencilla sin tener en cuenta las complejidades que cada función de fitness pueda requerir. La clase *tarea* debe ser una interfaz para la clase *población*, pero con un comportamiento interno configurable y extensible. Para la implementación de la *población* nunca debe ser necesario el *tipo de tarea* para la que se está evolucionando. 

18) Se implementarán varias tareas de [[experimentacionClasif][clasificación]] para ser aprendidas por las redes utilizando entranamiento supervisado.

19) Se implementará un [[experimentacionJuegos][juego de estratégia]] como ejemplo de tarea con aprendizaje por refuerzo para ser aprendida por las redes.

20) La librería debe incorporar herramientas para la exploración gráfica de los resultados. Se deben poder generar gráficas de rendimiento computacional y de la evolución del *fitness* de una poblacción.

A partir de estas especificaciones funcionales, se toman ciertas decisiones de [[diseno][diseño]] que escapan el alcance de esta sección. El diseño se encuentra en el capítulo \ref{diseno} aunque, por motivos prácticos relacionados con la presentación del documento, se ha separado el diseño de los operadores genéticos en el capítulo \ref{disenoGene}. Las optimizaciones mediante paralelización de las *capas* también ocupan [[disenoParal][su propio capítulo]].

\newpage
** Objetivos
#+LaTeX: \label{anaObjetivos}

Se probará la librería en casos concretos con el fin de contestar a las siguientes cuestiones:

1) ¿Se puede abstraer más el concepto de /capa neuronal/ para obtener estructuras más simples y simplificar la paralelización?

2) ¿Qué ventajas en el rendimiento se pueden obtener gracias a la paralelización?

3) ¿Qué efecto tienen las funciones de tipo escalón (que permiten codificar la salida de cada neurona como un bit en vez de como un número real) tanto en el rendimiento como en el aprendizaje? 

4) Disminuir la cardinalidad de los pesos reduce el espacio de búsqueda. ¿Qué efecto tiene la codificación de los pesos y umbrales con estructuras discretas de menor tamaño, números enteros acotados en lugar de números reales, tanto en el rendimiento como en el aprendizaje?

5) ¿Resulta efectivo el algoritmo para los problemas propuestos?

6) ¿Qué operadores genéticos resultan más adecuados en el entrenamiento de los problemas propuestos?

7) ¿Qué valores de los parámetros del algoritmo genético resultan más adecuados en el entrenamiento de los problemas propuestos?

8) Las mutaciones de los pesos causrán en ocasiones que una conexión, ya sea excitatoria o inhibitoria, se haga más débil en lugar de hacerse más fuerte ¿Puede la eliminación completa de conexiones neuronales aleatorias mejorar el aprendizaje?

A la primera pregunta se responderá en el [[disenoRedes][diseño de las redes neuronales]]. Y aunque el resto de cuestiones se resolveránen los capítlos de resultados, en el capítulo final de [[conclusiones]] se discute la respuesta a todas ellas.

\newpage
* HACER [1/4] Diseño general
#+LaTeX: \label{diseno}

Como se definió en la sección \ref{anaEspecLib}, la librería debe poder construir redes neuronales de cualquier topología y, al mismo tiempo, debe poder ser paralelizada usando diferentes tecnologías. Además, en la sección \ref{anaObjetivos} establecimos que las neuronas pueden ser de varios tipos (binarias, bipolares y reales). 

Para soportar las diferentes implementaciones y tipos de neuronas sin incrementar la complejidad de la API de la librería, se definirán clases abstractas como interfaces de las que luego heredarán las diferentes implementaciones. Para independizar completamente el manejo de estas clases de fachada [TODO bibliografía patrón diseño facade], las implementaciones concretas sólo serán visibles a una clase factoría que será el único método para instanciar las implementaciones siguiendo el patrón de diseño factoría [TODO bibliografía patrón diseño factory]. Para las diferentes implementaciones paralelas descritas en el capítulo \ref{disenoParal}, se crearán diferentes clases que extiendan de las fachadas. Para soportar los diferentes tipos de neuronas, estas clases paralelizadas se implementarán usando plantillas. Sólo serán utilizadas directamente las clases fachada y los métodos específicos de cada implementación concreta serán llamados utilizando la técnica que en el contexto de análisis, diseño y desarrollo orientado a objetos se denomina polimorfismo.

Primero se describirán las clases fachada y el resto de clases utilizadas para la implementación de las redes neuronales en la sección \ref{disenoRedes}. Las implementaciones concretas de las fachadas para la factoría se describirán con más detalle en la sección \ref{implFactoria}. 

#+CAPTION:    Diagrama de componentes de la librería implementada.
#+LABEL:      disenoComponentes
#+ATTR_LaTeX: scale=0.4
[[./img/uml/components.png]]

En la sección \ref{disenoGenetic} especificaremos de forma general las clases destinadas a la implementación del algoritmo genético y cómo se relacionan con las clases de las redes neuronales. 

Finalmente, en la sección \ref{disenoLoop} se describen las utilidades destinadas a probar la librería y medir su eficiencia, tanto en términos de rendimiento computacional como en términos de aprendizaje. Este último componente debe poder generar gráficas comparativas y ser suficientemente extensible para adaptarse a las necesidades del proyecto, sin acoplarse con este de forma que el componenete pueda ser reutilizado para otros proyectos. 

La figura \ref{disenoComponentes} muestra el diagrama de componentes de la librería. En las siguientes secciones se decribirá como se comunican los diferentes componentes con más detalle.

** REVISAR Estructura de las redes neuronales
#+LaTeX: \label{disenoRedes}

Las redes neuronales se implementan en el paquete *neural*, cuya interfaz de aplicaicón se describe en detalle en la sección \ref{apiNeural}. La clase principal es NeuralNet. Como se quiere independizar la red neuronal de la implementación concreta usada, las redes neuronales se comunican con el exterior mediante la clase Interface. Las entradas y salidas de la red serán objetos de esta clase. La red está compuesta de un conjunto de capas implementadas con la clase Layer. Como las capas de entrada leen de una interfaz, se crea una especialización de Layer llamada InputLayer, que copia sus propias salidas desde una Interface en vez de tomar otras Layer como entrada para calcular su estado a partir de éstas y de sus propios pesos.

Así, NeuralNet tiene dos listas de capas, una para las de entrada y otra para el resto. También mantiene dos grafos dirigidos que representan las conexiones entre esas capas. Uno de los grafos contiene las conexiones desde capas de entrada hacia las otras capas y el otro grafo contiene las conexiones entre las capas que no son de entrada (capas ocultas o de salida). No se hace ninguna distinción entre las capas ocultas y las de salida. Cuando el usuario de la librería solicita el estado de una capa desde el exterior de la red, NeuralNet devuelve una Interface que será creada dentro de la capa y que se actualizará con las salidas de la capa siempre que estas se calculen.

#+CAPTION:    Diagrama de clases del componente Neural.
#+LABEL:      classNeural
#+ATTR_LaTeX: scale=0.33
[[./img/uml/classNeural.png]]

Una Layer está compuesta, a su vez, de un Buffer (es equivalente a Interface, pero especializable para cada implementación) para la salida y de una lista de conexiones representadas por la clase Connection. Una Layer tendrá una conexión por cada capa que una tome como entrada. Además, los umbrales de la capa se alamcenan también en un objeto de la clase Connection, que se crea con el Buffer auxiliar de resultados (antes de aplicar la activación) como entrada y con tamaño de salida 1 (para tener sólo un peso, que en este caso representa a un umbral, por cada neurona de salida de la capa). Sobre esta conexión se llamará al método =activation= pasándole como parámetros el Buffer de salida de la capa y el /tipo de activación/ en vez de usar el método =calculateAndAddTo= tomando como parámetro el Buffer de resultados como hacen el resto de Connection, que realmente representan conexiones. En realidad, esta Connection especial guardada en el atributo =thresholds= puede ser considerada una conexión con conexiones 1 a 1 en vez de una conexión completa (con las conexiones normales cada neurona de salida conecta con todas neuronas de entrada). 

La clase Layer en sí es prescindible, pero la implementación interna de NeuralNet se complicaría mucho si se implementase como una colección de Buffers con conexiones entre sí, en las que algunos Buffer representan resultados intermedios en vez de neuronas reales y algunas Connection son del tipo especial que utiliza la activación en vez del calculo habitual con una matriz de pesos.

Como el almacenamiento de datos es, en principio, común para una misma implementación y su mapeo a Interface debe ser similar, la clase Connection hereda de la clase Buffer y, de no requerir un almacenamiento especial (como es el caso, por ejemplo, de CudaInvertedConnection) reutilizará estos métodos sin reimplementarlos. El diagrama de clases \ref{classNeural} resume lo comentado en esta sección.

** HACER Estructuras para algoritmos genéticos
#+LaTeX: \label{disenoGenetic}
** HACER Utilidades para la experimentación
#+LaTeX: \label{disenoLoop}
- [ ] Automatización de pruebas
- [ ] Recogida y presentación de datos
** MODIFICAR Problemas a resolver
#+LaTeX: \label{experimentacion}

En esta sección se describen los problemas para los que se entrenarán las redes neuronales y las utilidades implementadas para la experimentación.

*** Tareas de clasificación
#+LaTeX: \label{experimentacionClasif}

Las tareas de clasificación son una aplicación común de las redes neuronales entrenadas con retropropagación del error. También podemos entrenar nuestras redes neuronales para aprender a desempeñar este tipo de tares utilizando algoritmos genéticos. En general, la clasificación consiste en agrupar conjuntos de entradas posibles en clases. Por ejemplo, las entradas {e1, e3, e5} pentenecen a la clase c1; las entradas {e2, e4} pertenecen a la clase c2; las {e6, e7} a la clase c3, etc. Cada entrada sólo puede pertenecer a una clase. La clasificación tiene muchas aplicaciones el como reconocimiento de patrones o la construcción de filtros.

Las tareas de clasificación que se han elegido son simples. Se trata de operaciones lógicas entre dos vectores binarios. Las operaciones escogidas son AND (Y lógico), OR (O lógico) y XOR (O lógico exclusivo). Es sabido que para poder desempeñar la tarea XOR son necesarias redes neuronales de más de una capa, es decir, con capas ocultas. Si bien AND y OR eran tareas que un perceptrón simple (red neuronal de una sola capa) podía aprender, no puede aprender, sin embargo la tarea XOR. Esta última tarea fué la primera para la que se entrenó un perceptrón multicapa utilizando el algortimo de retropropagación del error y se ha convertido en un Benchmark común para algunos algortimos de aprendizaje artificial [TODO referencia bilbiográfica].

Puede parecer poco intuitivo que el cálculo de estas operaciones lógicas constituyan una tarea de clasificación, por lo que pondremos unos ejemplos ilustrativos. A continuación se muestran las clasificaciones para las tareas AND, OR y XOR para vectores de un tamaño de 1 bit. Las entradas, por tanto, son dos vectores de 1 bit (v1 y v2). Como la salida será de un bit, en estos casos sólo existen dos clases (0 ó 1) para cada operación/clasificación.

| v1 | v2 | Clase (OR) | Clase (AND) | Clase (XOR) |
|----+----+------------+-------------+-------------|
|  0 |  0 |          0 |           0 |           0 |
|  1 |  0 |          1 |           0 |           1 |
|  0 |  1 |          1 |           0 |           1 |
|  1 |  1 |          1 |           1 |           0 |

Expresado de otra forma, si llamamos c0 a la clase 0 y c1 a la clase 1, para la clasificación OR la entrada {00} pertenece a c0 y las entradas {10, 01, 11} pertenecen a c1; para AND, {00, 10, 01} pertenecen a c0 y {11} a c1; para XOR {00, 11} pertenecen a c0 y {10, 01} pertenecen a c1.

Como se describió en el capítulo \ref{manualProgrInterf}, para que nuestro sistema pueda aprender una tarea nueva, sólo es necesario implementar una clase que herede de la interzaz Task, en este caso, la clase implementada es BinaryTask. El método más importante es test, que toma un individuo como parámetro, lo prueba y le asigna el fitness resultante. También es importante el método setInputs, con el que se conectan las variables internas de la tarea con las entradas de la red neuronal de un individuo. Por último, getExample devuelve un individuo construido cuya estructura es suficiente para aprender la tarea concreta para la que se quiere entrenar a la población.

La clase BinaryTask es bastante flexible respecto a cómo puede ser inicializada. Hay dos parámetros que son olbigatorios: un enumerado BinaryOperation que indica que tipo de operación será realizada (OR, AND ó XOR) y el tamaño de los vectores de entrada, que es igual al tamaño del de salida.
Existe un tercer parámetro optativo numTests que hace referencia al número de pruebas para evaluar a un individuo. Si no se rellena, se probarán todas las combinaciones posibles entre los dos vectores de entrada; si se rellena, determinará el número de pruebas aleatorias que se realizarán para probar a cada individuo. Para las dos posibilidades, el individuo empieza con una puntuación igual al número de diferencias con las salidas esperadas que podría obtener cómo máximo y se irán restando las diferencias que se vayan encontrando. Así, la puntuación del inndividuo será mejor cuanto menor sea el número de diferencias sigueindo la siguiente fórmula: Fitness = Número máximo de diferencias posibles - número de diferencias obtenidas.
Las pruebas aleatorias consisten simplemente en dar valores aleatorios a los vectores, hacer que la red neuronal obtenga su salida, obtener la salida deseada realizando la operación lógica correspondiente y comparar las diferencias.

No rellenar el número de pruebas y dejar que se evalúen todas las posibilidades nos dará valores de fitness más precisos, pero puede hacer las pruebas muy lentas para tamaños de vectores más grandes.

Para que esta tarea pueda ser realizada por neuronas binarias, bipolares y reales, en lugar de comparar la salida de la neurona directamente con la salida deseada se usarán aproximaciones. Se entenderá que en la salida de las redes neuronales, cualquier valor mayor o igual a 0.5 es equivalente a un 1 y cualquier valor menor que 0.5 (por ejemplo -1 para una neurona bipolar; 0.1 ó -7 para una neurona real) es equivalente a un 0. Esto se podría implementar con una capa adicional, pero se ha preferido por simplicidad hacerlo directamente dentro de la clase BinaryTask.

*** Juegos de estrategia abstractos
#+LaTeX: \label{experimentacionJuegos}

En la sección anterior \ref{experimentacionClasif} hemos visto ejemplos de tareas para las que se podían entrenar redes neuronales con el método tradicional de retropropagación del error. En esta sección nos dedicaremos a una tarea para la que no es tan fácil conocer las salidas deseadas. Los juegos de estrategia abstractos[fn:juegEstratAbst] son aquellos juegos de estrategia para los que se trata de minimizar el factor suerte y que carecen de trasfondo o ambientación. Casi todos entran dentro de las categorías de tablero, cartas o piezas (como el dominó). No tienen información oculta ni elementos no determinísticos y frecuentemente se juegan por dos jugadores en turnos alternativos.

Nos hemos centrado en los juegos de tablero y en concreto en el juego conocido como Othello o Reversi. Otros juegos de estrategía abstractos de tablero podrían ser las damas, el tres en raya, el ajedrez, el go, el arimaa, etc. El tres en raya y las damas, por ejemplo, son problemas completamente resueltos matemáticamente y en esos casos sí sería relativamente fácil emparejar todas las posibles entradas con sus salidas deseadas para poder así entrenar a una red neuronal mediante retropropagación, pero no son particularmente interesantes desde el punto de vista del aprendizaje artificial. En otros juegos, los algoritmos de poda alfa-beta con alguna heurística diseñada por expertos y ejecutados en computadores son ampliamente superiores a los jugadores profesionales de los mismos. Es el caso del Reversi y el ajedrez.

En otros juegos, el árbol de posibilidades crece tanto con cada nivel que la ventaja de una mayor lectura en profundidad que disfrutan las máquinas se desvanece y la intuición humana aún es superior al cálculo computacional, por increíble que pueda parecer. El juego del Go (cercado), a pesar de tener un tablero tan simple como el del Reversi y pocas reglas simples de enumerar entra en esta categoría. Es un juego asiático más antiguo que el ajedrez y muy célebre en oriente, en especial en China (weiqui), Korea (baduk) y Japón (igo). Por el momento, la mejor de las máquinas (que no usa poda alfa-beta sino métodos probabilísticos y altamente paralelizables como el algortimo de Monte Carlo\cite[Chaslot2010]{Chaslot2010}) no es capaz de ganar al peor de los jugadores profesionales.

El Arimaa es un juego diseñado recientemente con el objetivo específico de que los algoritmos habituales de búsqueda en profundidad no fuesen efectivos  \cite[Syed03]{Syed03}. Es parecido al ajedrez, con el mismo tablero y piezas, pero sin una configuración inicial preestablecida, con casillas especiales, turnos de dos pasos independientes, movimientos de cambiar de posición una pieza con la del contrario, etc.

Aunque nuestras redes también pueden ser entrenadas para dar una heurística para juegos como el Ajedrez y el Arimaa, hemos preferido optar por los juegos más simples de implementar con piezas de un sólo tipo (el Reversi y el Go), pues esto permitirá reutilizar más código y también puede ser interesante éstudiar el aprendizaje de redes bipolares en este tipo de juegos. Se ha  implementado un tablero que serviría para ambos juegos, pero sólo se ha implementado la tarea Reversi. Para la terea Go, extremadamente interesante, se recomienda utilizar algún jugador ya implementado mediante software libre como puede ser GnuGo o FueGo.

Este tipo de tareas se implementará de forma general haciendo que las redes neuronales actúen como una heurística. Esta heurística puede ser usada como la base de un algoritmo de poda alfa-beta con profundidad configurable o simplemente considerando solamente el conjunto de todos los movimientos legales inmediatos, que es lo que se ha hecho para el caso Reversi. Para evaluar las redes neuronales, en lugar de enfrentarlas entre sí, se ha preferido utilizar un adversario también automático pero no basado en redes neuronales. Así, cada red que quiera ser evaluada se enfrenta a este jugador una o varias veces y se acumulan los resultados para obtener el fitness.

El jugador que se ha implementado para Reversi es extremadamente simple, pero, aún así, es capáz de ganar al jugador humano casual. No utiliza poda  alfa-beta, sino simplemente evalua todos los movimientos legales inmediatos y elige el mejor, igual que lo harán los jugadores neuronales. La diferencia es que el oponente no utiliza una red neuronal como heurística. La heurística del oponente consiste simplemente en contar la puntuación que resultaría si se realizase un movimiento concreto, como si la partida acabase en ese momento. El oponente, por tanto, tiene acceso a la puntuación actual de cada movimiento hipotético y en eso basa su heurística. La red neuronal, sin embargo, no tiene acceso a esas puntuaciones: solamente toma como entrada el tablero resultante de cada movimiento hipotético y debe con eso dar una aproximación de lo bueno que es el movimiento. Para poder ganar a nuestro oponente tendrá que ser capáz, por lo menos, de ser capaz de calcular la puntuación de forma similar a su oponente. No obstante, la red neuronal actúa como una caja negra y no sabemos realmente en qué criterios internos se está basando. Lo que sí se podría hacer es construir un circuito lógico equivalente a la red neuronal para analizarlo y tratar de extraer conclusiones sobre la estrategia aprendida.

La tarea Reversi se implementa en la clase ReversiTask que también hereda de la clase abstracta Task y que utiliza la clase ReversiBoard que implementa las reglas de juego de Reversi y que, a su vez, hereda de la clase que implementa el el tablero genérico para juegos con piezas iguales pero de dos jugadores Board. Éste último siempre es de un tamaño cuadrado (las mismas casillas a lo largo que a lo ancho) pero el tablero de Reversi tiene, además, la restricción de ser como mínimo de un tamaño 4x4. Esto es así por que las cuatro piezas centrales (2x2) empiezan ya rellenas para que los juegadores tengan movimientos legales al inicio.

\newpage
* Diseño del algoritmo genético
#+LaTeX: \label{disenoGene}
** Funcionamiento general
#+LaTeX: \label{disenoGeneFunc}

Como se vió en la sección \ref{basTeoGenetEstan} existen diferentes enfoques en cuanto a la gestión de la población de individuos. El algoritmo genético original adoptaba la política de reemplazo generacional, con el que la población completa es reemplazada en cada generación. En cambio, la política de estado estacionario reemplaza la población selectivamente, permitiendo mantener uno o varios miembros de la población por varias generaciones, siempre que estos mantengan su puntuación por encima de otros individuos de la población. Nuestra gestión de la población debe permitir ambas posibilidades de forma configurable.

Para ello, mantendremos a la población como una lista ordenada en la que se irán insertando (también ordenadamente) los nuevos individuos producidos. Si tras una inserción se tienen más individuos que el tamaño máximo, el peor individuo (sea el nuevo o no) será desechado. Si dos individuos comparten la misma puntuación al ser comparados durante una inserción, se le dará ventaja al nuevo individuo siguiendo el criterio de búsqueda neutral, por el que permitimos que se acumulen cambios aunque no tengan efecto en el fitness, para explorar más el espacio de búsqueda \cite[T. Weise, 2009]{Weise2009}. Este comportamiento es el propio del estado estacionario. Para obtener el comportamiento generacional, así como diferentes híbridos entre las dos posibilidades, definiremos una variable configurable para la población. Tras generar a los individuos de la siguiente generación, el sistema mirará esta variable para saber cuantos de los antiguos individuos debe conservar para competir con los nuevos y simplemente elimina al resto. Si el numéro de individuos a preservar es 0, el comportamiento será el generacional puro. Si el número de individuos a preservar es igual al tamaño máximo de la población (o es un número negativo), no se eliminará a ningún individuo de la generación anterior y todos ellos tendrán la oportunidad de sobrevivir compitiendo con los de la nueva generación. Si el número es algo intermedio entre 0 y el tamaño máximo de la población, estaremos usando un híbrido entre las políticas de reemplazo generacional y la de estado estacionario.

En general, para cada nueva generación se realiza la siguiente secuencia de acciones:

1) Selección: se puede definir una cantidad independiente de individuos a seleccionar con cada operador de selección. De esta manera, se pueden utilizar varios operadores de selección simultaneamente y combinarlos de infinidad de formas. Se deben seleccionar un mínimo de dos progenitores en cada generación para que el siguiente fallo no resulte en error.

2) Cruce: una vez seleccionados los progenitores, se genera a partir de ellos la descendencia, los nuevos individuos. Los progenitores se van eligiendo aleatoriamente y si van marcando para no ser usados dos veces. Si se han seleccionado menos individuos de los que se quieren generar mediante cruce, cuando todos hayan sido usados una vez se desmarcarán para poder ser reutilizados y continuar con la generación de la descendencia mediante el cruce. Por tanto, el número de nuevos individuos por generación puede ser tanto mayor como menor al número de progenitores seleccionados. Además, como ocurría en la selección, varios operadores de cruce diferentes pueden combinarse también. En este caso, cada operador de cruce puede ser aplicado a un nivel de cruce diferente (ver sección \ref{disenoGeneNiv}) y cada una de estas combinaciones se le puede asignar un número independiente de individuos a generar por cruce. Por tanto, en este caso las posibilidades son aún más abundantes que para la selección.

3) Olvido: a cada uno de los individuos de la descendencia se le aplica el operador de olvido determinístico o probabilístico (o los dos, aunque no tenga mucho sentido) como se detalla en la sección \ref{disenoGeneMut}.

4) Mutación: de forma similar al paso anterior, sobre cada uno de los individuos de la descendencia se le aplica el operador de mutación determinístico o probabilístico (o los dos, aunque de nuevo no tenga mucho sentido) como se detalla en la sección \ref{disenoGeneMut}.

5) Preservación de individuos antiguos: como se ha comentado antes, se puede definir un número de individuos antiguos a conservar en cada generación. Se mirará la variable "individuos a preservar" para conservar a los mejores y se eliminarán los que sean peores. Si la variable contiene un cero, se estará aplicando la política de reemplazo generacional, pues en tal caso se eliminarían en este paso todos los individuos antiguos.

6) Se probarán e insertarán ordenadamente en la población los individuos de la descendencia. Puede que alguno no llegue a estar en la población como tal si no hay hueco para él. Nótese que se han podido generar más descendientes en el paso 2 de lo que se haya definido como el tamaño máximo de la población. Y, además, puede que estos individuos tengan que competir no sólo con los individuos de su generación, sino con los conservados en el paso 5.

Para generar la popblación inicial, se tomará un individuo de ejemplo del que se copiará la estructura de la red neuronal para generar individuos aleatorios (con pesos y umbrales aleatorios) que se irán insertando ordenadamente en la población (lo que implica evaluarlos) hasta completar el tamaño máximo de la población. El criterio que se ha elegido es el de maximizar el fitness. La tarea debe ser diseñada de tal forma que un individuo con un fitness mayor sea mejor que uno con fitness menor.

\newpage
** Operadores de selección
#+LaTeX: \label{disenoGeneSel}
Los operadores de selección que se han implementado son los siguientes: ruleta, ranking, torneo y truncado.

*** Ruleta
#+LaTeX: \label{disenoGeneSelRule}

Este tipo de selección sólo admite individuos con fitness mayor que cero, si el peor individuo no cumple esta condición se lanczará un error.
Para la selección por ruleta lo primero que hay que hacer es sumar el fitness de todos los individuos (S).
Luego, por cada individuo a seleccionar por este método:

1) Se elige un número aleatorio del intevalo (0, S), que llamaremos E (de elegido).

2) Se recorre la población desde el mejor individuo. Si el fitness del individuo (más el fitness de los individuos anteriores) es mayor que E, se selecciona ese individuo. Si no, se pasa al siguiente, acumulando el fitness de este individuo para la siguiente comparación.

*** Ranking
#+LaTeX: \label{disenoGeneSelRank}

Para la selección por ranking se puntuan los individuos dependiendo de su posición en la población.
Tradicionalmente se asigna N (el máximo de la población) al mejor, N-1 al segundo mejor, y así sucesivamente hasta llegar al peor individuo al que se asigna un fitness de 1. En nuestro caso hemos querido que sea más configurable y hemos añadido dos variables configurables: el "salto para el ranking" y la "base para el ranking". El salto para el ranking es la diferencia de fitness entre un individuo y el siguiente, en el ejemplo anterior era 1, pero podemos aumentar la presión selectiva incrementando este número. La "base para el ranking" se suma al fitness de toda la población. Por ello, para utilizar el ranking tradicional, los valores por defecto son "salto para el ranking" = 1 y "base para el ranking" = 0.

Una vez tenemos estos fitness auxiliares, se realiza la selección siguiendo un método similar al de la ruleta, pero con estas puntuaciones en lugar de los fitness originales.

*** Por torneo
#+LaTeX: \label{disenoGeneSelTorn}

Para la selección por torneo se cuenta con una variable configurable "tamaño del torneo" que no puede ser menor que el tamaño máximo de la población. En caso contrario se generará un error. Para cada individuo a seleccionar por este método:

1) Se preseleccionan "tamaño del torneo" individuos de la población de forma totalmente aleatoria pero evitando que se repitan.

2) Se selecciona el individuo más apto de todos los que están en el torneo.

El tamaño típico y, por ello, el valor por defecto que hemos seleccionado para el tamaño del torneo es 2.

*** Elitísta o por truncado
#+LaTeX: \label{disenoGeneSelTrunc}

La selección elitista es la más sencilla de todas. Simplemente se cogen los N (donde N es el número de individuos a seleccionar por este método) más aptos desde el principio de la lista ordenada de la población.

\newpage
** Operadores de cruce
#+LaTeX: \label{disenoGeneCruz}

Aunque aceptamos varias definiciones de gen, como se explica en la sección \ref{disenoGeneNiv}, en esta sección trataremos las formas en que se pueden cruzar dos individuos, produciendo dos descencientes con los genes de los progenitores combinados de forma complementaria (todos los genes de los progenitores irán a un descendiente o a otro, aunque puede que uno de los descendientes se deseche si sobra). 

Todos los esquemas de cruce se aplican primero sobre un vector de bits (cada bit representa un gen) y luego se aplica el crossover usando ese vector. Esto permite compartir una sóla interfaz para el cruce a bajo nivel. Dada la diversidad de implementaciones de las redes neuronales, la cantidad de código se multiplicaría con los distintos esquemas de cruce de forma que el código sería mucho más complicado de desarrollar y mantener. Esto permite extender nuestro algoritmo genético con nuevos esquemas de cruce sin necesidad de modificar las distintas implementaciones (C, SEE2, CUDA). 

También es posible crear una nueva implementación (por ejemplo, usando openCL) sin necesidad de implementar por separado cada uno de los esquemas de cruce. De otra manera, la complejidad del código crecería NxM con respecto al número de esquemas de cruce y de implementaciones paralelas. De esta manera, sólo hay que implementar N + M.

Además, los pesos pueden estar dispuestos de forma diferente en memoria dependiendo de la implementación, como sucede en el caso descrito en las secciones \ref{disenoParalCUDAinv} y \ref{disenoParalCUDAcruza}, en el que la matriz de pesos se almacena invertida en memoria. En ese caso, basta con invertir la matriz de bits interfaz, en lugar de reimplementar el algoritmo de cruce que comparte con otros algoritmos CUDA.

*** Uniforme
#+LaTeX: \label{disenoGeneCruzUni}

Para el cruce uniforme, se debe indicar un parámetro "probabilidad", que puede ser configurado independientemente para cada nivel de cruce.
Para generar el hijo A, por cada gen de los progenitores, se elige un número aleatorio en el intervalo (0, 1). Si el número es menor que la probabilidad, se cogerá el gen del progenitor B, en caso contrario, el del progenitor A. Para generar el hijo B, se utilizan los genes que no se hayan utilizado para el descendiente A.

La probabilidad por defecto para todos los niveles es 0.7.

*** Proporcional
#+LaTeX: \label{disenoGeneCruzProp}

Este modo de cruce funciona de forma similar al anterior, con la diferencia de que la probabilidad no es especificada por el usuario, sino que se calcula a partir de los fitness de los progenitores. Tradicionalmente, se usa la siguiente fórmula:

\begin{equation}\label{eqCruzProp}
  probabilidad = finessA / (fitnessA + fitnessB)
\end{equation}

Esta fórmula sólo admite finess positivos, pero en nuestro caso hemos admitido más casos.

1) Si ambos son positivos, se aplica la fórmula \ref{eqCruzProp}.

2) Si ambos fitness son iguales a cero, la probabilidad es 0.5.

3) Si fitnessA es positivo y fitnessB es menor o igual que cero, la probabilidad es 1.

4) Si fitnessA es menor o igual que cero y fitnessB es positivo, la probabilidad es 0.

5) Por último, si ambos son negativos, se aplica otra fórmula parecida a la primera (pero en este caso, cuanto menos negativo mejor):

\begin{equation}\label{eqCruzPropNeg}
  probabilidad = -finessB / -(fitnessA + fitnessB)
\end{equation}

Aunque contemplar estos casos especiales puede parecer una complicación innecesaria, nos permite que este tipo de cruce sea compatible con tareas que admiten fitness negativos en lugar de tener que lanzar un error.

*** Multi-punto
#+LaTeX: \label{disenoGeneCruzMulti}

En la literatura convencional, frecuentemente se mencionan el "cruce de un punto" o el "cruce de dos puntos", pero en realidad son casos concretos del caso más general "cruce multipunto". Por ello, se ha decido implementar sólo esta última, creando un parámetro "número de puntos" que puede ser configurado independientemente para cada nivel de cruce. El número de puntos por defecto para todos los niveles es 1.

El funcionamiento general es el siguiente:

1) Se marcan aleatoriamente "número de puntos" genes, que serán como puntos de corte.

2) Desde el inicio, hasta el primer punto, se cogen los genes del progenitor A. A partir desde este punto de corte hasta el siguiente, se cogen los genes del progenitor B, luego de nuevo los del A y así sucesivamente hasta el final.

De esta manera, se va alternando el progenitor en cada punto. Como siempre, el decendiente B usará los genes que no haya usado el descendiente A.

** Niveles de cruce
#+LaTeX: \label{disenoGeneNiv}

*** Pesos y umbrales
#+LaTeX: \label{disenoGeneNivPes}

Este es el nivel de cruce más pesado y sensible de todos. Todas las capas se colocan una detrás de otra con sus pesos seguidos de sus umbrales. Cada peso o umbral es un gen.

*** Neurona
#+LaTeX: \label{disenoGeneNivNeu}

En este caso cada gen es una neurona, con todos sus pesos y con su umbral. Los pesos son los que se multiplican por las entradas a esta neurona.
Se colocan en orden todas las capas y todas las neuronas de cada capa.

*** Neurona invertida
#+LaTeX: \label{disenoGeneNivNeuInv}

Este caso es muy similar al anterior, pero se cambia la definición de lo que se considera una neurona. En este caso, junto con el umbral, forman parte del mismo gen los pesos que se multiplican por la salida de esta neurona, en lugar de los que utiliza esta neurona para calcular su estado. Esta representación ha sido también denominada "neurona en fregona" \cite[J. Merelo, 2012]{Merelo2012}.

*** Capa
#+LaTeX: \label{disenoGeneNivCap}

Para el nivel de capa, cada capa, valga la redundancia, es considerada un gen. Una capa incluye todas sus neuronas con sus pesos y umbrales, entendiendo una neurona como se hace en el apartado \ref{disenoGeneNivNeu} y no como la neurona invertida.

Aunque intuitivamente se puede pensar que este tipo de cruce no será muy útil si las capas son muy pocas o muy grandes, se ha decido implementar también este nivel de cruce para comparar el aprendizaje.

\newpage
** Mutación y olvido
#+LaTeX: \label{disenoGeneMut}

La forma en que se implementan el operador de mutación y el de olvido son muy similares. La principal diferencia es que mientras el operador de olvido o reset simplemente pone a cero el peso o umbral que toque, el de mutación le suma un número aleatorio del intervalo (-X, X), donde X es un parámetro configurable que llamaremos "rango de mutación", que por defecto toma el valor 1. En cierto sentido, se podría considerar al operador de olvido como un tipo especial de mutación. En la práctica equivale a destruir una conexión de la red neuronal.

Por lo demás, los dos operadores tienen dos formas de ser empleados: probabilística y determinista.

*** Probabilística
#+LaTeX: \label{disenoGeneMutProb}

Esta forma de mutación es la más habitual en los algoritmos genéticos. Se usa una probabilidad parámetro ("probabilidad de mutación" o "probabilidad de olvido", ambas 0 por defecto) para calcular con cada peso y umbral si será mutado o no. Se elige un número aleatorio entre 0 y 1 y si el número es menor que la probabilidad, se realiza la acción correspondiente. Si es mutación sumar al peso la mutación que se obtiene a partir del rango como se ha comentado anteriormente y si es olvido el peso se iguala directamente a cero.

*** Determinista
#+LaTeX: \label{disenoGeneMutDet}

Para evitar repetir el calculo de la probabilidad tantas veces y mejorar el rendimiento, se ofrece esta otra modalidad de mutación, con la esperanza de que el aprendizaje no se vea afectado negativamente.

En este caso en lugar de determinar probabilisticamente y peso por peso si un peso debe mutar o no, se configura un número determinado de mutaciones (u olvidos) que se aplicarán a cada individuo. Las variables "número de mutaciones" y "número de olvidos" tienen ambas por defecto el valor 0. Sabiendo el número de mutaciones que se van a realizar, sólo queda determinar aleatoriamente qué pesos y/o umbrales concretos serán mutados (u olvidados).

Para activar cualquiera de las dos modalidades en cualquiera de los dos operadores, basta con dar un valor positivo a las variables "probabilidad de mutación", "probabilidad de olvido", "número de mutaciones" y "número de olvidos". Como es habitual, se pueden emplear simultaneamente las varias opciones. En este caso también puede no activarse ningún tipo de mutación ni de olvido.
\newpage
* Optimizaciones mediante paralelización
#+LaTeX: \label{disenoParal}
** Introducción

Tanto los algoritmos genéticos como las redes neuronales requieren cálculos que presentan paralelismos inherentes. Para este proyecto se ha escogido explotar exclusivamente los de las redes neuronales (aunque también se paraleliza el operador genético de cruce para GPGPU, como se describe en la sección \ref{disenoParalCUDAcruza}). Pero la implementación se podría extender para aprovechar también los de los algoritmos genéticos, por ejemplo, utilizando múltiples CPUs y GPUs, usando una CPU para cada individuo y administrando las GPUs según su disponibilidad. Esto requeriría cambios no triviales en el modo en que las poblaciones son procesadas cada generación si se quiere extender la librería en ese sentido. Nuestras paralelizaciones solamente usan una CPU. Se han optado por dos alternativas que se comparan.

Gracias al diseño modular por el que se ha optado, es posible añadir otras implementaciones paralelas de las redes neuronales (por ejemplo, usando el lenguaje OpenCL) tan sólo implementando unos pocos métodos en un par de clases que extiendan las clases Fachada (patrón de diseño ya comentado en el diseño) que contienen toda la parte susceptible de ser cambiada para obtener mejor rendimiento.

La primera alternativa implementada es la utilización del conjunto ampliado de instrucciones SSE2 para acceder al co-procesador XMM. Este co-procesador está presente en todos los computadores recientes de la familia x86 liderada por Intel, que es probablemente la arquitectura más extendida en el mundo. La arquitectura vectorial del co-procesador multimedia permite operar sobre varios datos similares al mismo tiempo. En la sección \ref{disenoParalXMM} se explica con más detalle la arquitectura del mismo y como se ha utilizado para paralelizar nuestro algoritmo.

La segunda paralelización obedece a una tendencia bastante más reciente y en alza conocida como GPGPU (General Purpose Graphic Processor Units), que consiste en utilizar las terjetas especializadas en procesar gráficos para procesar otros cálculos que posiblemente nada tengan que ver con los gráficos. Debido a la gran demanda proveniente de diseñadores gráficos y, sobre todo, aficionados a los videojuegos, estos dispositivos comenzaron a tener unas especificaciones que resultaban muy atractivas a gran variedad de investigadores como físicos o bioquímicos. Al principio los investigadores dependian de su ingenio para mapear sus problemas específicos a un algoritmo que usase primitivas gráficas, pero con el creciente interés de esta técnica, los fabricantes decidieron ampliar su mercado de consumidores creando lenguajes específicos para este fin mucho más amigables y con facilidades para la optimización. El lenguaje C CUDA de NVIDIA, con el que desarrollamos la paralelización descrita en la sección \ref{disenoParalCUDA} es un ejemplo de estos lenguajes. Más tarde las compañías decidieron crear un lenguaje común que sirviese para todas las GPUs sin importar la marca llamado OpenCL. Hoy en día muchas de los supercomputadores más potentes del mundo utilizan múltiples GPUs para obtener los altos rendimientos que requieren[fn:cudaSuperComp].

Las redes neuronales, dada su alta paralelidad a nivel de datos son un buen candidato para la optimización por GPGPU, incluído el algoritmo /backpropagation/ \cite[Davis, 2001]{Davis2001}.

** Ensamblador con SSE2
#+LaTeX: \label{disenoParalXMM}
*** Introducción al coprocesador XMM
#+LaTeX: \label{disenoParalXMMintro}

Como ya se ha mencionado, el coprocesador XMM utiliza una arquitectura vectorial (SIMD, Single Instruction Multiple Data, figura \ref{SIMDexecutionModel}). Esto significa que tiene varias ALUs que pueden realizar la misma operación sobre múltiples datos en paralelo. Como veremos, la tecnología XMM parmite algunas cosas más como operaciones de reducción sobre el vector de datos. XMM es una extensión de MMX (que introducía el célebre procesador Pentium XMM) en la que se dobla el tamaño máximo de los vectores (de 64 a 128 bits) y se añaden algunas instrucciones. Este coprocesador es utilizado también para las operaciones habituales con números de doble precisión, por lo que alternar frecuentemente entre los dos usos puede resultar en serias penalizaciones al rendimiento.

#+CAPTION:    Modelo de ejecución SIMD. En nuestro caso el destino se almacena en el mismo registro de origen 1.
#+LABEL:      SIMDexecutionModel
#+ATTR_LaTeX: scale=0.4
[[./img/SIMD_Execution_Model.jpg]]

El tamaño de los registros-vectores depende del tipo de datos a procesar: se pueden tener 2 números en doble precisión, 4 números en coma flotante, 4 enteros (con o sin signo), 8 enteros cortos (short), 16 bytes, 128 bits para operaciones lógicas, etc. La figura \ref{XMMregister} lo ilustra con más detalle. 

#+CAPTION:    Posibles usos vectoriales de los 128 bits de un registro XMM.
#+LABEL:      XMMregister
#+ATTR_LaTeX: width=\textwidth
[[./img/XMMregisters.jpg]]

No es preciso indicar qué tipo de datos contiene cada registro vector, los datos de cada registro XMM serán interpretados de una manera u otra dependiendo de la operación que se aplique sobre ellos. El compilador o en este caso el programador es responsable de mantener la integridad de los mismos. Por ejemplo, la instrucción PADDB, sumará dos registros interpretándolos como Bytes idependientes, PADDW sumará palabras (2 Bytes) y PADDD los tomará como palabras dobles (4 Bytes, el tamaño del típico int de C). Si queremos saturación con o sin signo debemos utilizar instrucciones que lo indiquen como PADDSB (saturación con signo) o PADDUSB
 (saturación sin signo). ADDPS para números en coma flotante con precisión simple (4 bytes), etc. Las instrucciones para usar registros MMX pertenecen al conjunto extendido SSE y las que operan sobre registros XMM pertenecen a SSE2.

*** Operaciones vectoriales con números en coma flotante
#+LaTeX: \label{disenoParalXMMfloat}

La función desarrollada para XMM para optimizar los cálculos de una red neuronal o capa de tipo float (sin optimizar la activación) puede ser llamado desde C/C++ usando el siguiente prototipo:

#+begin_src c
    void XMMreal(float* bufferEntrada, unsigned numeroBloques,
                 float* pesos, float &resultado);
#+end_src

Para calcular el estado de una neurona de tipo float se escribirá en la variable de salida resultado (sobre la que se tendrá que aplicar posteriormente la activación), tomamos como entrada dos vectores y un entero. Los arrays son el buffer de entrada (la salida de una capa de tipo float) y otro con los pesos asociados a esa entrada para esta neurona de salida concreta. El entero nos indica el número de bloques de entrada que han de ser procesados. Como se trada de números flotantes en precisión simple, podemos operar con cuatro de ellos simultáneamente en el coprocesador XMM. Por tanto los bloques son de tamaño 4 y los ambos arrays deben reservar un tamaño en memoria que sea múltiplo de cuatro floats. Los números sobrantes también serán procesados, por lo que es preciso anular las entradas y/o los pesos para evitar que estos valores sobrantes no afecten al resultado final.

Internamente, se van recorriendo ambos vectores, multiplicándo los elementos y acumulando los resultados. El núcleo del bucle contiene estas dos instrucciones:

#+begin_src asm
 	MULPS XMM0, XMM1
	ADDPS XMM3, XMM0
#+end_src

La primera multiplica 4 entradas contenidas en XMM0 por sus pesos correspondientes contenidos en XMM1. La segunda instrucción va acumulando los resultados en XXM3. Al final sólo hay que sumar los 4 subtotales que hay en cada uno de los elementos de XMM3 y devolver el resultado en la variable resultado.

*** Operaciones vectoriales con Bytes
#+LaTeX: \label{disenoParalXMMbyte}

Para poder aprovechar al máximo las capacidades del coprocesador XMM, se decide implementar un tipo de capa con unas características concretas.
La primera es que el estado de las neuronas será almacenado en bits, ya se trate de neuronas binarias cuyos estados pertenecen al conjunto {0, 1} o de neuronas de tipo bipolar cuyos estados pueden ser {-1, 1}. Esto nos ahorrará mucho espacio en memoria y, sobre todo, muchas lecturas de memoria para procesar el mismo número de neuronas de entrada.

La segunda característica es que los pesos tendrán valores pertenecientes al conjunto de enteros [-128, 127] y, por tanto, cada peso ocupará un byte en memoria. Esto significa que, además de leer menos datos de memoria como ocurre con las entradas, podremos procesar los pesos de 16 en 16 (los bytes que caben en un registro XMM de 128 bits) en lugar de hacerlo de 4 en 4 como en la función anterior que operaba con números en copa flotante con precisión simple. El hecho de que los pesos puedan tomar menos valores nos permitirá además reducir el espacio de búsqueda en el algoritmo genético, pero a la vez impone mutaciones enteras y, por tanto, cambios más bruscos. Los resultados en términos de aprendizaje al comparar los dos tipos de pesos se encuentran en el apartado \ref{aprendDiscretLineales}.

Las funciones para las capas de tipo binario y las de tipo bipolar son muy similares, sus prototipos son:

#+begin_src c
    int XMMbinario(void* bufferEntrada, unsigned numeroBloques, unsigned char* pesos);
    int XMMbipolar(void* bufferEntrada, unsigned numeroBloques, unsigned char* pesos);
#+end_src

Se ha escogido en este caso devolver el resultado directamente en lugar de usar un parámetro de salida, pero la decisión no tiene consecuencias trascendentes. Se explicará primero como funciona internamente la primera de las funciones y luego, para la segunda, sólo se explicarán las partes que la hacen diferente. Para una mayor claridad a la hora de presentar porciones de código, usaremos nombres descriptivos (similares a nombres de variables en lenguajes de más alto nivel) en lugar de los nombres de los registros XMM que se han utilizado en el código real: XMM0, XMM1...XM7.

Como en el caso en coma flotante, las entradas y los pesos se procesarán por bloques y se deberán rellenar adecuadamente los pesos y entradas para evitar que se sumen cálculos no desados. Para el caso bipolar es imprescindible anular los pesos, no basta con anular las entradas pues los bits nulos serán interpretados por el algortimo como -1 en vez de como 0. Para las entradas, los bloques contendran 128 bits, cada uno representando a una neurona de entrada. 

Para los pesos, los bloques serán de 16 bytes, uno para cada peso. De este modo, por cada bloque de entrada completo se requerirán 8 bloques de pesos (8 * 16 = 128). El número de bloques que se recibe por parámetro se refiere al número de bloques de pesos. Así, si no se van a usar todas las neuronas de entrada en el último bloque, no hay que seguir leyendo pesos que se sabe que deben ser nulos para el funcionamiento correcto. Esos bloques sobrantes no han de procesarse, ni siquiera almacenarse en memoria. Lo importante es que dentro del bucle principal que recorre las entradas (que se irán almacenando en el registro XMMentrada), hay un sub-bucle que se ejecuta hasta ocho veces, una vez por cada 16 pesos que se requieran, que se irán almacenando en el registro XMMpesos.

Para acceder a los bits de un bloque de entrada de 16 en 16 (el número de pesos que se van a procesar en cada vuelta del bucle de pesos), usaremos el registro XMMmascara que tendrá un bit activo por cada uno de los 16 bytes del bloque. La máscará se inicializará por cada bloque de entrada con 16 bytes iguales a 128 (el primer bit activo y todos los demás nulos en binario) y luego se irá deplazando todo el registro una posición a la derecha por cada nueva lectura hacia XMMpesos que no suponga también una lectura en XMMentradas y, por tanto la inicialización de la máscara. Los 16 byes con un 128 vienen de una constante en memoria. Para evitar la penalización que supondría leer esta constante por cada 8 bloques de pesos leídos, se reservará el registro XMM128 de los 8 disponibles (con arquitecturas de 64 bits, el coprocesador XMM dispone de 16 registros en vez de 8) y que en todo momento contendrá dicha constante leída de memoria una sola vez al principio de la función. Para ello se usará la siguiente instrucción (la misma que se usa para leer entradas y pesos):

#+begin_src asm
	MOVDQU XMM128, [cte_mascara_en_mem];
#+end_src

Cuando se quiera inicializar la máscara simplemente se utilizará la siguiente instrucción, que copia el contenido de un registro a otro y es mucho menos costosa que la anterior:

#+begin_src asm
	MOVDQA XMMmascara, XMM128
#+end_src

Para no estropear la mascará, previamente se ha copiado su contenido a XMMaux, sobre el que se harán varias operaciones. Ahora para acceder a cada uno de los bits en la posición que toque de las ocho, bastará con hacer un AND lógico con el registro de entradas. 

#+begin_src asm
	PAND XMMaux, XMMentradas
#+end_src

Ahora dependiendo de si el byte tiene algún bit activo o no, se sumará o no el peso correspondiente. Esta colocación de los bits con respecto al orden en que se cogen los pesos no es igual a la del algoritmo equivalente implementado en C, por tanto la función de activación de los tipos binario y bipolar para la implementación SSE2 (aunque esté escrita en C), debe tener en cuenta la disposición especial de los bits de entrada que espera esta función. Lo mismo sucede para los métodos que copian vectores de bits desde los Buffer dependientes de la implementación a los vectores más generales de la clase Interface que usamos para acceder a las entradas y salidas de la red neuronal desde el exterior, independizando así el manejo de estos datos de la representación interna que pueda tener cada implementación, como ya se ha descrito en la sección \ref{diseno}.

¿Cómo llegamos a partir de lo que tenemos en XMMaux (cada byte tiene en bit activo o no, dependiendo del estado de la neurona de entrada procesada) y en XMMpesos a un registro en el que sólo se tengan los pesos que correspondan a neuronas activas y que tenga anulados los pesos que corresponden a neuronas inactivas? Son necesarios algunos trucos de bastante bajo nivel que son realmente la parte más interesante de las funciones. Primero ejecutaremos la siguiente instrucción:

#+begin_src asm
    PCMPEQB XMMaux, XMMnulo
#+end_src

PCMPEQB compara cada byte de ambos registros y, si son iguales, pone a 255 (todos los bits activos) del byte en el primer registro (XMMaux). Si son distintos, pone cero (todos los bits inactivos) en ese mismo byte. En nuestro caso lo estamos comparando con un registro en el que todos los bits son nulos. Por ello, los bytes de XMMaux que tuviesen un bit activo se anularán enteros (por ser distintos a cero) y los que no tuviesen ninguno activo tomarán el valor 255 (por haber sido iguales a cero). Pero nosotros queríamos justamente lo contrario, por lo que invertimos completamente XMMaux para obtener el resultado deseado.Para invertir un registro, ejecutamos XOR contra un registro que tenga todos los bits activos (XMM255):

#+begin_src asm
    PXOR XMMaux, XMM255
#+end_src

Para iniciar los registros XMMnulo y XMM255 no se requieren constantes en memoria. Basta con usar de nuevo instrucciones lógicas:

#+begin_src asm
    PXOR XMM255, XMM255
#+end_src

Como cualquier registro independientemente de su contenido inicial es "igual a sí mismo", la comparación activará el registro por completo.

#+begin_src asm
    PXOR XMMnulo, XMMnulo
#+end_src

Como XOR requiere uno y sólo uno de los bits de entrada activos para activar la salida y como de nuevo el registro es "igual a sí mismo", el registro se anulará todos sus bits.

Una vez que tenemos en XMMaux cada byte a 255 ó 0 dependiendo del estado del bit correspondiente a cada una de las 16 neuronas de entrada procesadas, podemos desechar los pesos que no deban sumarse con un simple AND:

#+begin_src asm
    PAND XMMaux, XMMpesos
#+end_src

En la figura \ref{mascaraBinariaXMM} se trata de ilustrar la forma de acceso a los bits individuales. 

#+CAPTION:    Ejemplo ilustrativo del acceso paralelo a los bits individuales.
#+LABEL:      mascaraBinariaXMM
#+ATTR_LaTeX: width=\textwidth
[[./img/ejemploXMM.jpg]]

Todavía tenemos que sumar los pesos entre sí y acumularlos. Este es el paso que consigue una mayor mejora en las optimizaciones binaria y bipolar con respecto a la flotante. Aunque no hay ninguna instrucción que nos permita sumar todos los bytes de un registro XMM directamente, existe otra que nos es muy útil porque hace una reducción similar. Se trata de PSADBW. Con registros MMX (de 64 bits en vez de 128), calcula la diferencia absoluta entre los bytes de cada registro operando y suma todas esas diferencias, dejando el resultado en los 4 bytes bajos del registro MMX. Con registros XMM, opera de forma similar pero dejando dos resultados: uno en la los 4 bytes bajos de los 8 bajos y otro en los 4 bajos  de los  8 altos. Es decir, duplica la operación. 

#+begin_src asm
    PSADBW XMMaux, XMMnulo
#+end_src

Si uno de los operandos es un registro nulo, la diferencia absoluta entre 0 y un número siempre es ese mismo número, por lo que simplemente sumara los bytes. Surge aquí un pequeño problema dado que suma los bytes sin tener en cuenta su signo, como si todos fueran positivos. Cómo queriamos los pesos pertenecientes a [­128, 127], debemos hacer algo al respecto.

Antes de ejecutar la instrucción anterior, ejecutaremos: 

#+begin_src asm
    PAND XMMaux128, XMMaux
#+end_src

Y así tendremos 128 en los bytes cuyos bits estaban activos. Después, en lugar de sólo una instrucción de reducción, ejecutamos:

#+begin_src asm
    PSADBW XMMaux, XMMnulo
    PSADBW XMMaux128, XMMnulo
    PSUBD XMMaux, XMM128
#+end_src

Esto equivale a restarle 128 a cada byte que fuesemos a sumar, porque se suman con PSADBW tantos 128 como pesos haya. Hay que tener en cuenta que los pesos pueden ser [­128, 127] pero no equivalen, por ejemplo, a los char de C++. En C++, los números se representan en complemento a dos mientras que en nuestra representación alternativa el 0 es el ­128, el 128 es el 0, el 129 el 1, etc. Realmente no es importante, siempre y cuando lo tengamos presente. Ya sólo queda sumar las dos partes. Después, se repite el proceso hasta completar los 8 bits por bytes, cargando cada vez 16 pesos nuevos. Luego se reinicia la máscara, se lee el siguiente bloquede entrada y se repite todo hasta que hayamos completado en número de bloques.

Al final, hay que sumar las dos partes (alta y baja) que se están acumulando en un registro XMM. Esto se omite, como la gestión del bucle, porque no tiene demasiado interés en lo que a nuestros esfuerzos de optimización se refiere.

La explicación que se ha dado se refería al algoritmo para neuronas binarias, que pueden tomar los valores {0, 1}. Para las neuronas bipolares que pueden tomar los valores {-1, 1}, el código es bastante similar, aunque ligeramnte más complicado. En este caso, todos los pesos se utilizan, simplemente unos cambian su signo y otros no. Ahora, cuando invertimos XMMaux, también conservamos el original y también lo operamos con AND con los pesos. El resultado son los pesos que tendrán que ser restados en vez de sumados. También lo operamos el registro auxiliar invertido con el XMM128, pues por cada peso restado se tendrá que sumar 128 (en vez de restarlo). Por ejemplo, 129 es sólo 1 en nuestra representación, por tanto, para restar 1 (restar un peso igual a uno), restamos 129 y sumamos 128.

Recordamos resumadiamente lo que hacíamos en el núecleo de la versión binaria para luego señalar las diferencias.

#+begin_src asm
	MOVDQA XMMaux, XMMmascara      ;copiamos la máscara en una mascara auxiliar

	PAND XMMaux, XMMentradas       ;obtenemos el valor del bit a procesar en cada byte
    PCMPEQB XMMaux, XMMnulo        ;si el bit estaba activo->se pone a 0 todo el byte, 
                                   ;si no-> se pone a 1 todo el byte (255)
	PCMPEQB XMM255, XMM255         ;ponemos 255 en todos los byes del registro XMM255
    PXOR XMMaux, XMM255            ;invertimos XMMaux 
                                   ;(ahora hay 255 en los bytes que tenian el bit que tocaba activo)

	MOVDQU XMMaux128, XMM128       ;128 en todos los bytes de XMMaux128
	PAND XMMaux128, XMMaux         ;128 sólo en los bytes que estaban activos

	MOVDQU XMMpesos, [ptrPesos]    ;leemos el bloque actual de pesos
	PAND XMMaux, XMMpesos          ;asi tenemos el peso de cada conexión 
                                   ;solamente en los bytes con el bit activo

	PSADBW XMMaux, XMMnulo         ;sumamos todos los bytes (los que estaban activos)
	PSADBW XMMaux128, XMMnulo      ;sumamos 128 por cada byte que estaba activo

	PADDD XMMacumulador, XMMaux    ;sumamos estos pesos a los ya sumados previamente
	PSUBD XMMacumulador, XMMaux128 ;sustraemos 128 por cada bit que estaba activo
#+end_src

En el caso bipolar se hacen más cálculos. Además de los dos primeros, como en el caso binario, para el caso bipolar se hacen los dos últimos cálculos descritos en esta lista:

1) Se suman todos los pesos de las neuronas activas
2) Se resta 128 por cada neurona activa
3) Se restan todos los pesos de las neuronas inactivas
4) Se suma 128 por cada neurona inactiva

Como ahora no tenemos que desechar ningún peso, sino sumar unos y restar otros, el código quedaría así:

#+begin_src asm
	MOVDQA XMMaux, XMMmascara      ;copiamos la máscara en una mascara auxiliar

	PAND XMMaux, XMMentradas       ;obtenemos el valor del bit a procesar en cada byte
    PCMPEQB XMMaux, XMMnulo        ;si el bit estaba activo->se pone a 0 todo el byte,
                                   ;si no-> se pone a 1 todo el byte (255)
	PCMPEQB XMMauxInv, XMMauxInv   ;ponemos 255 en todos los byes del registro XMMauxInv
    PXOR XMMauxInv, XMMaux         ;ponemos el inverso de XMMaux en XMMauxInv

	MOVDQU XMMaux128, XMM128       ;128 en todos los bytes de XMMaux128
	PAND XMMaux128, XMMauxInv      ;128 sólo en los bytes que estaban activos
	PSADBW XMMaux128, XMMnulo      ;sumamos 128 por cada byte que estaba activo
	PSUBD XMMacumulador, XMMaux128 ;sustraemos 128 por cada bit que estaba activo

	MOVDQU XMMaux128, XMM128       ;128 en todos los bytes de XMMaux128
	PAND XMMaux128, XMMaux         ;128 sólo en los bytes que estaban inactivos
	PSADBW XMMaux128, XMMnulo      ;sumamos 128 por cada byte que estaba activo
	PADDD XMMacumulador, XMMaux128 ;sumamos 128 por cada bit que estaba inactivo

	MOVDQU XMMpesos, [ptrPesos]    ;leemos el bloque actual de pesos
	PAND XMMauxInv, XMMpesos       ;asi tenemos el peso de cada conexión 
                                   ;solamente en los bytes con el bit activo
	PAND XMMaux, XMMpesos          ;asi tenemos el peso de cada conexión 
                                   ;solamente en los bytes con el bit inactivo

	PSADBW XMMauxInv, XMMnulo      ;sumamos todos los bytes (los que estaban activos)
	PSADBW XMMaux, XMMnulo         ;sumamos todos los bytes (los que estaban inactivos)

	PADDD XMMacumulador, XMMauxInv ;sumamos los pesos "positivos" al acumulador
	PSUBD XMMacumulador, XMMaux    ;sustraemos los pesos "negativos" al acumulador
#+end_src

Todavía se podrían mejorar las soluciones si contásemos con la arquitectura de 64 bits. En tal caso tendríamos 16 registros XMM en lugar de sólo 8, no habría que reusar tanto los registros y algunos trucos (como los de poner a 255 ó a 0 todo un registro) podrían realizarse solamente una vez al principio en vez de cada vez que necesitamos alguno de estos valores en un registro que usamos para multiples cosas. Hemos optado por la compilación para 32 bits por su mayor portabilidad. En los sistemas operativos de 64 bits se puede simular la arquitectura de 32 bits y ejecutar nuestra optimización. No sucede lo mismo al contrario: si hubiesemos optado por la implementación de 64 bits no podríamos ejecutar la optimización sobre un sistema operativo de 32 bits.

Como hemos dicho, la colocación de los bits para la implementación XMM debe adaptarse para que se puedan obtener los mismos resultados que con el algoritmo implementado en C. Pero además en el algoritmo C no se puede usar el tipo char para los pesos (hay que usar unsigned char) y hay que restarles 128 antes de operar con ellos. Esto podría ralentizar "injustamente" al algoritmo C, por lo que también se hicieron pruebas de rendimiento sin restar 128 y usando el tipo char para comparar los tiempos. Logicamente, esa implementación C no obtiene resultados equivalentes a los de la SSE2, pero tan sólo se pretendía comparar el rendimiento. Sorprendentemente, con esta implementación C se obtenían resultados aún peores. También se probó usadno el tipo unsigned char pero sin restar 128 y el rendimiento era de nuevo ligeramente peor. Por alguna razón que no alcanzamos a explicar, el algoritmo en C funciona más rápido si ha de restar 128 a cada peso. Por ello, dejamos de lado nuestra preocupación sobre la posible penalización causada por nuestra representación de pesos en bytes.

\newpage
** GPGPU con CUDA
#+LaTeX: \label{disenoParalCUDA}

Debido a la insaciable demanda de mercado de gráficos 3D de alta definición y en tiempo real, las unidades de procesamiento gráfico (Graphic Processor Unit, GPU) han evolucionado en procesadores altamente paralelos y multihilo, con muchos núcleos, tremenda capacidad de computación y con gran ancho de banda de memoria. La técnica consistente en utilizar el este poder computacional para realizar trabajos de proposito general, que pueden no tener nada que ver con los gráficos se denomina GPGPU (General Purpose Graphic Processor Unit). Los pioneros de la técnica buscaban homorfismos entre los algoritmos que pretendían ejecutar y cálculos que las librerías gráficas realizan internamente. 

Gracias a los lenguajes de alto nivel especializados para GPGPU como C CUDA u OpenCL, ya no es necesario modelar tu problema utilizando conceptos puramente gráficos como superficies y texturas. Sin embargo, para poder aprovechar las máximas posibilidades de rendimiento es necesario conocer la arquitectura de las unidades de procesamiento gráfico y así como los cuellos de botella que potencialmente puedan perjudicar a la optimización de nuestro algoritmo.

Aunque otras arquitecturas gráficas puedan ser similares en muchos aspectos, describiremos los conceptos básicos de la arquitectura CUDA, que es la que hemos utilizado para paralelizar los cálculos de estado de las redes neuronales y que fue diseñada explícitamente para soportar GPGPU incluso desde lenguajes de alto nivel. En principio C, pero luego también otros leguajes como FORTRAN, C++ y OpenCL. Desde la serie NVIDIA GeForce 8000 todas las gráficas que ha producido NVIDIA obedecen a la arquitectura básica CUDA (excepto las específicas para dispositivos móviles, que siguen la aruitectura Tegra). Aunque tarjetas posteriores ofrecen nuevas capacidades y posibilidades de ajuste de los algoritmos, son retrocompatibles con respecto al código implementado para versiones anteriores.

Por simplicidad, los trozos de código mostrados tratarán exclusivamente la versión real (float) de las neuronas, sin mostrar ni explicar las complejidades adicionales de las versiones binaria y bipolar.

*** Modelo de programación
#+LaTeX: \label{disenoParalCUDAprog}

C para CUDA es una extensión de C que permite al programador definir funciones, llamadas kernels (núcleos) que cuando son llamadas se ejecutan N  veces por N hilos CUDA diferentes, en vez de una sola vez como las funciones C habituales\cite{progGuide2009}. Para definir un kernel se usa el especificador de declaración =__global__= y el número de hilos CUDA para cada llamada se especifica con la nueva sintaxis \verb=<<<=...\verb=>>>= del siguiente ejemplo:

#+begin_src c
// Definicion del Nucleo
__global__ void MiKernel(float* A, float* B, float* C)
{
    ...
}
int main()
{
    ...
    // Invocacion del Nucleo
    MiKernel<<<1, N>>>(A, B, C);
}
#+end_src

A cada uno de los hilos que ejecuta el kernel se le da un identificador de hilo único que es accesible desde el kernel con la variable interna =threadIdx=. El siguiente código de ejemplo suma dos vectores A y B de tamaño N y guarda el resultado en el vector C:

#+begin_src c
// Definicion del Nucleo
__global__ void SumaVectores(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}
int main()
{
    ...
    // Invocacion del Nucleo
    SumaVectores<<<1, N>>>(A, B, C);
}
#+end_src

Cada uno de los hilos que ejecuta SumaVectores() realiza la suma de un par de elementos diferente.

Por conveniencia, threadIdx es un vector de tres componenetes para que los hilos puedan ser identificados usando un índice de una, dos o tres dimensiones, formando bloques de hilos unidimensionales, bidimensioneales o tridimensionales. Como ejemplo, el siguiente código suma los elementos de las matrices A y B de tamaño NxN y almacena el resultado en la matriz C:

#+begin_src c
// Definicion del Nucleo
__global__ void SumarMatriz(float A[N][N], float B[N][N], float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}
int main()
{
    ...
    // Invocacion del Nucleo
    dim3 dimBlock(N, N);
    SumarMatriz<<<1, dimBlock>>>(A, B, C);
}
#+end_src

El índice del hilo y su ID se relacionan de manera directa: para un bloque unidimensional, son iguales; para un bloque bidimensional de tamaño (Dx, Dy), el ID del hilo en con índice (x, y) es (x + y Dx); para uno tridimensional de tamaño (Dx, Dy, Dz), el ID del hilo con índice (x, y, z) es (x + y Dx + z Dx Dy).

Los hilos dentro de un mismo bloque pueden cooperar entre ellos compartiendo datos a través de la memoria compartida y sincronizando su ejecución para coordinar el acceso a memoria. Para ser más precisos, uno puede especificar puntos de sincronización en el kernel llamando a la función interna =__syncthreads()= que actua como una barrera que hace esperar a todos los hilos del bloque antes de que ninguno pueda seguir. 

Para una cooperación eficiente, se espera que la memoria compartida sea de baja latencia y cercana al núcleo del procesador, como una caché de primer nivel, también que =__syncthreads()= sea ligera y todos los hilos de un bloque deben estar en el mismo núcleo de procesamiento. Por ello el número de hilos por bloque está restringido por los recursos limitados de memoria de un núcleo de procesamiento. En GPUs actuales un bloque de hilos puede contener hasta 512 hilos.

Se pueden conseguir mejores resultados para un determinado algoritmo en determinada máquina ajustando el número de hilos por bloque. Los recursos de un multiprocesador se reparten entre los hilos del bloque. Por ejemplo, cuantos más hilos por bloque, menos registros tendrá disponibles cada hilo. Dependiendo del algoritmo, corremos el riesgo de que algunas variables locales tengan que almacenarse en memoria global en vez de en registros si lanzamos muchos hilos por bloque. Si elegimos pocos hilos por bloque, nos arriesgamos a no utilizar todos los procesadores de un multiprocesador (aunque a menudo este segundo problema lo resuelve automáticamente la arquitectura).

Sin embargo, un kernel puede ser ejecutado por múltiples bloques de hilos similares, de forma que el número total de hilos sea igual al número de hilos por bloque multiplicado por el número de bloques. Estos múltiples bloques se organizan en grids unidimensionales o bidimensionales de bloques de hilos como se muestra en la figura \ref{figGridThreadBlocks}. 

#+CAPTION:    Grid de bloques de hilos.
#+LABEL:      figGridThreadBlocks
#+ATTR_LaTeX: scale=0.6
[[./img/gridBlockThreads.jpg]]

La dimensiones del grid se especifica con el primer parámetro específico del kernel entre la sintaxis \verb=<<<=...\verb=>>>=. Cada bloque dentro del grid se puede identicar con un índice unidimensional o bidimensional a través de la variable interna blockIdx. Las dimensiones de el bloque de hilos es accesible desde el kernel usando la variable interna blockDim. El código de ejemplo anterior quedaría así:

#+begin_src c
// Definicion del Nucleo
__global__ void SumarMatriz(float A[N][N], float B[N][N], float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}
int main()
{
    ...
    // Invocacion del Nucleo
    dim3 dimBlock(16, 16);
    dim3 dimGrid((N + dimBlock.x – 1) / dimBlock.x,
                 (N + dimBlock.y – 1) / dimBlock.y);
    SumarMatriz<<<dimGrid, dimBlock>>>(A, B, C);
}
#+end_src

El bloque de tamaño 16x16 = 256 se ha cogido algo arbitrariamente, y el grid se crea con suficientes bloques para tener un hilo por elemento de la matriz como antes. 

Los bloques de hilos deben poder ejecutarse independientemente, en cualquier orden, en paralelo o en serie. Este requerimiento de independencia hace que los bloques de hilos se puedan planificar en cualquier orden en cualquier número de núcleos, permitiendo a los programadores escribir código que escala para cualquier número de núcleos. El número de bloques en un grid es típicamente dictado por el tamaño de los datos a procesar en lugar del número de procesadores en el sistema, al que puede superar ampliamente.

Además de la jerarquía para la organización de los hilos descrita existe una jerarquía de memorias a cuyos diferentes espacios pueden acceder los hilos CUDA como muestra la figura \ref{cudaMemory}. Cada hilo tiene una memoria privada local. Cada bloque de hilos tiene uuna memoria compartida visible para todos los hilos del bloque y cuyos datos se mantienen por lo que dure el procesamiento del bloque. Finalmente, todos los hilos de todos los bloques tienen acceso a la misma memoria global. 

#+CAPTION:    Modelo de jerarquías de memoria CUDA.
#+LABEL:      cudaMemory
#+ATTR_LaTeX: scale=0.7
[[./img/memDrDobb.jpg]]

También hay dos espacios de memoria adicionales de sólo lectura que accesibles por todos los hilos: los espacios de memoria constantes y texturas. La memoria global, la de constantes y la de texturas pueden ser optimizadas para diferentes usos de memoria y son consistentes entre llamadas a kernels de la misma aplicación.

El modelo de programación CUDA asume que los hilos se ejecutarán en un dispositivo físicamente separado que opera como coprocesador de un programa C anfitrión. Los kernels se ejecutan en la GPU y el resto del programa C se ejecuta en una CPU anfitrión. También asume que tanto el anfitrión como el dispositivo mantienen su propia DRAM, llamadas memoria anfitrión y memoria de dispositivo, respectivamente.

Por tanto, un programa gestiona los espacios de memoria global, de constantes y de texturas visibles a los kernels a través de llamadas a la librería CUDA runtime. Esto incluye reserva, liberación de memoria y transferencia entre las memorias anfitrión y de dispositivo.

Ademas, se puede hacer un sistema anfitrión con varias GPUs y hacer llamadas de kernels con diferentes datos y configuraciones a cada uno de los dispoitivos. En el presente proyecto, sin embargo, se prescinde de esa posibilidad, así como de usar las memorias de constantes y de texturas.

*** Arquitectura CUDA
#+LaTeX: \label{disenoParalCUDAarq}

La arquitectura CUDA se construye alrededor de un conjunto escalable de Multiprocesadoes de flujo (Streaming Multiprocessors, SMs). Cuando un programa CUDA en la CPU anfitrión invoca a un grid de kernel, los bloques del grid son enumerados y distribuidos a los multiprocesadores con capacidad de ejecución como se muestra en la figura \ref{automaticScalability}. Los hilos de un bloque se ejecutan concurrentemente en un mismo multiprocesador. Cuando los bloques terminan, nuevos bloques son lanzados en los Multiprocesadores que queden libres.

#+CAPTION:    Escalabilidad automática: un dispositivo con más mutliprocesadores ejecutará un grid automáticamente más rápido que un dispositivo con menos multiprocesadores.
#+LABEL:      automaticScalability
#+ATTR_LaTeX: scale=0.6
[[./img/automaticScalability.jpg]]

Un multiprocesador consta de 8 núcleos que son procesadores escalares (Scalar Processors, SP), dos uunidades de función especial para trascendentales, una unidad de intrucciones multi-hilo, y una memoria compartida interna. El multiprocesador crea, gestiona y ejecuta los hilos concurrentes en un hardware con sobrecarga de planificación de ejecución nula. Implementa la barrera de sincronización intrínseca =__syncthreads()= con una sola instrucción. La rápida barrera de sincronización junto con la ligera creación de hilos y la sobrecarga nula de planificación soporta eficientemente un paralelismo muy granulado, permitiendo, por ejemplo, una descomposición con baja granulidad de problemas asignando un hilo a cada elemento de datos (como un pixel en una imagen, una celda en un calculo basado en una rejilla [grid] o una neurona de salida en una capa de una red neuronal).

Para gestionar cientos de hilos corriendo varios programas diferentes, el multiprocesador emplea una nueva arquitectura llamada SIMT (Single-instruction, multiple-trhead; una instrucción, múltiples hilos). El multiprocesador mapea cada hilo en un núcleo de procesamiento escalar, y cada hilo escalar se ejecuta independientemente con su propia dirección de instrucción y registro de estado. El multiprocesador SIMT crea, gestiona, planifica y ejecuta hilos en grupos de 32 hilos paralelos llamados warps (Termino originado en Weaving, la primera tecnología de hilos paralelos). Los hilos individuales que componen un warp SIMT empiezan juntos en la misma dirección de programa, pero son libres de divergir y seguir una ejecución independiente.

Cuando un multiprocesador recibe uno o más bloques de hilos para ejecutar, los divide en warps que son programados por la unidad SIMT. El modo en que un bloque es dividido en warps es siempre el mismo; cada warp contiene hilos con IDs consecutivos y crecientes con el primer warp conteniendo el hilo 0. 

Por cada tiempo de ejecución de intrucción, la unidad SIMT selecciona un warp que esté preparado para ser ejecutado y les da la siguiente instrucción a los hilos activos del warp. El warp ejecuta una instrucción común cada vez, por lo que la máxima eficiencia se alcanza cuando los 32 hilos del warp siguen el mismo flujo de ejecución. Si los hilos que divergen debido a una bifurcación condicional dependiente de los datos, el warp ejecuta de forma seliarizada cada camino de ejecución, desactivando los hilos que no están en ese camuno. Cuando todos los caminos se completan, los hilos convergen de nuevo en el mismo flujo de ejecución. Estas bifucaciones ocurren sólo dentro de un mismo warp; los diferentes warps se ejecutan independientemente sin importar si estos están ejecutando caminos de código comunes o disjuntos.

La arquitectura SIMT se parece a las organizaciones vectoriales SIMD (Single Instruction, Multiple Data) en que una sola instrucción controla multiples elementos de procesamiento. Una diferencia clave entre es que las arquitecturas vectoriales SIMD exponen el tamaño del vector SIMD al software, mientras que las instrucciones SIMT especifican la ejecución y el comportamiento de las bifurcaciones para un solo hilo. A diferencia de las maquinas vectoriales SIMD, SIMT permite a los programadores escribir código a nivel de hilo para hilos independientes y escalares, así como código con paralelismo de datos para hilos coordinados. En lo que se refiere a la corrección del código, el programador puede basicamente ignorar el comportamiento SIMT; sin embargo, se pueden conseguir mejoras de rendimiento sustanciales teniendo en cuenta que el código puede hacer que los hilos de un warp se bifurquen y evitándolo en la medida de lo posible. En la práctica, esto es similar al rol de las lineas de cache en el código tradicional: el tamaño de las lineas de cache se puede ignorar de forma segura cuando se diseña para que el código sea correcto pero debe ser considerado en la estructura del código cuando se diseña para un rendimiento máximo. Las arquitecturas vectoriales, sin embargo, requieren que el software haga las cargas coalescentes y gestione las divergencias manualmente.

Como se muestra en la figura \ref{arqCUDAdetalle}, cada mutliprocesador tiene memoria interna de 4 tipos:

- Un conjunto de registros locales de 32 bits por procesador.
- Una cache de datos paralelos o memoria compartida que es compartida por todos los procesadores escalares.
- Una cache de constantes de sólo lectura que es compartida por todos los procesadores escalares y acelera las lecturas desde el espacio de memoria de constantes, que es una región de sólo lectura de la memoria del dispositivo.
- Una cache de texturas de sólo lectura que es compartida por todos los procesadores escalares y acelera las lecturas desde el espacio de memoria de texturas, que es una región de sólo lectura de la memoria del dispositivo; cada multiprocesador accede a la cache de texturas a través de la unidedad de texturas que implementa los diferentes modos de direccionamiento y filtrado de datos especiales para texturas.

#+CAPTION:    Un conjunto de multiprocesadores SIMT con memoria compartida en el chip.
#+LABEL:      arqCUDAdetalle
#+ATTR_LaTeX: scale=0.6
[[./img/arqCUDAdetalle.jpg]]

Los espacios de memoria locales y globales son regiones de lectura/escritura de la momoria del dispositivo y no son cacheados. 
El número de bloques que un multiprocesador puede procesar a la vez - lo que se denomina como número de bloques activos por multiprocesador - depende de cuantos registros por hilo y cuánta memoria compartida por bloque son necesarios para un kernel dado, debido a que los registros de un multiprocesador y la momoria compartida son divididos entre todos los hilos de los bloques activos. Si no hay suficientes registros o memoria compartida disponibles por multiprocesador para procesar al menos un bloque, el kernel no podrá ser lanzado. El número máximo de bloques activos por multiprocesador, así como él número máximo de warps activos y el número de hilos activos dependen del dispositivo CUDA concreto.

Si una instrucción no atómica ejecutada en un warp escribe en la misma localización global de momoria compartida para más de un hilo dentro de warp, el número de escrituras serializadas que suceden en esa localización y el orden en que ocurren no está definido, pero se garantiza que al menos una de las escrituras se realizará. Si una instrucción atómica ejecutada por un warp lee, modifica o escribe sobre la misma localización en momodia global para más de uno de los hilos del warp, todas las lecturas, modificaciones y escrituras sobre esa localización se realizan y son todas serializadas, epro el orden en que ocurren tampoco está definido.
*** Reducción paralelizada
#+LaTeX: \label{disenoParalCUDAreduc}

La primera aproximación para la implementación de nuestro algoritmo genérico de cálculo de estado de una capa de red neuronal sobre la arquitectura CUDA estaba claramente influída por la optimización SSE2 anterior. Aquella función de ensamblador calculaba el resultado para una neurona de salida. En este primer kernel CUDA haremos lo mismo: cada llamada al kernel nos servirá para calcular la salida de un neurona (de nuevo, dejando la función de activación aparte). De este modo, cada hilo hará una multiplicación entre una entrada y un peso y luego se sumarán todos los resultados usando una reducción paralela.

En la sección \ref{disenoParalXMMbyte} utilizamos una instrucción especial para hacer la reducción y poder sumar los 16 elementos de un bloque, pero no existe una instrucción similar en CUDA. En \ref{disenoParalXMMfloat} se acumulaban los resultados en un registro con cuatro floats que se sumaban al final. El modo en que se sumaban no era lo que más aceleraba al algortimo, pero nos da una pista de lo que debemos hacer. Primero se sumaban dos y dos en una sóla instrucción (tras dduplicar el registro y desplazarlo) y después los otros dos resultantes. Generalizando a cualquier número de elementos iniciales y sumando los elementos de dos en dos de forma paralela, vemos que lo que tenemos que usar es una reducción en árbol\cite[Harris2007]{Harris2007}.

Pero queremos que el algortimo sirva para un tamaño de entrada cualquiera y como hemos visto, dividiremos los datos en un grid también de tamaño arbitrario compuesto de bloques de hilos cuyo tamaño sí depende de la máquina concreta. Este tamaño lo podremos ajustar a cada dispositivo con pruebas, pero surge un problema. No existe un método de sincronización global entre los hilos de los diferentes bloques en CUDA, sólo uno para sincronizar los hilos de un mismo bloque. Para superarlo analizaremos dos posibilidades. 

La primera es tener dos kernels y ejecutar el segundo cuantas veces sea necesario. El primer kernel multiplicará cada entrada por su peso correspondiente y hará la primera reducción. Cada bloque de hilos almacenará el resultado parcial de la reducción que ha realizado en la memoria global del dispositivo. El segundo kernel hará sólo reducción, sin multiplicar entradas ni pesos. Éste tomará como entrada las salidas de la ejecución anterior y se llamará tantas veces como sea necesario hasta que se llame al kernel con un grid de un sólo bloque de hilos que pueda sumar todo en una sola variable. Aunque la sobrecarga de tiempo que el hardware tarda en lanzar un kernel es despreciable y la sobrecarga que generaremos mediante software para configurar correctamente las sucesivas llamadas a los diferentes kernels parece en principio asumible, vemos que esta aproximación tiene otra gran desventaja. En cada ejecución de bloque, tras el cálculo inicial de la entrada por el peso, la primera reducción la realizan tan sólo la mitad de los hilos, la otra mitad están parados casi desde el principio, lo que sin duda parece un desperdicio. Además, cuando queden pocos resultados parciales el número de bloques por grid será también pequeño y si el dispositivo tiene muchos multiprocesadores los estaremos también infrautilizando.

La segunda posibilidad a analizar es renunciar a hacer una llamada al kernel por cada neurona de salida y, en vez de ello, permitir que cada bloque se encargue de una neurona de salida, contando el grid con tantos bloques como neuronas de salida tenga la capa de la red que esté siendo procesada. Siempre que el número de multiprocesadores de un dispositivo sea inferior al número de neuronas de salida (algo bastante razonable) no debemos temer el último problema de infrautilización comentado para la primera posibilidad. Además, también se desvanece nuestra preocupación de que los hilos de cada bloque queden demasiado pronto inactivos. Todas las lecturas de entradas y pesos, y los cálculos y acumulaciones correspondientes se dividirán entre todos los hilos del bloque destinado a calcular el valor de esa salida. De esta manera mantendremos todos los hilos de un bloque ocupados hasta la reducción final de los resultados parciales de cada hilo.

Elegimos, por tanto, esta segunda posibilidad, que consiste en realidad en combinar la reducción paralela con la secuencial (en varios hilos). No por ello dejaremos de explorar las posibilidades de optimización para la reducción paralela que sí se hará para reunir todos los resultados parciales de los diferentes hilos de un mismo bloque. Antes de analizar esas posibles optimizaciones, veamos como sería nuestro kernel sin tener en cuenta esas optimizaciones en la reducción (y, como hemos dicho anteriormente, sin contemplar neuronas binarias ni bipolares). Nuestro kernel tomará por entradas el número de neuronas de entrada (=input_size=) de la capa, al vector con las neuronas de entrada (=inputPtr=) y la matriz con los pesos (=weighs=). Como parámetro de salida tendrá el vector con las neuronas de salida listas para que se les aplique la función de activación (=results=).
Este sería el código que implementa el kernel:

#+begin_src c
__global__ void ReductionKernel(float* inputPtr, unsigned input_size, 
                    float* weighs, float* results)
{
    extern __shared__ float sdata[];

    unsigned weighsOffset = (blockIdx.x * input_size);

    float result = 0;
    unsigned i = threadIdx.x;

    while (i < input_size) {
        result += inputPtr[i] * weighs[weighsOffset + i];
        i += blockDim.x;
    }
    __syncthreads();

    unsigned tid = threadIdx.x;
    sdata[tid] = result;
    __syncthreads();

    // Reducción paralela de los resultados parciales en memoria compartida
    // ...

    if (tid == 0) {
        results[blockIdx.x] += sdata[0];
    }
}
#+end_src

Los pesos están ordenados de forma que todos los que correspondan a una misma neurona de salida estén juntos. Por ello, cada bloque calcula un desplazamiento en la matriz de pesos (=weighsOffset=) multiplicando su id de bloque por el tamaño de las entradas para acceder a los pesos que corresponden a la neurona de salida que le toca calcular. Hasta que se acaben las entradas, cada hilo tomará una entrada y la multiplicará por su peso correspondiente, acumulando su resultado parcial en una variable local (=result=) que empezará a cero. El iterador =i= que es diferente para cada hilo del bloque se irá incrementando con el tamaño del bloque, para que no se repitan los accesos a una misma entrada ni un mismo bloque. Tras esto, cada hilo almacena su resultado parcial en la memoria compartida que es donde se realizará la reducción paralela y sólo uno de ellos escribirá el resultado todal desde la memoria compartida a la memoria global del dispositivo, donde se almacenan las salidas de la capa.

Detengamonos ahora a analizar la parte de la reducción paralelizada y sus posibles optimizaciones. Una primera aproximación sería un acceso entrelazado como el del siguiente código:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=1; s < blockDim.x; s *= 2) 
    {
        if (tid % (2*s) == 0) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    // ...
#+end_src

Esta aproximación, sin embargo tiene defectos. En primer lugar, el operador módulo (=%=) es muy lento. Pero eso no es lo peor. Como vimos en la sección \ref{disenoParalCUDAarq}, además de los bloques hay una unidad menor llamada warp compuesta de 32 hilos y si todos los hilos del warp no siguen el mismo flujo de ejecución sufrimos una penalización importante en redimiento. Como los hilos de un mismo warp tienen ids consecutivos, vemos que la condición =if= del interior del bucle producirá warps altamente divergentes, lo que a su vez es altamente ineficiente.
Podemos mantener el mismo acceso entrelazado pero desde hilos consecutivos reemplazando el código anterior con el siguiente, con indexación por saltos:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=1; s < blockDim.x; s *= 2) 
    {
        int index = 2 * s * tid;

        if (index < blockDim.x) 
        {
            sdata[index] += sdata[index + s];
        }
        __syncthreads();
    }
    // ...
}
#+end_src

Esta condición =if= no produce ejecuciones divergentes en un mismo warp tan a menudo, sólo en las fases finales de la reducción (los 32 últimos elementos). Sin embargo, nos encontramos con un nuevo problema y es que los bancos de la memoria copartida no están diseñados para acceder a ellos de esta manera. Si dos hilos acceden de forma conflictiva a posiciones de la memoria compartida que se encuentran en el mismo banco, se produce un conflicto de bancos y se accederá a las dos posiciones secuencialmente, perdiendo las ventajas del acceso paralelo. Una excepción a esto sería que todos los hilos accediesen simultáneamente al mismo elemento, entonces en vez de obtener un conflicto simplemente utilizará una instrucción de broadcast (el número de hilos que tienen que acceder simultaneamente para usar el broadcast dependen de la generación del dispositivo, cuanto más nuevos, más permisivos).

Hay 16 bancos en la memoria compartida (32 en generaciones más nuevas, Fermi), que están entrelazados con una granularidad de 32 bits. Así, las direcciones de memoria estarían situadas en bancos de la siguiente manera:

| Banco       | 0           | 1           | ... | 15              |
|-------------+-------------+-------------+-----+-----------------|
| Direcciones | 0  1  2  3  | 4  5  6  7  | ... | 60  61  62  63  |
| Direcciones | 64 65 66 67 | 68 69 70 71 | ... | 124 125 126 127 |
| ...         | ...         | ...         | ... | ...             |

Floats con posiciones consecutivas en la memoria compartida pertenecen a distintos bancos y si dos hilos del mismo medio-warp (halfwarp) acceden al mismo banco a la vez se producirá el conflicto. En nuestro caso, es el propio acceso entrelazado el que nos está causando los problemas, por lo que nos desharemos de él en la siguiente aproximación con acceso secuencial:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=blockDim.x/2; s>0; s>>=1) 
    {
        if (tid < s) 
        {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    // ...
}
#+end_src

Hemos invertido el bucle y basado la indexación en el id del hilo. Así, además de una condición que no produce divergencias dentro de los warps, tenemos que hilos consucutivos acceden a posiciones de memoria compartida consecutivas. De nuevo, podemos mejorar esta parte del algortimo.

Conforme avanza la reducción, tenemos cada vez menos hilos activos en el bloque y cuando la s es menor o igual a 32, sólo nos queda un warp. Las instrucciones dentro de un warp se calculan de forma síncrona y vectorial (SIMD). Esto significa que cuando ~s <= 32~ no necesitamos llamar a =__syncthreads()=. Tampoco necesitamos hacer la comprobación ~if(tid < s)~, pues no ahorrará ningún cálculo. Por ello podemos desenrollar (loop unrolling) las 6 últimas iteraciones del bucle:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=blockDim.x/2; s>32; s>>=1) 
    {
        if (tid < s)
        {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid < 32)
    {
        sdata[tid] += sdata[tid + 32];
        sdata[tid] += sdata[tid + 16];
        sdata[tid] += sdata[tid +  8];
        sdata[tid] += sdata[tid +  4];
        sdata[tid] += sdata[tid +  2];
        sdata[tid] += sdata[tid +  1];
    }   
    // ...
}
#+end_src

Esto no sólo ahorra trabajo inútil en el último warp sino en todos ellos. Sin desenrollar, todos los warps siguen ejecutando las vueltas del bucle y las condiciones =if=.
NVIDIA dejó de mantener el modo emulado para el compilador nvcc a partir de la versión CUDA v2.3 (esta fué la última que lo incluía). Al tratarse de software privativo nadie pudo continuar con su mantenimiento y este modo ya no es útil en nuevas versiones. Sin embargo, dado que nuestro software está preparado para ser compilado en modo emulación usando la versión mencionada, estimamos oportuno precisar que el emulador no trata los warps de forma idéntica a los dispositivos y para obtener resultados correctos, hay que añadir unas indicaciones para el precompilador:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
#if __DEVICE_EMULATION__
    if (tid < 32) {sdata[tid] += sdata[tid + 32];} __syncthreads();
    if (tid < 16) {sdata[tid] += sdata[tid + 16];} __syncthreads();
    if (tid < 8) {sdata[tid] += sdata[tid + 8];} __syncthreads();
    if (tid < 4) {sdata[tid] += sdata[tid + 4];} __syncthreads();
    if (tid < 2) {sdata[tid] += sdata[tid + 2];} __syncthreads();
    if (tid < 1) {sdata[tid] += sdata[tid + 1];} __syncthreads();
#else
    if (tid < 32) {
        if (blockSize >= 64) sdata[tid] += sdata[tid + 32];
        if (blockSize >= 32) sdata[tid] += sdata[tid + 16];
        if (blockSize >= 16) sdata[tid] += sdata[tid + 8];
        if (blockSize >= 8) sdata[tid] += sdata[tid + 4];
        if (blockSize >= 4) sdata[tid] += sdata[tid + 2];
        if (blockSize >= 2) sdata[tid] += sdata[tid + 1];
    }
#endif
    // ...
}
#+end_src

El paso que hemos usado anteriormente consistente en desenrrollar bucles (loop unrolling), no es solamente útil para aprovechar las peculiaridades de la arquitectura CUDA. De forma general, esta técnica se usa para ahorrar la sobrecarga que suponen los cálculos que se ocupan de gestionar el bucle. Podemos obtener un mayor rendimiento desenrollando completamente el bucle. Como contrapartida obtendremos un código compilado más largo, pero centrándonos en el rendimiento no nos importa tener unos binarios más grandes.

Para poder hacer esto necesitaríamos saber el tamaño del bloque en tiempo de compilación y repetir el resto del código para los posibles tamños de bloque. Esta penosa tarea perjudicará además gravemente la legibilidad y mantenibilidad del código si no se cuenta con plantillas, una técnica que implementa el lenguaje C++ pero no el C original. Las primeras versiones de CUDA no soportaban C++, tan sólo C, sin embargo, las plantillas están disponibles desde el principio, también para C CUDA.

Así quedaría la parte de código dedicada a la reducción paralelizada con los bucles completamente desenrrollados usando plantillas:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    if (blockSize >= 512) { 
        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads();
    }
    if (blockSize >= 256) { 
        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); 
    }
    if (blockSize >= 128) { 
        if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); 
    }

#if __DEVICE_EMULATION__
    if (blockSize >= 64) {
        if (tid < 32) {sdata[tid] += sdata[tid + 32];} __syncthreads();
    }
    if (blockSize >= 32) {
        if (tid < 16) {sdata[tid] += sdata[tid + 16];} __syncthreads();
    }
    if (blockSize >= 16) {
        if (tid < 8) {sdata[tid] += sdata[tid + 8];} __syncthreads();
    }
    if (blockSize >= 8) {
        if (tid < 4) {sdata[tid] += sdata[tid + 4];} __syncthreads();
    }
    if (blockSize >= 4) {
        if (tid < 2) {sdata[tid] += sdata[tid + 2];} __syncthreads();
    }
    if (blockSize >= 2) {
        if (tid < 1) {sdata[tid] += sdata[tid + 1];} __syncthreads();
    }
#else
    if (tid < 32) {
        if (blockSize >= 64) sdata[tid] += sdata[tid + 32];
        if (blockSize >= 32) sdata[tid] += sdata[tid + 16];
        if (blockSize >= 16) sdata[tid] += sdata[tid + 8];
        if (blockSize >= 8) sdata[tid] += sdata[tid + 4];
        if (blockSize >= 4) sdata[tid] += sdata[tid + 2];
        if (blockSize >= 2) sdata[tid] += sdata[tid + 1];
    }
#endif
    // ...
}
#+end_src

Además, podemos sustituir la variable local =blockDim.x= por la constante =blockSize= cuando aparezca. El kernel completo final (sólo para neuronas reales) es este:

#+begin_src c
template <unsigned int blockSize>
__global__ void ReductionKernel(float* inputPtr, unsigned input_size, 
                    float* weighs, float* results)
{
    extern __shared__ float sdata[];

    unsigned weighsOffset = (blockIdx.x * input_size);

    float result = 0;
    unsigned i = threadIdx.x;

    while (i < input_size) {
        result += inputPtr[i] * weighs[weighsOffset + i];
        i += blockSize;
    }
    __syncthreads();

    unsigned tid = threadIdx.x;
    sdata[tid] = result;
    __syncthreads();

    // Reducción paralela de los resultados parciales en memoria compartida
    if (blockSize >= 512) { 
        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads();
    }
    if (blockSize >= 256) { 
        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); 
    }
    if (blockSize >= 128) { 
        if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); 
    }
    
#if __DEVICE_EMULATION__
    if (blockSize >= 64) {
        if (tid < 32) {sdata[tid] += sdata[tid + 32];} __syncthreads();
    }
    if (blockSize >= 32) {
        if (tid < 16) {sdata[tid] += sdata[tid + 16];} __syncthreads();
    }
    if (blockSize >= 16) {
        if (tid < 8) {sdata[tid] += sdata[tid + 8];} __syncthreads();
    }
    if (blockSize >= 8) {
        if (tid < 4) {sdata[tid] += sdata[tid + 4];} __syncthreads();
    }
    if (blockSize >= 4) {
        if (tid < 2) {sdata[tid] += sdata[tid + 2];} __syncthreads();
    }
    if (blockSize >= 2) {
        if (tid < 1) {sdata[tid] += sdata[tid + 1];} __syncthreads();
    }
#else
    if (tid < 32) {
        if (blockSize >= 64) sdata[tid] += sdata[tid + 32];
        if (blockSize >= 32) sdata[tid] += sdata[tid + 16];
        if (blockSize >= 16) sdata[tid] += sdata[tid + 8];
        if (blockSize >= 8) sdata[tid] += sdata[tid + 4];
        if (blockSize >= 4) sdata[tid] += sdata[tid + 2];
        if (blockSize >= 2) sdata[tid] += sdata[tid + 1];
    }
#endif
    if (tid == 0) {
        results[blockIdx.x] += sdata[0];
    }
}
#+end_src

Para no tener que decidir el tamaño del bloque en tiempo de ejecución y poder mantener =block_size= como parámetro, al llamar al kernel utilizamos una estructura =switch= con los 10 posibles valores:

#+begin_src c
unsigned grid_size = output_size;
unsigned shared_mem_size = block_size * sizeof(float);

switch (block_size) {
    case 512:
        ReductionKernel<512, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 256:
        ReductionKernel<256, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 128:
        ReductionKernel<128, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 64:
        ReductionKernel< 64, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 32:
        ReductionKernel< 32, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 16:
        ReductionKernel< 16, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 8:
        ReductionKernel<  8, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 4:
        ReductionKernel<  4, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 2:
        ReductionKernel<  2, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 1:
        ReductionKernel<  1, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
}
#+end_src

*** Paralelizar las salidas
#+LaTeX: \label{disenoParalCUDAsal}

En la sección \ref{disenoParalCUDAreduc} hemos realizado varias optimizaciones, pero no hemos usado al máximo la memoria compartida cuya lantencia es aproximadamente de 100 a 150 veces menor que la de la memoria global\cite[Farber2008]{Farber2008}. Tan sólo usamos un float por cada hilo del bloque, esto es, como máximo usamos 512 floats ó 512x4 = 2048 Bytes, cuando la memoria cmpartida tiene un tamaño de 16 KB (48KB para dispositivos de capacidad de computación 2.0 o superiores). A esto hay que descontar lo que ocupen los parámetros del kernel, que también se ubicarán en esta memoria, pues todos los hilos de un bloque deben poder acceder a ellos. Por tanto, tiene sentido buscar otra aproximación distinta de la de la reducción con la que se aproveche al máximo este espacio de memoria que sabemos es más rápido.

En esta aproximación, cada hilo calculará de forma completa una neurona de salida, en vez de hacerlo cada bloque como en la versión de reducción. De nuevo, todos los bloques tendrán que leer todas las entradas, pero ahora los hilos de un mismo bloque podrán usar la memoria compartida para almacenar estas entradas que en este caso deberán leer todos los hilos en vez de dividirselas. 
En una primera fase, cargaremos las entradas en la memoria compartida:

#+begin_src c
__global__
void OutputsKernel(float* inputs, unsigned input_size, unsigned output_size, float* weighs,
                                float* results)
{
    extern __shared__ float sdata[];

    unsigned pos = threadIdx.x;
    while (pos < input_size) {

        sdata[pos] = inputs[pos];
        pos += blockDim.x;
    }
    __syncthreads();
    // ...
#+end_src

Una vez tengamos todas las entradas en memoria, cada hilo accederá a los pesos que le correspondan para ir multiplicandolos por las entradas en memoria compartida e ir acumulando los resultados:

#+begin_src c
    // ...
    unsigned outputNeuron = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned weighsOffset = outputNeuron * input_size;
    float result = 0;

    if (outputNeuron < output_size) {

        for (unsigned i = 0; i < input_size; i++) {
            result += sdata[i] * weighs[weighsOffset + i];
            __syncthreads();
        }

        results[outputNeuron] += result;
    }
}
#+end_src

Finalmente, cada hilo almacena el resultado en la la nuerona de salida que le corresponde.
Notesé que en este caso todos los hilos del bloque acceden a la misma posición de memoria compartida a la vez. No esperamos que se presenten de nuevo conflictos de bancos de memoria sino, por el contrario, aprovechar la posibilidad de las lecturas de tipo broadcast. Desafortunadamente, el acceso a la memoria global para leer los pesos no es coalescente lo que se estropean nuestras expectativas. El acceso coalescente permite al dispositivo leer secciones contiguas de memoria con una sóla operación. Para un acceso coalescente, hilos contiguos deben acceder a posiciones de memoria global contiguas. Sin embargo, el salto entre la posición que lee un hilo y el siguiente es igual al tamaño de la capa de entrada, como se aprecia en la asignación =weighsOffset = outputNeuron * input_size=.

Como los pesos no están siendo accedidos de forma coalescente, las lecturas a memoria global serán secuenciales y, por tanto, las de la memoria compartida pueden no suceder al mismo tiempo. Además de estropear el broadcast, las lecturas desde la memoria global serán mucho más lentas. Esta es probablemente la optimización más importante de un kernel y la primera a tener en cuenta, pues aprovechar al máximo el ancho de banda de la memoria es fundamental para obtener un buen rendimiento usando GPGPU\cite{bestPract2009}.

Otro problema de esta aproximación es que el tamaño máximo de la capa de entrada está limitado por el tamaño de la memoria compartida. Como hemos dicho, la memoria compartida tiene una capacidad de 16 KB (aunque pueda ser superior en dispositivos más modernos). Para aceptar entradas de cualquier tamaño tendríamos que modificar el kernel de forma que tomase por parámetro con un índice que indicase el númeo de entrada que toca leer y adaptar los índices de las entradas en consecuencia. Como el algoritmo no podrá ser óptimo debido al acceso no coalescente, no se implementa dicha modificación. La presente aproximación se mantiene y se implementa también para los tipos de neurona binaria y bipolar; simplemente para comparar su rendimiento. Pero en la siguiente sección \ref{disenoParalCUDAinv} se trata de adaptar este algoritmo para conseguir un acceso coalescente a los pesos que sí podrá tomar entradas de cualquier tamaño.

*** Matriz de pesos invertida
#+LaTeX: \label{disenoParalCUDAinv}

En la sección anterior \label{disenoParalCUDAsal} la causa del acceso no coalescente era que los todos pesos de cada neurona de salida estaban juntos. Para acceder al primer peso de dos neuronas de salida consecutivas, que es a lo que quieren acceder dos hilos consecutivos, aplicabamos un salto de =input_size=. Pero si los pesos estuviesen en memoria colocados de la forma en la que serán accedidos, tendríamos un acceso coalescente. Para ello, tan sólo tenemos que invertir la matriz de pesos. Así, los pesos que corresponden a una misma neurona de entrada serán los que están juntos en lugar de los de una misma neurona de salida.

Si estuviesemos considerando un algortimo genérico de multiplicación de un vector por una matriz, tendríamos que contabilizar el coste de invertir la matriz como parte del coste total de ejecución, pues la inversión se realizaría siempre antes de hacer la operación. Pero nuestro caso es diferente. Podemos tener los pesos siempre invertidos en memoria. Tan sólo cuando la red sea almacenada o leída desde disco (por compatibilidad con las otras implementaciones paralelas), deben ser invertidas las matrices su formato normal. Cuando una capa es generada aleatoriamente, ni siquiera es necesaria la inversión. Sin embargo el cáculo del estado de una capa de una red neuronal se ejecutará probablemente varias veces durante la evaluación de un individuo y la evaluación de los individuos se repetirá por muchas generaciones. Por ello, no tenemos en cuenta los costes de la inversión de la matriz de pesos al comparar este algoritmo con otros, pues es bastante irrelevante.

La inversión de la matriz, sin embargo, sí tiene costes en cuanto a desarrollo de software. Las operaciones de mutación, cruce y olvido tienen que tener en cuenta esta colocación especial. El caso más complicado sería el cruce, aunque con el diseño descrito en la sección \ref{disenoGeneCruz}, la inversión se podrá hacer sobre el vector de bits que actua como interfaz para los diferentes esquemas de cruce y las diferentes implementaciones paralelas, cuyas disposiciones de pesos en memoria, como sucede en este caso, pueden variar. 

Resuelto el problema de la inversión desde el diseño y fuera de nuestro código CUDA, veamos los cambios que se han de aplicar sobre el kernel anterior para conseguir el acceso coalescente y poder mejorar su rendimiento:

#+begin_src c
__global__
void InvertedKernel(float* inputs, unsigned input_size, float* weighs, float* results,
                                        unsigned output_size)
{
    extern __shared__ float sdata[];

    unsigned input_pos = threadIdx.x;
    while (input_pos < input_size) {

        sdata[input_pos] = inputs[input_pos];
        input_pos += blockDim.x;
    }
    __syncthreads();

    unsigned output_pos = blockIdx.x * blockDim.x + threadIdx.x;
    float result = 0;

    if (output_pos < output_size) {

        for (unsigned i = 0; i < input_size; i++) {
            result += sdata[i] * weighs[output_pos + (i * output_size)];
            __syncthreads();
        }
        results[output_pos] += result;
    }
}
#+end_src

El kernel resulta incluso más simple. Ahora no hay una variable, =weighsOffset= para el salto entre hilos contiguos, porque el salto es simplemente  =output_pos= que depende directamente del número de bloque y de hilo. Ahora en vez de avanzar una posición en cada vuelta de bucle se avanza output_size, que es lo que se lee en total en cada vuelta.

Quedaba también pendiente por resolver el problema de la limitación del tamaño de las entradas que la memoria compartida nos impone. Para la proximación en la que paralelizabamos las salidas sin invertir la matriz de peso, resolver esto era más complejo, dijimos que teníamos que utilizar un parámetro adicional de indice de entradas y adaptar los índices a los pesos con este nuevo parámetro. Luego llmaríamos al kernel cuantas veces fuese necesario cambiando el valor de ese parámetro. 

Ahora, sin embargo, tenemos los pesos ordenados por entradas. Podemos directamente cambiar los punteros de entradas y pesos en cada llamada y darle como parámetro =input_size= el número de entradas que vaya a procesar en esa llamada en lugar del tamaño total de las entradas. Así, llamaremos a este kernel de la siguiente manera:

#+begin_src c
unsigned grid_size = ((output_size - 1) / block_size) + 1;
unsigned shared_mem_size;

while (input_size > CUDA_MAX_SHARED_FLOATS) {

    shared_mem_size = CUDA_MAX_SHARED_FLOATS * sizeof(float);
    InvertedKernel<<< grid_size, block_size, shared_mem_size >>>
        (inputPtr, weighs, results, CUDA_MAX_SHARED_FLOATS, output_size);
    inputPtr += CUDA_MAX_SHARED_FLOATS;
    weighs += (CUDA_MAX_SHARED_FLOATS * output_size);
    input_size -= CUDA_MAX_SHARED_FLOATS;
}
shared_mem_size = input_size * sizeof(float);
InvertedKernel<<< grid_size, block_size, shared_mem_size >>>
       (inputPtr, weighs, results, input_size, output_size);
#+end_src

La constante =CUDA_MAX_SHARED_FLOATS= nos dice las entradas que podremos almacenar en memoria compartida en cada llamada al kernel. Para que funcione en todos los dispositivos, consideraremos un tamaño de memoria compartida de 16 KB ó 16384 Bytes a la que hay que descontar los parámetros del kernel. Hay ya 16 Bytes de memoria compartida que están reservados para almacenar las variables internas blockIdx, blockDim y gridDim (threadIdx se almacena en un registro especial). Además, se pueden usar hasta 256 bytes  como parámetros propios a los kernels. Nuestros parámetros propios son 3 punteros y 2 enteros sin signo. Todos ellos ocupan 4 Bytes, por lo que en nuestro caso debemos reservar 20 bytes más para los parámetros del kernel. En total, tenemos 16384 - 16 - 20 = 16348, por lo que para la versión de neurona real (dividir entre los 4 bytes que ocupa un float) tenemos que el tamaño máximo de la capa  de entrada es 4087. Para dispositivos de capacidad de computación 2.0 o superiores, se puede adaptar el valor de esta constante y así aprovechar los 48 KB en lugar de usar sólo 16 KB. Pero como los accesos coalescentes tienen que ser desde posiciones de memoria alineadas con 64 bytes, utilizamos 16348-64 en lugar de 16348-36.

Independientemente del valor de la constante, en cada llamada al kernel se incrementa el puntero a las entradas en ese número de posiciones. El puntero de pesos se incrementa más rápido, puesto que por cada entrada que se procesa, se leen los pesos de todas las neuronas de salida correspondientes a esa entrada. Mientras el número de entradas por procesar sea mayor a la constante, se seguirá llamando al kernel, indicándole la constante como el número de entradas que debe procesar. Para la última llamada se indican las entradas que queden y también se adapta el parámetro de memoria compartida para reservar sólo la que se va a usar.

*** Función de activación
#+LaTeX: \label{disenoParalCUDAactiv}

Una vez que hemos obtenido los resultados de las neuronas de salida usando alguno de los algoritmos anteriores para cada una de las múltiples capas que una capa puede tomar como entrada, podemos aplicar la función de activación para completar el trabajo de esa capa de neuronas. El kernel para la activación se implementa con el siguiente código:

#+begin_src c
__global__
void activationKernel(float* results, float* thresholds, float* output, unsigned output_sz,
                             FunctionType functionType)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < output_sz) {
        output[idx] = Func(results[idx] - thresholds[idx], functionType);
    }
}
#+end_src

Como vemos, es una tarea bastante más simple. Cada uno de los hilos representa a una neurona de salida y accede a un resultado y a un umbral, los que le corresponden. Notamos, sin embargo, que se está llamando a una funcion =Func= dentro del kernel. Aunque sea una simple función C, no podemos reutilizar la definición que usan el resto de implementaciones (C++ y SSE2), sino que tenemos que redefinirla dentro de nuestro código CUDA como una función especial que será ejecutada dentro del dispositivo. Para este tipo de funciones se utiliza el especificador de declaración =__device__=. Esta es nuestra función:

#+begin_src c
__device__
float Func(float number, FunctionType functionType)
{
    switch (functionType) {

        case FT_BINARY_STEP:
            if (number > 0) {
                return 1;
            } else {
                return 0;
            }
        case FT_BIPOLAR_STEP:
            if (number > 0) {
                return 1;
            } else {
                return -1;
            }
        case SIGMOID:
            return 1.0f / (1.0f - exp(-number));
        case FT_BIPOLAR_SIGMOID:
            return -1.0f + (2.0f / (1.0f + exp(-number)));
        case FT_HYPERBOLIC_TANGENT:
            return tanh(number);
        case FT_IDENTITY:
        default:
            return number;
    }
}
#+end_src

Como vimos en el apartado del diseño \ref{disenoRedes}, las diferentes funciones de activación soportadas tienen un valor enumerado asociado y se pueden añadir más modificando tan sólo el enumerado y esta función. No obstante es importante recordar que esta función está definida en dos sitios que tendrán que actualizarse: aquí para el código CUDA y en el exterior para las otras implementaciones.

*** Operador de cruce
#+LaTeX: \label{disenoParalCUDAcruza}

Como se explicó en \ref{disenoGeneCruz} y en \ref{disenoParalCUDAinv}, la implementación del operador genético de cruce es independiente del esquema de cruce y de la disposición de los pesos en memoria de cada implementación paralela. Tan sólo se tiene que recorrer dos matrices de pesos como si fuesen dos vectores y intercambiar el valor de cada peso o no en función de si está activo el bit correspondiente en el vector de bits del mismo tamaño que también se recibirá como parámetro. Por tanto, todas las demás consideraciones han debido aplicarse ya de antemano sobre el vector de bits.

El vector de bits llegará al kernel como un array de enteros sin signo y se deberá acceder a los bits individuales usando máscaras y operaciones lógicas.
Esta sería una primera aproximación:

#+begin_src c
__global__
void crossoverKernel(float* buffer1, float* buffer2, unsigned* bitBuffer, unsigned size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {

        unsigned bit = bitBuffer[ idx / BITS_PER_UNSIGNED ];
        __syncthreads();
        unsigned mask = 0x80000000;
        mask >>= (idx % BITS_PER_UNSIGNED);
        
        if (bit & mask) {
            float aux = buffer1[idx];
            buffer1[idx] = buffer2[idx];
            buffer2[idx] = aux;
        }
    }
}
#+end_src

Es bastante simple, cada hilo coge la parte del bitBuffer que le corresponde y aplica sobre la máscara (que contiene inicialmente =100...00=) los deplazamientos hacia la derecha que correspondan para acceder a su bit. Si haciendo un =AND= lógico entre esa parte del vector de bits y la máscara para seleccionar el suyo se obtiene algún 1, se intercambian en los vactores por la posición del hilo. La constante =BITS_PER_UNSIGNED= es igual a 32.

Sin embargo, esta forma de acceso al vector de bits es altamente ineficiente. Los 32 hilos contiguos accederán a la misma posición del vector de bits. Esto no sólo implica que no se accederá coalescentemente y no se leerán 16 valores (medio warp, que es cómo se accede a la memoria global en condiciones óptimas) con una sola instrucción de lectura. Además, el acceso a ese mismo elemento por los 32 hilos se hará de forma secuencial. Esta penalización es intolerable.

Todo esto se podría evitar si los datos estuviesen dispuestos de otra manera en memoria. No podemos modificar la posición de los pesos porque las optimizaciones para el cálculo de estado de las neuronas descritas en las secciones anteriores dependen de estas posiciones. Sin embargo podemos modificar la posición en el vector de bits para conseguir nuestro objetivo.

Si cada hilo se ocupase de los 32 bits de una posición del array =bitBuffer=, el acceso también podría ser coalescente y a además el valor se podría alamacenar directamente en un registro. Pero entonces dejaría de ser coalescente el acceso a los propios vectores de pesos, pues cada hilo querría acceder a 32 pesos contiguos. A no ser, que el vector de bits viniese ya colocado de la forma que queremos, para que una posición de =bitBuffer= no representase 32 pesos contiguos, sino 32 los 32 pesos a los que su hilo puede acceder de forma coalescente. Debemos aplicar una transformación adicional sobre el vector de bits (si es que ya ha tenido alguna, por ejemplo, por la inversión de las matriz de pesos) para adaptarlo a nuestro nuevo algoritmo antes de llamar al kernel. Podemos aprovechar el momento en que creemos un buffer CUDA a partir de una Interfaz (ver Buffer e Interface) para realizar esta transformación. 

Pero antes de ver cómo realizaríamos la transformación, vemos como sería nuestro nuevo kernel. Cada hilo accederá a =BITS_PER_UNSIGNED= elementos de los arrays de pesos, cada bloque accederá a =blockDim.x * BITS_PER_UNSIGNED=. El salto entre las posiciones de un bloque y las del siguiente multiplicará el tamaño del bloque por =BITS_PER_UNSIGNED=, pero los hilos de un mismo bloque sequirán accediendo a posiciones contiguas:

#+begin_src c
__global__
void crossoverKernel(type* buffer1, type* buffer2, unsigned* bitBuffer, unsigned size)
{
    unsigned weighPos = (blockIdx.x * blockDim.x * BITS_PER_UNSIGNED) + threadIdx.x;
    unsigned maxPosForThisBlock = device_min (size,
        (blockIdx.x + 1) * blockDim.x * BITS_PER_UNSIGNED);

    unsigned bitsForTheThread, mask;
    if (weighPos < maxPosForThisBlock) {
        bitsForTheThread = bitBuffer[(blockIdx.x * blockDim.x) + threadIdx.x];
        mask = 0x80000000;
    }
    __syncthreads();

    while (weighPos < maxPosForThisBlock) {
        if (mask & bitsForTheThread) {
            type aux = buffer1[weighPos];
            buffer1[weighPos] = buffer2[weighPos];
            buffer2[weighPos] = aux;
        }
        __syncthreads();
        weighPos += blockDim.x;
        mask >>= 1;
    }
}
#+end_src

Vemos entonces que la posición que los bits deben tomar en el vector para corresponderse con los pesos en este kernel depende del tamaño del bloque con el que se vaya a lanzar el kernel. Además, las partes finales de algunos bloques de bits no serán tenidas en cuenta dependiendo del tamaño del vector y del bloque CUDA, cuando antes sólo se quedaban sin usar las del último bloque de 32 bits. Así es que el array de bits adaptado puede ser más grande que el original.

Este es el constructor especial de la clase CudaBuffer para construir el array de bits adecuado:

#+begin_src c
CudaBuffer(Interface* bitBuffer, unsigned block_size)
{
    if (bitBuffer->getBufferType() != BT_BIT) {
        std::string error = 
            "The Buffer type must be BIT to use this constructor.";
        throw error;
    }
    unsigned bitBufferSize = bitBuffer->getSize();
    unsigned maxWeighsPerBlock = BITS_PER_UNSIGNED * block_size;

    tSize = (bitBufferSize / maxWeighsPerBlock) * maxWeighsPerBlock;
    tSize += min(bitBufferSize % maxWeighsPerBlock, block_size) * BITS_PER_UNSIGNED;

    Interface interfaceOrderedByBlockSize = Interface(tSize, BT_BIT);

    unsigned bit = 0, thread = 0, block_offset = 0;
    for (unsigned i = 0; i < bitBufferSize; i++) {

        unsigned weighPos = (thread * BITS_PER_UNSIGNED) + bit + block_offset;
        thread++;
        interfaceOrderedByBlockSize.setElement(weighPos, bitBuffer->getElement(i));

        if (thread == block_size) {
            thread = 0;
            bit++;
            if (bit == BITS_PER_UNSIGNED) {
                bit = 0;
                block_offset += (block_size * BITS_PER_UNSIGNED);
            }
        }
    }
    unsigned byteSize = interfaceOrderedByBlockSize.getByteSize();
    data = cuda_malloc(byteSize);
    cuda_copyToDevice(data, interfaceOrderedByBlockSize.getDataPointer(), byteSize);
}
#+end_src

Esta función no es pprecisamente elegante, pues básicamente reproduce lo que tendŕía que hacer el kernel de forma serializada para cambiar las posiciones para el acceso coalescente. Y será llamado antes de realizar un cruce con dos redes CUDA. Aunque esta transforamción puede parecer una penalización importante, mantenemos esta implementación del cruce se lo asignamos a la versión de reducción inicial (implementada en la clase CudaReduction0Connection), que también se mantiene para hacer comparativas.

Afortunadamente, exite otra solución mucho más elegante y que nos parece también más eficiente (aunque reservamos los resultados de la comparación para la sección de resultados \ref{rendImplCruce}).

Si leyesemos primero el vector de bits a memoria compartida quizá podríamos evitar la transformación del vector de bits además de los accesos no coalescentes. Podríamos también evitar los posibles conflictos de bancos de memoria compartida con un broadcast para un warp (justamente 32 hilos, como los bits en una palabra de 4 bytes) del bloque de bits compartido por ese warp. El mínimo número de hilos contiguos que deben acceder a la misma posición de memoria compartida para que se haga un broadcast en lugar de producir un conflicto de bancos de memoria es 16 (un halfwarp), por lo que 32 hilos nos servirán:

#+begin_src c
__global__
void crossoverKernel(float* buffer1, float* buffer2, unsigned* bitBuffer, unsigned size)
{
    extern __shared__ float sdata[];

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned bitBuffer_pos = idx / BITS_PER_UNSIGNED;
    unsigned bitblocks_per_block = blockDim.x / BITS_PER_UNSIGNED;

    if (threadIdx.x < bitblocks_per_block) {
        sdata[threadIdx.x] = bitBuffer[ bitBuffer_pos ];
    }
    __syncthreads();

    if (idx < size) {

        unsigned bit = sdata[ threadIdx.x / BITS_PER_UNSIGNED ];
        __syncthreads();

        unsigned mask = 0x80000000;
        mask >>= ( threadIdx.x % BITS_PER_UNSIGNED );
        
        if (bit & mask) {
            float aux = buffer1[idx];
            buffer1[idx] = buffer2[idx];
            buffer2[idx] = aux;
        }
    }
}
#+end_src

El último bloque de hilos también leerá =bitblocks_per_block=, por lo que habrá que hacer el array de bits artificialemente más grande para que esas lecturas puedan hacerse, aunque no se vayan a utilizar esos valores.

Hemos, mejorado mucho el acceso al vector de bits. No obstante, el número de lecturas de desde memoria global a memoria compartida es muy pequeño. Sólo la hacen los primeros hilos del bloque. Como mucho, cuando el bloque tenga 512 hilos, se harán 4 lecturas por bloque desde =bitBuffer=. Para hacer una lectura coalescente se necesite que cada 16 hilos del bloque accedan a 16 posiciones consecutivas (y que los medios warps no se también estén seguidos). Pero en este caso algunos medios warps no realizarán lectura alguna y los que los hagan, no leerán 16 valores. La lectura coalescente puede no utilizar los 16 valores leídos, 4 es el mínimo. Con un tamaño de bloque de 512 hilos, se harían 4 lecturas. Y el resto de los 16 se ignorarían. Pero no basta con que sean valores contiguos para que la lectura sea coalescente, además, tiene que ser sobre posiciones alineadas sobre 16 elementos. Con este método sólo una de cada cuatro lecturas del vector de bits estará alineada y podrá ser coalescente.

Por ello deberíamos leer a memoría compartida más de 4 bloques de bits por cada bloque de hilos, como poco, 16. Lo ideal sería que cada hilo leyese un bloque de bits. Pero entonces, cada hilo tendría que procesar después 32 pesos, aunque no necesariamente los que corresponden al bloque de bits que ha leído. Si se elige un tamaño de bloque fijo de 32 hilos siempre que se llame a este kernel, se puede acceder a los bloques de bits en memoria compartida de uno en uno y hacer que cada hilo se ocupe de uno de los bits. Esto debería resultar en un broadcast. Este sería el kernel que realiza el cruce como acabamos de describir:

#+begin_src c
__global__
void crossoverSharedKernel(type* buffer1, type* buffer2, unsigned* bitBuffer, unsigned size)
{
    extern __shared__ unsigned sdata[];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned bitBlocks =  ( (size - 1) / BITS_PER_UNSIGNED ) + 1;

    if (idx < bitBlocks) {
        sdata[threadIdx.x] = bitBuffer[ idx ];
    }
    __syncthreads();

    unsigned weighPos = (blockIdx.x * blockDim.x * BITS_PER_UNSIGNED) + threadIdx.x;
    for (int bit_block = 0; bit_block < blockDim.x; ++bit_block) {

        unsigned bit = sdata[ bit_block ];

        unsigned mask = 0x80000000;
        mask >>= ( threadIdx.x );
        __syncthreads();

        if (bit & mask && weighPos < size) {
            type aux = buffer1[weighPos];
            buffer1[weighPos] = buffer2[weighPos];
            buffer2[weighPos] = aux;
        }
        weighPos += blockDim.x;
        __syncthreads();
    }
}
#+end_src

En la versión real, se sustituye =blockDim.x= por la constante =BITS_PER_UNSIGNED= (número de bits por entero sin signo ó bloque de bits) para mejorar el tiempo de ejecución. Aquí se ha mantenido =blockDim.x=, aunque se sepa que siempre va a ser 32, para mejorar la legibilidad. 

El kernel descrito antes que este podía ajustar el tamaño de los bloques para encontrar un buen equilibrio con la ocupación de núcleos y los recursos por hilos. Este, sin embargo, siempre usará bloques de 32 hilos. Además, el otro kernel debería ser más rápido por no tener siquiera que usar la memoria compartida para mantener accesos coalescentes a memoria global. Sin embargo, requería una reordenación del vector de bits como precondición. El coste esa reordenación es incluído en la pruebas de rendimiento cuyos resultados se muestran en la figura \ref{grafImplCrossover} del capítulo de resultados de rendimiento.

*** Otros operadores genéticos

Otros operadores genéticos como la mutación y el olvido también se lanzando kernels CUDA. Este es el kernel para la mutación:

#+begin_src c
__global__
void mutateKernel(float* buffer, unsigned pos, float mutation)
{
    if (threadIdx.x == 0) {
        buffer[pos] += mutation;
    }
}
#+end_src

Y este es el de olvido:

#+begin_src c
__global__
void resetKernel(float* buffer, unsigned pos)
{
    if (threadIdx.x == 0) {
        buffer[pos] = 0;
    }
}
#+end_src

Sólo actuará un hilo por bloque, no parece que se optimice mucho. Sin embargo, no se trata tanto de optimizar estos kernels sino de mantener los pesos en memoria de dispositivo en todo momento. El ancho de banda máximo en la transferencia de entre la memoria de dispositivo y la GPU es mucho mayor (por ejemplo, 141 GBps en la NVIDIA GeForce GTX 280) que el ancho de banda en la transmisión de datos entre la memoria huesped y la del dispositivo (8 GBps para PCI Express ×16 de segunda generación). Por ello, para el mejor rendimiento general de la aplicación, es de una prioridad alta minimizar las transferencias de datos etre el husped (CPU) y el dispositivo (GPU), incluso si eso significa lanzar kernels en la GPU que no suponen ninguna mejora de rendimiento comparados con ejecutar lo mismo en la CPU\cite{bestPract2009}. 

\newpage
* HACER [0/4] Manual del programador
#+LaTeX: \label{manualProgr}
** HACER Guia de instalación
#+LaTeX: \label{manualProgrInstal}

sudo aptitude install make g++ nasm gnuplot
sudo aptitude install texlive-latex-recommended texlive-latex-extra
** HACER [4/8] Interfaz de aplicación (API)
#+LaTeX: \label{manualProgrApi}
En esta sección se describen las clases implementadas y sus métodos. Explicando lo que hacen y cómo deben ser llamadas pero no necesariamente el funcionamiento interno. Se omiten los constructores por defecto (los constructores que no toman ningún parámetro) y los destructores.

*** REVISAR Common
**** Enumerations
<<<Enumerations>>>

Dado que la librería cuenta con tantas posibles configuraciones, su gestión básica en esta clase. Se han definido 10 tipos enumerados para: 

+ 4 tipos de Buffer: FLOAT, BIT, SIGN y BYTE.
+ 6 funciones de activación: identidad, escalón binario, escalón bipolar, sigmoide, sigmoide bipolar y tangente hiperbólica.
+ 6 implementaciones paralelas: C++, SSE2, CUDA por reducción 0, CUDA por reducción mejorada, CUDA paralelizando salidas y CUDA con la matriz de pesos invertida.
+ 4 algortimos de selección: ruleta, ranking, por torneo y truncado.
+ 3 algortimos de cruce: uniforme, proporcional y multipunto.
+ 4 niveles para el cruce: peso, neurona, neurona invertida (representación fregona) y capa completa.
+ 3 modos de mutación: desactivado, por individuo y probabilística
+ 3 modos de olvido: desactivado, por individuo y probabilística
+ 3 operaciones binarias: OR, AND y XOR.
+ 4 tareas: las tres operaciones binarias y la tarea Reversi/Othello.

Cada enumerado tiene un método que devuelve una cadena descriptora a partir de un valor de enumerado. También hay otro más genérico al que además del valor se le pasa la enumeración (hay otra enumeración de los enumerados) cuya declaración es: =static std::string toString(EnumType enumType, unsigned enumValue)=. Por último, otro método devuelve el número de elementos de un enumerado concreto =static unsigned enumTypeDim(EnumType enumType)=. Todos los métodos son estáticos. Como nunca es necesario instanciar la clase, el constructor se hace privado para que no se pueda hacer.

**** <<<Util>>>

Estos ficheros contienen varias utilidades variadas. Se definen macros para recorrer vectores y listas de la librería C++ estándar para evitar más dependencias de librerías externas como [[http://en.wikipedia.org/wiki/Boost_(C%2B%2B_libraries)][Boost]]. Para poder concatenar otros tipos con objetos de la clase std::string de forma más legible y cómoda, se define la función plantilla \verb=std::string to_string(const T& t)=. El resto de funciones de utilidad se agrupan en clases, aunque por ser simples se definen e implmentan en los mismos ficheros. Como pasaba con Enumerations, estas clases tienen todos sus  métodos (y atributos si los tienen) estáticos y no es necesario instanciarlas. La única excepción es SimpleGraph. Las clases son:

***** Util : 

utilidades generales. Contiene los métodos:

+ \verb=FILE* openFile(string path):= Abre un fichero en modo escritura y si no existe lo crea. Si surge algún problema lanza un string como excepción. Devuelve el descriptor del fichero para poder hacer escrituras con =fprintf()= ó =fwrite()=.
+ \verb=void check(bool condition, std::string message):= Si se cumple la condición se imprime la cadena en la salida estándar y se lanza la cadena como excepción.

***** <<<Random>>> :

para generar números aleatorios. Contiene los métodos:

+ =int integer(unsigned range):= Devuelve un entero aleatorio entre -range y range.
+ =float floatNum(float range):= Devuelve un número en coma flotante aleatorio entre -range y range.
+ =unsigned positiveInteger(unsigned range):= Devuelve un entero aleatorio entre 0 y range.
+ =float positiveFloat(float range):= Devuelve un número en coma flotante aleatorio entre 0 y range.

***** <<<MemoryManagement>>> :

en vez de llamar directamente a malloc y free para reservar memoria para los arrays de los Buffers y las interfaces, se llama a métodos de esta clase para controlar que el programa no pierda memoria. Es útil principalmente para el desarrollo. Contiene los métodos:

+ =void* malloc(unsigned size):= funciona como el malloc tradicional, se le dice un tamaño en bytes y devuelve un puntero a memoria con ese espacio reservado. La diferencia es que en este caso  se guarda el puntero y el tamaño en una lista estática que mantiene la clase MemoryManagement.
+ =void free(void* ptr):= funciona como el free tradicional, esto es, libera la memoria reservada en el puntero parámetro. En este caso también elimina el registro correspondiente de la lista antes mencionada.
+ =void clear():= Libera todos los punteros que hubiesen quedado en la lista y libera también la propia lista.
+ =void printTotalAllocated():= Imprime por la salida estándar el número total de bytes reservados con la clase (expresando las cantidades en KB ó MB si es posible).
+ =void printTotalPointers():= Imprime por la salida estándar el número total de punteros en la lista.
+ =void printListOfPointers():= Imprime por la salida estándar una lista con todos los punteros reservados y sus respectivos tamaños.
+ =unsigned getPtrCounter():= Devuelve el número total de punteros en la lista.
+ =unsigned getTotalAllocated():= Devuelve el número total de bytes reservados.

***** <<<SimpleGraph>>> :

una implementación simple de un grafo que almacena simplemente pares de índices a elemntos del grafo (los arcos, aristas o flechas). Pueden ser guardados a y cargados desde un fichero en disco. Esta clase la usa NeuralNet para almacenar el grafo de conexiones entre las diferentes capas. Contiene los métodos (se omite el destructor):

+ =void addConnection(unsigned source, unsigned destination):= Añade un arco al grafo que va desde el origen (source) al destino (destination) indicados. Si ya existía un arco así no lo añade.
+ =bool removeConnection(unsigned source, unsigned destination):= Elimina del grafo un arco que vaya desde el origen (source) al destino (destination) indicados. Si existía un arco así, lo borra y devuelve =true=. Si no existía, devuelve =false=.
+ =bool checkConnection(unsigned source, unsigned destination):= Busca en el grafo un arco que vaya desde el origen (source) al destino (destination) indicados. Si existe devuelve =true=, si no devuelve =false=.
+ \verb=std::vector<std::pair<unsigned, unsigned> >::iterator getIterator():= Devuelve un iterador al vector de pares de enteros sin signo que representan los arcos del grafo.
+ \verb=std::vector<std::pair<unsigned, unsigned> >::iterator getEnd():= Devuelve el puntero al final del vector de pares de enteros sin signo que representan los arcos del grafo.
+ =void save(FILE* stream):= Guarda la lista de arcos en el fichero descrito por el parámetro =stream=. Lo primero que guarda es el número de arcos.
+ =void load(FILE* stream):= Carga una lista de arcos desde el fichero descrito por el parámetro =stream=. Lo primero que carga es el número de arcos.

**** <<<Chronometer>>>

Esta clase se utiliza para realizar las mediciones de tiempo de ejecución y poder comparar el rendimiento de las diferentes implmentaciones paralelas descritas en el capítulo \ref{disenoParal}. Si se compila la librería para que soporte la implementación CUDA, se obliga al dispositivo GPU a que sincronice su ejecución con la de la CPU al empezar y al terminar de cronometrar para poder medir los tiempos adecuadamente. Contiene los siguientes métodos:

+ =void start():= Comienza la cuenta del cronómetro. Si ya estaba empezada anteriormente, imprime en la salida estándar un mensaje de advertencia.
+ =void stop():= Detiene la cuenta del cronómetro. Si no se había llamado previamente a =start()= lanza una cadena como excepción.
+ =float getSeconds():= Devuelve el contenido del cronómetro en segundos. Si no se había detenido el chronómetro llamando a =stop()=, devolverá 0.

*** REVISAR Neural
#+LaTeX: \label{apiNeural}

Describiremos clases utilizadas empezando desde el nivel más bajo hasta llegar a la clase NeuralNet que implementa una red neuronal completa.

**** <<<Interface>>>

Para unificar ciertas tareas comunes y tener una estructura de datos manipulable desde el exterior de las clases de esta sección se implementa esta clase. es necesaria, por ejemplo para las entradas y salidas de NeuralNet. Como es muy parecida a Buffer, ésta última reutiliza, como ya se ha mencionado algunos métodos de Interface internamente. Los métodos con los que cuenta la clase son:

+ =Interface(unsigned size, BufferType bufferType) := Constructor parametrizado. Se indica el tamaño y el tipo. El constructor por defecto es privado para obligar al usuario de la librería a utilizar algún otro constructor.
+ =Interface(Interface* toCopy) := Constructor copia. Con este constructor, el interfaz se incializa directamente con el tamaño, tipo y valores del interfaz parámetro. Puede ser llamado implicitamente, como cuando un Interface es un parámetro convencional (en vez de un puntero o un parámetro por referencia) de una función o método.
+ =Interface(FILE* stream) := Con éste constructor el interfaz carga directamente su tamaño, tipo y valores desde el archivo descrito por el parámetro stream.
+ =void* getDataPointer() := Devuelve un puntero al array de datos en memoria.
+ =virtual unsigned getByteSize() := DEvuelve el tamaño en bytes del array de datos en memoria.
+ =unsigned getSize() := Devuelve el número de elementos del interfaz.
+ =BufferType getBufferType() := Devuelve el tipo de vector (BufferType).
+ =float getElement(unsigned pos) := Devuelve el valor del elemento en la posición pasada por parámetro. Si la posición indicada es igual o superior al tamaño del interfaz, se lanza un error. 
+ =void setElement(unsigned pos, float value) := Asigna al elemento del vector en la posición indicada por el parámetro =pos= el valor del parámetro =value=. Si la posición indicada es igual o superior al tamaño del interfaz, se lanza un error. 
+ =void copyFrom(Interface* other) := Copia los elementos del interfaz parámetro acceciendo a ellos de uno a uno con los métodos anteriores. No verifica que sean del mismo tipo, pero si no son del mismo tamaño se lanza una excepción.
+ =void copyFromFast(Interface* other) := Copia los datos del interfaz parámetro llamando a memcpy. Si los interfaces no son del mismo tipo o tamaño se lanza una excepción.
+ =void print() := Muestra en la consola los contenidos del vector en un formato legible por humanos.
+ =float compareTo(Interface* other) := Recorre el vector y compara cada elemento con los elementos del vector parámetro, acumulando las diferencias para luego devolverlas sumadas. Si los dos interfaces no tienen el mismo tamaño a los tipos de buffer no coinciden, se lanzará un error.
+ =void random(float range) := Inicializa la interfaz con valores aleatorios. Si el tipo de buffer es =BYTE= y valor introducido introducido es mayor que 128, se generan valores entre -127 y +128. Si el tipo de buffer es binario o bipolar (internamente se representan igual), siempre se generan valores pertenecientes a {0, 1} (en el caso bipolar {-1, +1}).
+ =void reset() := Inicializa la interfaz con todos sus valores a cero.
+ =void save(FILE* stream) := *Guarda* el tamáño, tipo y contenido contenido del buffer *en* el fichero descrito por el parámetro stream.
+ =void load(FILE* stream) := *Carga* el tamáño, tipo y contenido contenido del buffer *desde* el fichero descrito por el parámetro stream.
+ =void transposeMatrix(unsigned width) := Este método trata al vector como si fuese una matriz e lo transforma en la matriz invertida. Toma por parámetro el ancho de la matriz (=width=) para poder hacerlo. Si el tamaño del vector no es múltiplo de =width= se lanza una excepción. Este método lo llama internamente CudaInvertedConnection tanto para salvar y cargar como para transformar el vector de bits antes de realizar la operación de cruce entre dos conexiones.

**** <<<Buffer>>>

Tanto las entradas, las salidas como los pesos se almacenana en estructuras lineales similares a vectores matemáticos. Pero la representación interna de estos datos puede variar bastante dependiendo de la técnica de paralelización utilizada y el tipo de neuronas. Por esta razón y para mantener una API limpia y un código legible y mantenible se implementa la clase abstracta Buffer. Esta clase implementa varios métodos comunes e impone a la clases hijas la implementación de ciertos métodos en los que se describirán las pecualiaridades de cada representación, sin exponerlas al código que utilice la propia clase Buffer. Como se indicó en el diseño, a esas peculiaridades se accederá utilizando polimorfismo.  

Al ser una clase abstracta, no se puede instanciar. Las clases que heredan de Buffer se describen en la sección \ref{implFactoria} y estas son sólo accesibles desde la clase Factory, por lo que la única forma de instanciarlas es usando la propia clase Factory.

Esta clase tiene métodos de tres tipos: convencionales, virtuales y virtuales puros. El tener métodos virtuales puros es lo que hace que la clase sea abstracta. Estos no están definidos pero deben ser definidos en las clases que hereden de ésta para que no sean abstractas y puedan ser instanciadas. Los métodos virtuales están implementados, pero si es preciso puede cambiarse su implementación en las clases hijas. Los métodos convencionales no se pueden reimplementar para tener polimorfismo. Los métodos privados y protegidos de la clase se describen en la sección \ref{manualProgrInterfRed}, estos incluyen métodos no públicos que son virtuales puros. A continuación se describen los métodos públicos que ofrece Buffer. 

Los métodos públicos y virtuales puros (deben ser implementados en las clases hijas):

+ =ImplementationType getImplementationType() := Devuelve el tipo de implementación del Buffer. =ImplementationType= es un enumerado descrito en Enumerations.
+ =BufferType getBufferType() := Devuelve el tipo de Buffer. =BufferType= es un enumerado descrito en Enumerations.
+ =void reset() := Inicializa a cero el valor de todos los elementos del vector.

Los demás métodos públicos y virtuales son:

+ =void copyFrom(Buffer* buffer) := Copia los valores contenidos en el buffer parámetro y los asigna al propio objeto. No es necesario que sean de la misma implementación, pero si no son del mismo =BufferType=, se lanzará una excepción. Si el objeto que llama al método tiene un tamaño *menor* que el buffer parámetro también se lanzará una excepción. Este método llama internamente a =copyFromInterface= y =toInterface=, por lo que no es especialmente eficiente. Sin embargo, se puede reimplementar para optimizar la copia entre buffers con la misma implementación.
+ =void copyTo(Buffer* buffer) := Copia los valores contenidos en el propio objeto y los asigna al buffer parámetro. No es necesario que sean de la misma implementación, pero si no son del mismo =BufferType=, se lanzará una excepción. Si el objeto que llama al método tiene un tamaño *mayor* que el buffer parámetro también se lanzará una excepción. Este método llama internamente a =copyFromInterface= y =toInterface=, por lo que no es especialmente eficiente. Sin embargo, se puede reimplementar para optimizar la copia entre buffers con la misma implementación.

El resto de métodos públicos son:

+ =void copyFromInterface(Interface* interface) := Copia los valores contenidos en la interfaz parámetro y los asigna al propio objeto. Si no son del mismo =BufferType=, se lanzará una excepción. Si el objeto que llama al método tiene un tamaño *menor* que el interfaz parámetro también se lanzará una excepción.
+ =void copyToInterface(Interface* interface) := Copia los valores contenidos en el propio buffer y los asigna a la interfaz parámetro. Si no son del mismo =BufferType=, se lanzará una excepción. Si el objeto que llama al método tiene un tamaño *mayor* que el interfaz parámetro también se lanzará una excepción.
+ =void* getDataPointer() := Devuelve un puntero a la posición de memoria en donde se almacenan los datos. No tiene por qué ser una posición de memoria en la RAM de la CPU, también puede apuntar a la memoria global de un dispositivo GPU, como es el caso para CudaBuffer.
+ =unsigned getSize() := Devuelve el número de elementos del buffer.
+ =Interface* toInterface() := Crea una interfaz con el mismo tamaño, =BufferType= y los mismos datos contenidos en este buffer. Internamente llama al método protegido y virtual puro _copyTo, que es dependiente de cada implementación.
+ =void save(FILE* stream) := *Guarda* el tamáño, tipo y contenido contenido del buffer *en* el fichero descrito por el parámetro stream. Internamente llama al método homónimo de Interface.
+ =void load(FILE* stream) := *Carga* el tamáño, tipo y contenido contenido del buffer *desde* el fichero descrito por el parámetro stream. Internamente llama al método homónimo de la clase Interface.
+ =void print() := Muestra en la consola los contenidos del buffer en un formato legible por humanos. Llama internamente al método homónimo de Interface.
+ =float compareTo(Buffer* other) := Recorre el vector y compara cada elemento con los elementos del vector parámetro, acumulando las diferencias para luego devolverlas sumadas. Si los dos buffers no tienen el mismo tamaño a los tipos de buffer no coinciden, se lanzará un error. Internamente llama al método homónimo de la clase Interface.
+ =void random(float range) := Inicializa el buffer con valores aleatorios. Si el tipo de buffer es =BYTE= y valor introducido introducido es mayor que 128, se generan valores entre -127 y +128. Si el tipo de buffer es binario o bipolar (internamente se representan igual), siempre se generan valores pertenecientes a {0, 1} (en el caso bipolar {-1, +1}). Internamente llama al método homónimo de Interface.

**** <<<Connection>>>

Al igual que Buffer, esta clase es abstracta y sólo se puede instanciar usando Factory. De está clase heredarán también las clases que la terminan de implementar, cada una cons su =ImplementationType=. Esta clase, a su vez, hereda de Buffer para reutilizar los métodos que tratan la gestión de la memoria entre otros. Las implementaciones concretas de Connection deberán heredar de forma virtual (no repetir campos) de Connection y además de su implementación concreta de Buffer. Por ejemplo, CppConnection hereda de Connection y de CppBuffer.

Esta clase conecta un Buffer de entrada con un Buffer de salida y almacena una matriz de pesos. Como caso especial, una conexión puede conectar un vector de resultados con la verdadera salida. En este caso, lo que almacena es un vector de umbrales y usará el método =activation= en lugar de =calculateAndAddTo=. 

Todos los Buffer asociados a una Connection deben ser del mismo tipo de implementación, como se describe en más detalle en la sección \ref{implFactoria}.

Los métodos públicos (ninguno virtual) que ofrece son:
+ =Buffer* getInput() := Devuelve el puntero al Buffer de entrada a la conexión.
+ =void calculateAndAddTo(Buffer* results) := Partiendo de su matriz de pesos interna y del estado actual de su buffer de entrada, calcula el rresultado parcial de cada neurona de salida para este Buffer de entrada y lo suma a la posición correspondiente en el vector =results= que toma como parámetro. El tamaño de la conexión (número de elementos ó pesos) debe ser igual al tamaño de su entrada por el tamaño del vector de resultados (que debería ser igual al tamaño del vector de salidas), de no cumplirse esta condición se lanzará una excepción.
+ =void activation(Buffer* output, FunctionType functionType) := cuando la conexión representa a un vector de umbrales, esta apunta a un vector de resultados, el parámetro del método anterior. Este método lee el vector de resultados (el input de la Connection que lo llama) y usando su vector de umbrales aplica la función de activación indicada por el parámetro =functionType=, poniendo los resultados en el Buffer =output= indicado como parámetro. Si el tamaño de la Connection (el vector de umbrales) no es del mismo tamaño que la entrada a la conexión (el vector de resultados) o que el =output=, se lanza una excepción. Si la entrada a la conexión o la propia Connection (el vector de umbrales) no son de tipo FLOAT también se lanzan excepciones. Por tanto, para que una conexión pueda llamar correctamente a este método debe ser inicializa con tamaño de salida igual a 1.
+ =void crossover(Connection* other, Interface* bitBuffer) := Aplica el operador de cruce entre la conexión y otra Connection similar recibida en el parámetro =other= dependiendo del contenido de la interfaz de tipo =BIT= recibida en el parámetro =bitBuffer=. Los pesos para cuyas posiciones haya un 1 en el vector de bits se intercambian, para los que haya un 0, se dejan como están. Las conexiones deben ser del mismo tipo y tamaño. El vector de bits debe tener también el mismo tamaño. Si cualquiera desde estas condiciones no se cumplen, se lanza la excepción correspondiente.
+ =void mutate(unsigned pos, float mutation) := Se suma =mutation= al peso en la posición =pos=. Se controlan los máximos si los pesos son de tipo =BYTE=. Si la posición es mayor que el tamaño de la conexión se lanza una excepción.
+ =void reset(unsigned pos) := Se asigna 0 al peso en la posición =pos=. Si la posición es mayor que el tamaño de la conexión se lanza una excepción. Este es el método que implementa la nueva operación de olvido descrita en la sección \ref{disenoGeneMut}.
**** <<<Layer>>>

Esta clase utiliza Buffer y Connection para construir una capa neuronal completa. A pesar de que las capas pueden ser de diferentes implementaciones, la implementación de esta clase es completamente genérica, pues las peculiaridades ya están encapsuladas en Buffer y Connection, que esta clase construye llamando a Factory. 

El constructor por defecto está protegido para obligar al usuario de la librería a utilizar uno de los constructores paramétrizados.

- =Layer(unsigned size, BufferType outputType, FunctionType functionType, ImplementationType implementationType) := Este es el constructor que será utilizado con más frecuencia. Se le indica el tamaño, el /tipo de neurona/ (=BufferType=), la función de activación (=FunctionType=) y la implementación optimizada a usar (/ImplementationType/). Las entradas se añaden posteriormente de forma dinámica.
- =Layer(FILE* stream, ImplementationType implementationType) := este otro constructor resulta útil para construir nuevas capas a partir de un fichero, por ejemplo, cuando se carga una población desde el dicos duro. Se le indica un descriptor de fichero (parámetro =stream=) y el /tipo de implementación/, pues el almacenamiento en disco es independiente de la implementación.

Luego tenemos ciertos métodos que, aunque son públicos, pueden ser modificados por otra clase que herede de Layer, como es el caso de InputLayer:

- =void addInput(Layer* input) := Se añade una conexión a otra capa como entrada. Como se especificó en el [[anaEspecLib][análisis]], la capa puede ser un puntero a la propia capa. Si las capas no tienen la misma implementación se lanza una excepción.
- =void calculateOutput() := Calcula el estado de sus neuronas en función de las entradas a la capa (y, por supuesto, los pesos y umbrales).
- =void randomWeighs(float range) := Establece valores aleatorios entre =-range= y =+range= para los pesos y umbrales.
- =void copyWeighs(Layer* sourceLayer) := Copia los pesos desde la capa en el parámetro =sourceLayer=. Si las capas no tienen la misma implementación, =BufferType= o tamaño se lanza la excepción correspondiente.
- =void save(FILE* stream) := Se guarda en el fichero el tipo de activación y el Buffer que alamcena las neuronas de salida. Es el inverso del constructor que utiliza un fichero.
- =void saveWeighs(FILE* stream) := Se guarda en el fichero el número de entradas y después cada una de las Connection alamcenadas que contienen los pesos de cada conexión entre esta capa y las entradas. Luego se guarda la Connection que contiene el vector de umbrales.
- =void loadWeighs(FILE* stream) := Se carga el número de entradas, despues las Connection asociadas a esas entradas y, por último, la Connection que contiene el vector de umbrales. Es la operación inversa a la del método anterior.
- =Connection* getThresholds() := Devuelve la conexión que contiene el vector de pesos.

Por último, el resto de métodos públicos:

- =unsigned getNumberInputs() := Devuelve el número de entradas de la capa.
- =Connection* getConnection(unsigned inputPos) := Devuelve la Connection que contiene los pesos de la capa de entrada en la conexión =inputPos=.
- =Buffer* getOutput() := Devuelve el Buffer que contiene el estado de las neuronas de salida de la capa.
- =Interface* getOutputInterface() := Devuelve un puntero a una interfaz que siempre contiene los mismos datos que el Buffer que contiene las salidas. Si no existía la interfaz, se crea. La propia capa se ocupará de borrar la interfaz cuando se llame a su propio destructor. Esta intertaz servirá para acceder a los estados de las neuronas de una capa. No es recomendable llamar a este método con capas que no sean de salida porque en adelante las salidas se copiarán a esta interfaz siempre que sean calculadas con =calculateOutput=.
- =Connection* getThresholds() := Devuelve la Connection que contiene el vector de umbrales.
- =unsigned getSize() := Devuelve el tamaño de la capa (número de neuronas de salida).
- =ImplementationType getImplementationType() := Devuelve el /tipo de implementación/ de la capa.
- =FunctionType getFunctionType() := Devuelve el /tipo de activación/ de la capa.
- =BufferType getBufferType() := Devuelve el /tipo de neurona/ de la capa.

**** <<<InputLayer>>>

Esta clase hereda de Layer y, por tanto, comparte sus métodos. Sin embargo, los constructores son diferentes:

- =InputLayer(Interface* interface, ImplementationType implementationType) := Construye una capa a partir de una Interface de entrada y el /tipo de implementación/.
- =InputLayer(FILE* stream, ImplementationType implementationType) := Construye una capa de entrada desde un fichero. Este sí tiene parámetros similares al constructor desde fichero de Layer, pero internamente funciona diferente. Como no recibe una interfaz, la crea con las características necesarias. La interfaz de entrada, se recibida por parámetro o construída de esta forma, debe eliminarse desde fuera de la capa: la InputLayer no destruye la Interface de entrada en ningún caso.

Además, se añade un nuevo método exclusivo de InputLayer:

- =Interface* getInputInterface() := Devuelve el Interface de entrada desde el que esta capa de entrada copia los valores. Si la capa ha sido construída desde fichero en vez de recibir la Interface por parámetro, está interfaz ha sido creada por la Capa y este método se vuelve imprescindible para acceder a la Interfaz y poder cambiar sus valores.

Por último, se redefinen los métodos que no deben funcionar igual que en Layer.

- =virtual void calculateOutput() := En lugar de calcular el estado de las neuronas de salida a partir de los pesos y las entradas (que no tiene), la capa de entrada copia sus valores directamente de la interfaz de entrada. Si tiene una interfaz de salida (aunque no tenga mucha lógica), el estado también se copia a la interfaz de salida como para las capas normales.
- =virtual void save(FILE* stream) := A diferencia del método de Layer, éste no guarda el /tipo de activación/ (=FunctionType=) porque nunca se usa.
- =virtual void randomWeighs(float range) := Como no almacena pesos, no hace nada. Pero tampoco lanza excepción si se le llama.
- =virtual void copyWeighs(Layer* sourceLayer) := Hace las mismas comprobaciones que el método original y si alguna no se cumple, lanza una excepción. Si todas se cumplen no hace nada.
- =virtual void addInput(Layer* input) := Lanza una excepción, este método nunca debería ser llamado para una capa de entrada.
- =Connection* getThresholds() := Lanza una excepción, este método nunca debería ser llamado para una capa de entrada.

**** <<<NeuralNet>>>

Esta es la clase que gestiona las redes neuronales al nivel más alto. Contiene un grafo dirigido con capas (Layer). Se accede a las entradas y salidas utilizando la clase Interface. Las capas de entrada se guardan separadas del resto y tienen sus propios índices para establecer conexiones. Estos son los métodos públicos que implementa:

- =NeuralNet(ImplementationType implementationType = IT_C) := El constructor sólo toma el /tipo de implementación/ y de forma optativa. Si no se introduce ninguno, se usará la /implementación de referencia/ en =C++=.
- =void addInputLayer(Interface* interface) := Crea una capa de entrada que estará conectada y se sincronizará (cuando se llame al método =calculateOutput= para esa Layer) con la Interface recibida por parámetro.
- =void addInputLayer(unsigned size, BufferType bufferType) := Crea una capa interna con el tamaño y /tipo de neurona/ indicados por parámetro. No es necesario indicar si se tratará de una capa de salida u oculta. Todas son ocultas en principio hasta que se accede al estado de la capa una vez usando el método =getOutput= descrito a continuación.
- =void updateInput(unsigned inputPos, Interface* input) := Hay ocasiones en las que los datos de entrada no se suministran siempre a través de la misma interfaz porque se crea una interfaz para cada dato (clasificación genérica, ver ClassificationTask), porque se quiere ver un tablero futuro hipotético (ReversiTask) o por cualquier otra razón. Este método copia los valores del interfaz recibida por parámetro =input= a la interfaz de la que normalmente coge su entrada la capa de entrada en la posición =inputPos=. Después, cuando se llame al método =calculateOutput=, se copiarán esas entradas al Buffer de la InputLayer.
- =Interface* getOutput(unsigned layerPos) := Devuelve una interfaz sincronizada con la capa en la posición =layerPos=. Si no existía tal interfaz, la capa correspondiente la crea.
- =unsigned getNumInputs() := Devuelve el número de capas de entrada.
- =unsigned getNumLayers() := Devuelve el número de capas que no son de entrada.
- =void addLayer(unsigned size, BufferType destinationType = BT_FLOAT, FunctionType functiontype = FT_IDENTITY) :=
- =void addInputConnection(unsigned sourceInputPos, unsigned destinationLayerPos) := Añade la capa de entrada en posición =sourceInputPos= como conexión a la capa en posición destinationLayerPos. Recuérdese que los índices para capas de entrada y para el resto de capas van separados.
- =void addLayersConnection(unsigned sourceLayerPos, unsigned destinationLayerPos) := Añade una conexión entre capas que no son de entrada. La capa en la posición =destinationLayerPos= toma como entrada a la que está en posición =sourceLayerPos=.
- =virtual void calculateOutput() := Calcula el estado de la red neuronal. Primero, todas las capas de entrada copian sus valores de sus interfaces correspondientes. Luego el resto de capas calculan su estado en el orden en que fueron añadidas a la red (su posición).
- =void randomWeighs(float range) := Todas los pesos (y umbrales) de todas las capas de la red toman valores aleatorios entre =-range= y =range=.
- =void save(FILE* stream) := Se salva la red neuronal completa en un fichero descrito por =stream=. Primero se guarda el número de capas de entrada y de salida. Luego las capas de entrada, después las de salida, el grafo de conexiones con las capas de entrada, el grafo de conexiones entre capas "normales" y, por último, se cargan los pesos y umbrales de cada capa en el orden en el que fueron añadidas a la red neuronal.
- =void load(FILE* stream) := Se carga la red neuronal completa en un fichero descrito por =stream=. Es la operación inversa a =save=.
- =void createFeedForwardNet(unsigned inputSize, BufferType inputType, unsigned numLayers, unsigned sizeLayers, BufferType hiddenLayersType, FunctionTyp functiontype = FT_IDENTITY) := Crea una red neuronal con conexiones hacia delante completa con las conexiones que es de esperar (y pone las capas en el orden habitual en el que tienen que ser activadas). Sólo hay una capa de entrada, para la que hay que indicar tamaño y /tipo de neurona/. Todas las capas internas tendrán el mismo tamaño, tipo (que pueden ser diferentes a los de la de entrada) y /tipo de activación/. Si no se indica la función de activación, se usará =IDENTITY= por defecto. También hay que indicar el número de capas internas. Tras crear una red neuronal utilizando este método, se pueden seguir añadiendo más capas y conexiones por lo que, aunque no se quiera esta topología, puede ser útil para crear la base de otra topología de forma más cómoda.
- =void createFullyConnectedNet(unsigned inputSize, BufferType inputType, unsigned numLayers, unsigned sizeLayers, BufferType hiddenLayersType  FunctionType functiontype = FT_IDENTITY) := Este método es similar al anterior, pero en lugar de estar conectadas cada capa con la anterior, cada una de las capas internas toma como entrada todas las otras capas (incluso a sí misma). Además, todas las capas están directamente conectadas con la capa de entrada. Como sucedía con el método anterior, se puede usar como base para otra topología. Al ser esta la más compleja, este método es el más recomendable paraprobar la implementación de la red neuronal y los cambios que se hagan

*** REVISAR Genetic
#+LaTeX: \label{disenoGenetic}
**** <<<Individual>>>

Esta clase hereda de la clase NeuralNet y, por tanto, comparte todos sus atributos y métodos. Las funcionalidades relacionadas con el algoritmo genético (cruce, mutación) están separadas aquí. Así, aunque las clase Connection sí tiene métodos relacionados con operadores genéticos (que todas las implementaciones de la clase abstracta deben proveer), se pueden tener los operadores genéticos fuera de Layer y NeuralNet, manteniendo el código más encapsulado y legible. El constructor de Individual es similar al de la clase padre NeuralNet: sólo toma el /tipo de implementación/ y como un parámetro opcional que usa por defecto la implementación de referencia.

Los métodos públicos que implementa la clase son:

- =Individual* newCopy(bool copyWeighs) := genera un nuevo individuo con el mismo tipo de implementación y la misma topología (las mismas capas de los mismos tipos y tamaños y las mismas conexiones). Si el parámetro =copyWeighs= es verdadero, también se copian los pesos.
- =Individual* newCopy(ImplementationType implementationType, bool copyWeighs) := Funciona exactamente igual que el método anterior pero en este caso se puede especificar un /tipo de impplementación/ diferente al del individuo original.
- =void mutate(unsigned numMutations, float mutationRange) := realiza sobre el individuo mutaciones con el esquema *determinístico* (=PER_INDIVIDUAL=, descrito en \ref{disenoGeneMutDet}). Realizará un número de mutaciones fijo indicado por el parámetro =numMutations=. Por cada una, se escoje un peso o umbral del individuo aleatoriamente y se le suma un número aleatorio entre =-range= y =range=.
- =void mutate(float probability, float mutationRange) := realiza sobre el individuo mutaciones con el esquema *probabilístico* (=PROBABILISTIC=, descrito en \ref{disenoGeneMutProb}). Por cada peso y umbral del individuo se calcula un número aleatorio entre 0 y 1. Si el número es mayor o igual que el parámetro =probability= no se hace nada. En caso contrario se le suma al peso o umbral un número aleatorio entre =-range= y =range=.
- =void reset(unsigned numResets) := realiza el operador de olvido descrito en la sección \ref{disenoGeneMutDet} con el esquema *determinístico*.
- =void reset(float probability) := realiza el operador de olvido descrito en la sección \ref{disenoGeneMutDet} con el esquema *probabilístico*.
- =void uniformCrossover(CrossoverLevel crossoverLevel, Individual* other, float probability) := Realiza el cruce *uniforme* descrito en la sección \ref{disenoGeneCruzUni} con el individuo recibido por el parámetro =other=. El otro parámetro indica la probabilidad de que cada peso se coja de este invididuo en vez del de parámetro.
- =void proportionalCrossover(CrossoverLevel crossoverLevel, Individual* other) := Realiza el cruce *proporcional* descrito en la sección \ref{disenoGeneCruzProp} con el individuo recibido por el parámetro =other=. Internamente usa el /fitness/ de ambos individuos, que está contenido en los mismos.
- =void multipointCrossover(CrossoverLevel crossoverLevel, Individual* other, unsigned numPoints) := Realiza el cruce *multi-punto* descrito en la sección \ref{disenoGeneCruzMulti} con el individuo recibido por el parámetro =other=. El parámetro numPoints indica el número de puntos de corte que tendrá la cruza. La posición de los puntos de corte se determina aleatoriamente.
- =float getFitness() := Devuelve la puntuación /fitness/ del individuo.
- =void setFitness(float fitness) := Sirve para asignarle al individuo su /fitness/.
- =unsigned getNumGenes() := Devuelve el número total de pesos y umbrales del individuo.

**** <<<Task>>>

Esta clase es abstracta y no puede ser instanciada. Todos sus métodos deben ser implementados por las clases que hereden de ésta para poder ser usadas como tarea por una población para la evaluación de los individuos. Se describe aquí lo que deben hacer los métodos de forma general, aunque la implementación puede variar mucho de un individuo a otro. 

- =virtual void test(Individual* individual) = 0 := Evalúa el individuo tomado por parámetro haciendo las pruebas que necesite la tarea. Esto puede requerir activar la red neuronal del individuo muchas veces. Al final, establece el /fitness/ del individuo.
- =virtual void setInputs(Individual* individual) = 0 := Toma un individuo y le añade los datos de estado del problema (en forma de uno o varios Interface) como entradas a la red del individuo. De esta forma, muchos individuos de la misma Population pueden apuntar a las mismas interfaces.
- =virtual Individual* getExample(ParametersMap* parameters) = 0 := Devuelve un individuo de ejemplo con una topología que podría resolver el problema. Es muy conveniente en casos en los que se sabe de antemano qué topología requerirá la red para poder aprender el problema. Por ejemplo, AND y OR, sólo necesitan una capa del mismo tamaño de las entradas. Para resolver XOR se necesita una capa oculta adicional o la red nunca será capáz de resolver el problema (un perceptrón monocapa no puede resolver XOR). Si no se tiene ninguna idea de cómo puede ser la topología de los individuos, el entrenador tendrá que establecerla manualmente o usar algún otro método. En esos casos, la tarea puede devolver simplemente un individuo sin capas ni conexiones, pero este método debe implementarse siempre. Es muy útil para usarlo junto a la utilidad de generación y presentación de resultados de aprendizaje TaskPlotter. 
- =virtual string toString() = 0 := Devuelve una cadena que identifica la tarea que se está realizando. Puede contener algún número como el tamaño del problema. Como el método anterior, su mayor utilidad es ser llamada desde la clase TaskPlotter.

**** <<<Population>>>

Esta clase gestiona el algortimo genético y la población de individuos, de los cuales almacena una lista ordenada. Tiene mcuhos parámetros internos para configurar su funcionamiento y muchos de ellos son obtativos. Por ello, contiene un mapa de parámetros de la clase ParametersMap para almacenar la mayoría de ellos, evitando así un excesivo número de atributos para la clase. 

El constructor por defecto es privado. Los constructores parametrizados necesitan, como mínimo una tarea que aprender.

- =Population(Task* task, unsigned size) := Se le pasa por parámetro la tarea que la población debe usar para entrenar a sus individuos. También se le indica el tamaño máximo de la población.
- =Population(Task* task, Individual* example, unsigned size, float range) := Además de la tarea, se le pasa un individuo de ejemplo del que los nuevos copiarán la topología, el tamaño máximo de la población (que llenará)
- =Population(Population* other) := Se crea la población a partir de otra. Se copia la tarea, el tamaño máximo de la población y los individuos que ésta contenga, pero los parametros del algoritmo genético se establecen con los valores por defecto.

Los parámetros del algoritmo genético, por su abundancia, se establecen en una mapa de parámetros implementado por la clase ParametersMap. Todos estos parámetros empizan con valores por defecto descritos en el capítulo \ref{disenoGene} que se repiten aquí por comodidad. El ParametersMap está en un atributo público de Population y se puede acceder a él con =params=. Por ejemplo, para establecer un rango para las mutaciones utlizaríamos el siguiente código:

#+begin_src C++
miPoblacion.params.putNumber(Population::MUTATION_RANGE, 5.5);
#+end_src

También se pueden poner muchos parámetros a la vez utilizando el siguiente método:

- =void setParams(ParametersMap* parametersMap) := éste método toma los parámetros del =parametersMap= y los asigna a los propios parámetros de la población. Este métodod es llamado desde TaskPlotter para ir asignando los parámetros automáticamente para una serie de pruebas para generar datos y gráficas.

Para las claves de los parámetros en la población, se utilizan cadenas constantes que se declaran como atributos públicos y estáticos de la clase Population. A continuación se explica cada uno:

- =SIZE := Sólo se utiliza desde TaskPlotter, no es necesario configurarlo si no tiene valor por defecto ni es utilizado por population para nada.

- =NUM_PRESERVE := Establece el número de Individuos que se mantienen de la generación anterior (si no son superados por los de la nueva) en cada generación. Si el valor es 0, la aproximación es generacional. Si toma valor -1, es estado estacionario puro (no se elimina ningún individuo que no sea superado). El valor por defecto es -1.
- =TOURNAMENT_SIZE := Establece el tamaño del torneo para el algoritmo de selección por torneo. El valor por defecto es 2.
- =RANKING_BASE := Establece la base (lo que se le suma a todos los individuos) para el algoritmo de selección por /Ranking/. Su valor por defecto es 0.
- =RANKING_STEP := Establece el salto entre un individuo y otro para el algoritmo de selección por /Ranking/. Su valor por defecto es 1.
- =MUTATION_NUM := Establece el número fijo de mutaciones que se aplicará a cada nuevo individuo. Su valor por defecto es 0.
- =MUTATION_PROB := Establece la probabilidad de mutación de cada peso de cada nuevo individuo.  Su valor por defecto es 0.
- =MUTATION_RANGE := Establece el rango aleatorio para las mutaciones. Estás irán de =-MUTATION_RANGE= a =MUTATION_RANGE=. Su valor por defecto es 1.
- =RESET_NUM := Establece el número fijo de olvidos (conexiones eliminadas) que se aplicará a cada nuevo individuo. Su valor por defecto es 0.
- =RESET_PROB := Establece la probabilidad de olvido (de que el peso se establezca a cero) de cada peso de cada nuevo individuo.  Su valor por defecto es 0.

Algunos parámetros utilizan una de estas cadenas como base para la clave, que se completa con valores enumerados con las siguientes funciones de utilidad. Esto permite tener valores diferentes para el mismo parámetro dependiendo del valor del enumerado:

- =string getKeyNumSelection(SelectionAlgorithm selectionAlgorithm) := Este método devuelve la clave donde se alamacena el número de individuos a seleccionar con cada uno de los algoritmos de selección. Como ya se ha comentado, se pueden utilizar varios esquemas a la vez. Con este método se puede establecer un número diferente de individuos a seleccionar por cada algoritmo de selcción. =NUM_SELECTION= contiene la cadena base que este método utiliza.
- =string getKeyNumCrossover(CrossoverAlgorithm crossoverAlgorithm, CrossoverLevel crossoverLevel) := Este método devuelve la clave donde se alamacena el número de individuos a generar po cruce con cada uno de las combinaciones entre algoritmos y niveles de cruce. Como ya se ha comentado, se pueden utilizar varios esquemas a la vez. En este caso se puede establecer un valor diferente por cada par (algortimo, nivel de cruce). =NUM_CROSSOVER= contiene la cadena base que este método utiliza.
- =string getKeyProbabilityUniform(CrossoverLevel crossoverLevel) := Sirve para guardar la probabilidad de cruza para el algoritmo de cruce uniforme. Se puede poner un valor diferente para cada nivel de cruce. =UNIFORM_CROSS_PROB= contiene la cadena base que este método utiliza.
- =string getKeyNumPointsMultipoint(CrossoverLevel crossoverLevel) := Sirve para guardar el número de puntos de cruce para el algoritmo de cruce multi-punto. Se puede poner un valor diferente para cada nivel de cruce. =MULTIPOINT_NUM= contiene la cadena base que este método utiliza.

El resto de métodos públicos son:
    
- =void insertIndividual(Individual* individual) := Evalúa al individuo recibido por parámetro con la tarea de la población que le asignará su fitness correspondiente. Usando ese fitness, se inserta en la lista ordenada que contiene la población. Si la población ya tiene el máximo número de individuos, pueden pasar dos cosas. Si el nuevo individuo insertado es peor que todos los individuos que ya hay, no se insertará y será eliminado. Si es igual o mejor que alguno de los que ya hay, ocupa su posición y se elimina el peor individuo (para no superar el número máximo con la nueva inserción).
- =unsigned nextGeneration() := Realiza la selección de los individuos de la lista ordenada (usando todos los algoritmos de selección que se hayan configurado), se crean nuevos individuos mediante los operadores de cruce configurados, se mutan y se les aplica el operador de olvido de acuerdo con la configuración, se insertan todos los nuevos individuos usando el método anterior y, por último, se incrementa el contador de generaciones interno.
- =void learn(unsigned generations) := Se entrena a la población hasta que el contador interno de generaciones sea igual al parametros =generations=.
- =void learn(unsigned generations, float goal) := Se entrena a la población mientras el contador interno de generaciones sea igual al parametros =generations= y el mejor individuo de la población tenga un /fitness/ menor que el parámetro =goal=.
- =unsigned getGeneration() := Devuelve el valor del contador interno de generaciones.
- =Individual* getBestIndividual() := Devuelve un puntero al mejor individuo.
- =float getBestIndividualScore() := Devuelve el /fitness/ del mejor individuo.
- =float getTotalScore() := Devuelve la suma de todos los /fitness/ de los individuos que forman parte actualemente de la población.
- =float getAverageFitness() := Devuelve la media de todos los /fitness/ de los individuos que forman parte actualemente de la población.
- =float getWorstIndividualScore() := Devuelve el /fitness/ del peor individuo.
- =Task* getTask() := Devuelve un puntero a la tarea de la población.
- =unsigned getSize() := Devuelve el tamaño actual de la población.
- =unsigned getMaxSize() := Devuelve el tamaño máximo de la población.
- =void changeMaxSize(unsigned newSize) := Cambia el tamaño máximo de la población, si el nuevo tamaño máximo es menor que el tamaño actual, se eliminarán los peores individuos.
- =Individual* getIndividual(unsigned pos) := Devuelve el individuo en la posición =pos= de la lista ordenada por /fitness/.
- =std::string toString() := Devuelve una cadena que identifica a la población. Llama a =Task::toString()= y le concatena el tamaño máximo de la población. 
- =void save(FILE* stream) := Guarda los parámetros de configuración del algoritmo genético y todos los individuos de la población al fichero especificado por el parámetro =stream=. Notesé que la tarea no se guarda.
- =void load(FILE* stream) := Carga los parámetros de configuración del algoritmo genético y los individuos, para insertarlos en la población, desde el fichero especificado por el parámetro =stream=. Notesé que la tarea no se carga y tiene que haber sido establecida en el constructor.
*** HACER [1/9] Loop
**** REVISAR <<<ParametersMap>>>

Esta clase implementa un mapa de parametros útil para los para los parámetros de configuración del algoritmo genético y para las utilidades de pruebas de este paquete. En realidad, la clase tiene tres mapas de parámetros diferentes: uno para números, otro para cadenas de texto y otro para punteros. La misma clave (key) puede ser repetida si el tipo es diferente. Se usa el contructor por defecto, que no tiene nada de especial.

- =void putNumber(std::string key, float number) := Asigna el valor del parámetro =number= al par con la clave =key= que haya en el mapa de números. Si no existe la clave, se crea un nuevo par (clave, valor).
- =float getNumber(std::string key) := Devuelve el valor en el mapa de números asociado a la clave =key=. Si no existe la clave en ese mapa se lanza una excepción.
- =void putPtr(std::string key, void* ptr) := Asigna el valor del parámetro =ptr= al par con la clave =key= que haya en el mapa de punteros. Si no existe la clave, se crea un nuevo par (clave, valor).
- =void* getPtr(std::string key) := Devuelve el valor en el mapa de punteros asociado a la clave =key=. Si no existe la clave en ese mapa se lanza una excepción.
- =void putString(std::string key, std::string str) := Asigna el valor del parámetro =str= al par con la clave =key= que haya en el mapa de punteros. Si no existe la clave, se crea un nuevo par (clave, valor).
- =std::string getString(std::string key) := Devuelve el valor en el mapa de punteros asociado a la clave =key=. Si no existe la clave en ese mapa se lanza una excepción.
- =std::string printNumber(std::string key) := Devuelve una cadena con la clave =key= concatenada con el número (pasado a cadena) en el mapa de números asociado con la propia clave. Si no existe la clave en ese mapa se lanza una excepción. Por ejemplo "miNumero_32.5"
- =std::string printString(std::string key) := Devuelve una cadena con la clave =key= concatenada con la cadena en el mapa de cadenas asociada con la propia clave. Por ejemplo, ~"miCadena_hola mundo!"~. Si no existe la clave en ese mapa se lanza una excepción. 
- =void print() := Escribe en la salida estándar todos los pares (clave, valor). Primero los del mapa de números, luego el mapa de punteros y, por último, el mapa de cadenas.
- =void copyTo(ParametersMap* parametersMap) := Asigna al mapa de parámetros recibido todos los pares (clave, valor) que contenga.
- =void copyFrom(ParametersMap* parametersMap) := Para todos los pares (clave, valor) que contenga intenta copiar el valor del ParametersMap recibido. Si no existe algun´valor, captura y silencia la excepción lanzada por el mapa recibido al intentar acceder a estos. Esos pares no los modifica.

**** HACER <<<Loop>>>

private:
    void setCallerLoop(Loop* callerLoop);
protected:

    std::string tKey;
    Loop* tCallerLoop;

    Loop();
    Loop(std::string key);
    unsigned tCurrentBranch;
    Loop* tInnerLoop;

    void __repeatBase(LoopFunction* func);
    virtual void __repeatImpl(LoopFunction* func) = 0;
public:
    friend class JoinEnumLoop;
    virtual ~Loop();

    void setKey(string key);
    string getKey();

    virtual void addInnerLoop(Loop* innerLoop);
    Loop* getInnerLoop();
    virtual Loop* dropFirstLoop();
    Loop* dropLastLoop();
    Loop* dropLoop(Loop* loop);

    virtual unsigned getCurrentBranch();
    virtual unsigned getNumBranches() = 0;
    unsigned getNumLeafs();
    virtual unsigned getDepth();

    virtual Loop* findLoop(std::string key);
    virtual void print() = 0;

    void repeatFunction(GenericLoopFuncPtr func, ParametersMap* parametersMap, std::string functionLabel);
    void repeatFunction(LoopFunction* func, ParametersMap* parametersMap);

    virtual std::string valueToString() = 0;
    virtual std::string getState(bool longVersion);

**** HACER <<<RangeLoop>>>
**** HACER <<<ExpLoop>>>
**** HACER <<<EnumLoop>>>
**** HACER <<<JoinEnumLoop>>>
**** HACER <<<GenericPlotter>>>
**** HACER <<<Test>>>
**** HACER <<<Plot>>>
*** HACER [0/3] LoopTest
**** HACER <<<ChronoPlotter>>>
**** HACER <<<Dummy>>>
**** HACER <<<TaskPlotter>>>

*** REVISAR Game

Este paquete se encarga de implementar utilidades para tareas relacionadas con juegos.

**** <<<Board>>>

Esta clase clase abstracta implementa un tablero básico y genérico para juegos de tablero de dos jugadores con un solo tipo de pieza como por ejemplo el Reversi/Othello o el Go. Tiene una Interface interna que las NeuralNet pueden utilizar como entrada. se define un enumerado =SquareState= con los valores posibles de las casillas: =EMPTY=, =PLAYER_1=, =PLAYER_2=.

Los constructores públicos son:

- =Board(unsigned size) := Crea un tablero cuadrado de tamaño =size * size= con todas las casillas vacías.
- =Board(Board* other) := Crea un tablero a partir de otro tablero, copiando los valores de todas las casillas.

El destructor libera el tablero y la interfaz. La clase tiene los siguientes métodos virtuales puros:

- =virtual bool legalMove(unsigned xPos, unsigned yPos, SquareState player) = 0 := Para una posición del tablero dada y uno de los dos jugadores devuelve verdadero si el movimiento es legal según las reglas del juego implementado por la clase hija y falso si el movimiento no es válido.
- =virtual bool canMove(SquareState player) = 0 := Devuelve verdadero si el jugador parámetro tiene algún movimiento legal y falso en caso contrario.
- =virtual void makeMove(unsigned xPos, unsigned yPos, SquareState player) = 0 := Realiza el movimiento en la posición indicada para el jugador =player=, actualizando el tablero como corresponda para el juego.
- =virtual void turn(SquareState player, Individual* individual = NULL) = 0 := Realiza un turno para el jgador =player=. Si se pasa por parámetro un individuo, el individuo jugará el turno. Si no se indica ninguno (el parámetro es opcional, con =NULL= por defecto), es la máquina sin aprendizaje que haya definida para el juego y contra la que se entrenan las poblaciones.
- =virtual float computerEstimation(unsigned xPos, unsigned yPos, SquareState player) = 0 := Devuelve una estimación de la calidad de la jugada indicada por parámetros realizada por la máquina sin aprendizaje.
- =virtual float individualEstimation(unsigned xPos, unsigned yPos, SquareState player, Individual* individual) = 0 := Devuelve una estimación de la calidad de la jugada indicada por parámetros realizada por el individuo recibido como parámetro.
- =virtual bool endGame() = 0 :=

Los siguientes métodos también son virtuales (se pueden redefinir por las clases hijas), pero ya vienen implementados:

- =virtual void initBoard() := Vacía todas las casillas del tablero. Se puede redefinir con las condiciones iniciales de cada juego y es llamada por el constructor básico (el que no copia desde otro tablero).
- =virtual int countPoints(SquareState player) := Devuelve la puntuación actual del jugador =player=.

El resto de métodos públicos son:

- =unsigned getSize() := Devuelve el tamaño del tablero.
- =void setSquare(unsigned xPos, unsigned yPos, SquareState squareState) := Establece el estado recibido en squareState en la casilla indicada por parámetro.
- =SquareState getSquare(unsigned xPos, unsigned yPos) := Devuelve el estado de la casilla indicada por parámetro.
- =bool squareIs(int xPos, int yPos, SquareState squareState) := Si la casilla indicada tiene el mismo estado que =squareState=, devuelve verdadero. Devuelve falos en caso contrario.
- =Interface* getInterface() := Devuelve un puntero a la interfaz interna que contiene la información del tablero representada como un vector de bits.
- =Interface* updateInterface() := Devuelve un puntero a la interfaz interna que contiene la información del tablero representada como un vector de bits. Antes de devolverlo, actualiza la interfaz con lo que haya en el tablero.
- =static SquareState opponent(SquareState player) := Devuelve el jugador contrario al recibido por parámetro. Si se le pasa el estado de casilla =EMPTY= lanzará una excepción.
- =void print() := Imprime el estado actual del tablero por la salida de texto estándar. "X" representa una casilla ocupada por el jugador 1, "O", una ocupada por el jugador 2 y "." una casilla vacía. Es muy útil para la depuración de la implementación de un nuevo juego sobre una clase que extienda de ésta.

**** <<<ReversiBoard>>>

Esta clase hereda de Board para implementar el juego Reversi/Othello comentado en la sección \ref{experimentacionJuegos}. esta clase es utilizada por la tarea ReversiTask. Los constructores son similares a los de Board, pero como el que recibe un tamaño llama a =initBoard()= y aquí se redefine, el tablero nuevo no tendrá todas las casillas vacías. Se implementan todos los métodos virtuales puros de =Board= siguiendo las reglas del juego, pero no repetiremos la descripción de las mismas.

- =virtual void initBoard() := incia el tablero según las reglas de reversi, es decir, rellena las 4 casillas del medio dándole dos casillas a un jugador y otras dos al otro. De forma que se crucen y haya movimientos legales al principio de la partida.
*** HACER [0/3] Tasks
**** HACER <<<BinaryTask>>>

Implementa la clase abstracta Task para implementar las tareas de clasificación OR, AND y XOR para dos vectores de entrada de cualquier tamaño (pero igual). Contiene dos interfaces internas para las entradas y una para la salida esperada. Las interfaces contienen vectores de floats en vez de bt

El contructor es el siguiente:

- =BinaryTask(BinaryOperation binaryOperation, BufferType bufferType, unsigned size, unsigned numTests = 0) := Tiene dos modos de operación dependiendo de si se rellena el parámetro opcional =numTests= o no. Si no se rellenea o se rellena con un cero, la tarea utilizará todas las posibles combinaciones entre los dos vectores de entrada para evaluar al individuo. Si se rellena, se utilizarán =numTests= combinaciones aleatorias al probar cada individuo, lo cuál puede reducir mucho el tiempo de evaluación de cada individuo si el tamaño de los vectores =size= es muy grande (pues el número de combinaciones crece exponencialemente con éste). El /tipo de neurona/ se le pasa para poder probar por separado redes similares con diferentes representaciones internas.

- =virtual void test(Individual* individual) := Para cada combinación de entradas que se vaya a probar (ver constructor), se activa al individuo (que ya tiene que tener las interfaces internas establecidas como entradas) y se accede a una Interface de su última capa, se actualiza el valor de la interfaz de salida deseada de la tarea a partir de las mismas interfaces de entrada usando el método privado =doOperation= y compara las diferencias entre la salida calculada por el individuo y la salida deseada utilizando el método =compareTo= ya descrito. El /fitness/ que se le asigna al individuo es igual a la puntuación máxima que podría obtener con la configuración actual de la prueba menos las diferencias acumuladas de cada combinación de entradas.
- =virtual void setInputs(Individual* individual) := Añade dos capas de entrada al individuo que apuntan a los interfaces internos de entrada.
- =virtual string toString() := Devuelve un string que indica la operación y el tamaño de los vectores, por ejemplo, "XOR4".
- =virtual Individual* getExample(ParametersMap* parameters) := Devuelve un individuo de ejemplo para resolver la operación dada

**** HACER <<<ClassificationTask>>>

- =ClassificationTask() :=
- =ClassificationTask(Interface** inputs, Interface** desiredOutputs, unsigned inputsDim) :=
- =virtual void test(Individual* individual) :=
- =virtual Individual* getExample(ParametersMap* parameters) :=
- =virtual string toString() :=

**** HACER <<<ReversiTask>>>

- =ReversiTask(unsigned size, unsigned numTests = 1) :=

- =virtual void test(Individual* individual) :=
- =virtual void setInputs(Individual* individual) :=
- =virtual std::string toString() :=
- =virtual Individual* getExample(ParametersMap* parameters) :=
*** HACER Ejecutables
** HACER Factoria e implementaciones optimizadas
#+LaTeX: \label{implFactoria}

*** HACER [0/12] Factory
**** HACER <<<Factory>>>
**** HACER configFactory
*** HACER [0/2] Implementación de referencia
**** HACER <<<CppBuffer>>>
**** HACER <<<CppConnection>>>
*** HACER [0/2] SSE2
**** HACER <<<XmmBuffer>>>
**** HACER <<<XmmConnection>>>
*** HACER [0/6] Cuda
**** HACER <<<CudaBuffer>>>
**** HACER <<<CudaBaseConnection>>>
**** HACER <<<CudaReduction0Connection>>>
**** HACER <<<CudaReductionConnection>>>
**** HACER <<<CudaOutputsConnection>>>
**** HACER <<<CudaInvertedConnection>>>
** HACER [0/3] Extensibilidad a partir de interfaces
#+LaTeX: \label{manualProgrInterf}

*** HACER Nuevas paralelizaciones de redes neuronales
#+LaTeX: \label{manualProgrInterfRed}

*** HACER Nuevas tareas para ser aprendidas
*** HACER Nuevas clases para pruebas y generación de gráficas.
\newpage
\newpage
* MODIFICAR [0/3] Resultados: Rendimiento
#+LaTeX: \label{rendimiento}

En esta sección se analizan resultados de rendimiento en tiempo de ejecución.

** MODIFICAR [1/2] Características técnicas de la máquinas utilizadas
#+LaTeX: \label{rendMaq}

*** MODIFICAR Máquina con capacidad CUDA
#+LaTeX: \label{rendMaqCUDA}

Para las pruebas en las que no se necesitaba capacidad CUDA se ha utilizado otra máquina con peores características técnicas. Esta es la máquina que se ha utilizado para las pruebas del apartado \ref{rendOperadores} y el capítulo \ref{aprendizaje}. Se trata de un ordenador de mesa (desktop) con bastante potencia, con una CPU Phenom II X6 1100T y un dispositvo GPU NVIDIA GTX 570.

Para más detalles técnicos sobre la máquina, a continuación se adjunta la salida de los comandos ejecutados para obtener información sobre la misma.

- CPU

#+begin_src sh
$ cat /proc/cpuinfo
#+end_src

- Dispositivos (GPU)

#+begin_src sh
$ lspci -vv
#+end_src

- Memoria

#+begin_src sh
$ cat /proc/meminfo
#+end_src

TODO
After compilation, go to NVIDIA_GPU_Computing_SDK/C/bin/linux/release in
the user’s home directory and run deviceQuery

*** REVISAR Máquina sin capacidad CUDA
#+LaTeX: \label{rendMaqLaptop}

Para las pruebas en las que no se necesitaba capacidad CUDA se ha utilizado otra máquina con peores características técnicas. Esta es la máquina que se ha utilizado para las pruebas del apartado \ref{rendOperadores} y el capítulo \ref{aprendizaje} (aunque en ese no se miden rendimientos en tiempo sino aprendizaje a lo largo de generaciones). Se trata de un pequeño ordenador portatil (laptop) de la marca Asus con 2 núcleos (core 2 duo) y 4 GB de memoria (de los que parte son utilizados por la GPU Intel, que no cuenta con su propia memoria).

Para más detalles técnicos sobre la máquina, a continuación se adjunta la salida de los comandos ejecutados para obtener información sobre la misma.

- CPU

#+begin_src sh
$ cat /proc/cpuinfo
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 23
model name	: Genuine Intel(R) CPU           U7300  @ 1.30GHz
stepping	: 10
cpu MHz		: 800.000
cache size	: 3072 KB
physical id	: 0
siblings	: 2
core id		: 0
cpu cores	: 2
apicid		: 0
initial apicid	: 0
fdiv_bug	: no
hlt_bug		: no
f00f_bug	: no
coma_bug	: no
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc arch_perfmon pebs bts aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
bogomips	: 2677.65
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 23
model name	: Genuine Intel(R) CPU           U7300  @ 1.30GHz
stepping	: 10
cpu MHz		: 800.000
cache size	: 3072 KB
physical id	: 0
siblings	: 2
core id		: 1
cpu cores	: 2
apicid		: 1
initial apicid	: 1
fdiv_bug	: no
hlt_bug		: no
f00f_bug	: no
coma_bug	: no
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc arch_perfmon pebs bts aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
bogomips	: 2676.61
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

#+end_src

- Dispositivos (GPU)

#+begin_src sh
$ lspci -vv
00:00.0 Host bridge: Intel Corporation Mobile 4 Series Chipset Memory Controller Hub (rev 07)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort+ >SERR- <PERR- INTx-
	Latency: 0
	Capabilities: <access denied>
	Kernel driver in use: agpgart-intel
	Kernel modules: intel-agp

00:02.0 VGA compatible controller: Intel Corporation Mobile 4 Series Chipset Integrated Graphics Controller (rev 07)
	Subsystem: ASUSTeK Computer Inc. Device 1862
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 28
	Region 0: Memory at fe400000 (64-bit, non-prefetchable) [size=4M]
	Region 2: Memory at d0000000 (64-bit, prefetchable) [size=256M]
	Region 4: I/O ports at dc00 [size=8]
	Capabilities: <access denied>
	Kernel driver in use: i915
	Kernel modules: i915

00:02.1 Display controller: Intel Corporation Mobile 4 Series Chipset Integrated Graphics Controller (rev 07)
	Subsystem: ASUSTeK Computer Inc. Device 1862
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Region 0: Memory at fe800000 (64-bit, non-prefetchable) [size=1M]
	Capabilities: <access denied>

00:1a.0 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #4 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 16
	Region 4: I/O ports at d880 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1a.1 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #5 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin B routed to IRQ 21
	Region 4: I/O ports at d800 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1a.2 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #6 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin D routed to IRQ 19
	Region 4: I/O ports at d480 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1a.7 USB Controller: Intel Corporation 82801I (ICH9 Family) USB2 EHCI Controller #2 (rev 03) (prog-if 20)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin C routed to IRQ 18
	Region 0: Memory at fe9fbc00 (32-bit, non-prefetchable) [size=1K]
	Capabilities: <access denied>
	Kernel driver in use: ehci_hcd

00:1b.0 Audio device: Intel Corporation 82801I (ICH9 Family) HD Audio Controller (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1443
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Interrupt: pin A routed to IRQ 22
	Region 0: Memory at fe9f4000 (64-bit, non-prefetchable) [size=16K]
	Capabilities: <access denied>
	Kernel driver in use: HDA Intel
	Kernel modules: snd-hda-intel

00:1c.0 PCI bridge: Intel Corporation 82801I (ICH9 Family) PCI Express Port 1 (rev 03)
	Control: I/O- Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Bus: primary=00, secondary=01, subordinate=01, sec-latency=0
	Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>
	Kernel driver in use: pcieport
	Kernel modules: shpchp

00:1c.1 PCI bridge: Intel Corporation 82801I (ICH9 Family) PCI Express Port 2 (rev 03)
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Bus: primary=00, secondary=02, subordinate=02, sec-latency=0
	Memory behind bridge: fea00000-feafffff
	Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>
	Kernel driver in use: pcieport
	Kernel modules: shpchp

00:1c.5 PCI bridge: Intel Corporation 82801I (ICH9 Family) PCI Express Port 6 (rev 03)
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Bus: primary=00, secondary=03, subordinate=03, sec-latency=0
	I/O behind bridge: 0000e000-0000efff
	Memory behind bridge: feb00000-febfffff
	Prefetchable memory behind bridge: 00000000c0000000-00000000c01fffff
	Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>
	Kernel driver in use: pcieport
	Kernel modules: shpchp

00:1d.0 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #1 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 23
	Region 4: I/O ports at d400 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1d.1 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #2 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin B routed to IRQ 19
	Region 4: I/O ports at d080 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1d.2 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #3 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin C routed to IRQ 18
	Region 4: I/O ports at d000 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1d.7 USB Controller: Intel Corporation 82801I (ICH9 Family) USB2 EHCI Controller #1 (rev 03) (prog-if 20)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 23
	Region 0: Memory at fe9fb800 (32-bit, non-prefetchable) [size=1K]
	Capabilities: <access denied>
	Kernel driver in use: ehci_hcd

00:1e.0 PCI bridge: Intel Corporation 82801 Mobile PCI Bridge (rev 93) (prog-if 01)
	Control: I/O- Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Bus: primary=00, secondary=04, subordinate=04, sec-latency=32
	Secondary status: 66MHz- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort+ <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>

00:1f.0 ISA bridge: Intel Corporation ICH9M-E LPC Interface Controller (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Capabilities: <access denied>
	Kernel modules: iTCO_wdt

00:1f.2 SATA controller: Intel Corporation ICH9M/M-E SATA AHCI Controller (rev 03) (prog-if 01)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz+ UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin B routed to IRQ 27
	Region 0: I/O ports at cc00 [size=8]
	Region 1: I/O ports at c880 [size=4]
	Region 2: I/O ports at c800 [size=8]
	Region 3: I/O ports at c480 [size=4]
	Region 4: I/O ports at c400 [size=32]
	Region 5: Memory at fe9fb000 (32-bit, non-prefetchable) [size=2K]
	Capabilities: <access denied>
	Kernel driver in use: ahci
	Kernel modules: ahci

02:00.0 Network controller: Atheros Communications Inc. AR9285 Wireless Network Adapter (PCI-Express) (rev 01)
	Subsystem: Device 1a3b:1089
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Interrupt: pin A routed to IRQ 17
	Region 0: Memory at feaf0000 (64-bit, non-prefetchable) [size=64K]
	Capabilities: <access denied>
	Kernel driver in use: ath9k
	Kernel modules: ath9k

03:00.0 Ethernet controller: Atheros Communications Atheros AR8132 / L1c Gigabit Ethernet Adapter (rev c0)
	Subsystem: ASUSTeK Computer Inc. Device 14e5
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Interrupt: pin A routed to IRQ 29
	Region 0: Memory at febc0000 (64-bit, non-prefetchable) [size=256K]
	Region 2: I/O ports at ec00 [size=128]
	Capabilities: <access denied>
	Kernel driver in use: atl1c
	Kernel modules: atl1c

#+end_src

- Memoria

#+begin_src sh
$ cat /proc/meminfo
MemTotal:        3062124 kB
MemFree:          209488 kB
Buffers:          293972 kB
Cached:           950716 kB
SwapCached:         8024 kB
Active:          1741556 kB
Inactive:         935852 kB
Active(anon):    1082592 kB
Inactive(anon):   496576 kB
Active(file):     658964 kB
Inactive(file):   439276 kB
Unevictable:          68 kB
Mlocked:              68 kB
HighTotal:       2201224 kB
HighFree:          95160 kB
LowTotal:         860900 kB
LowFree:          114328 kB
SwapTotal:       1490936 kB
SwapFree:        1436288 kB
Dirty:               136 kB
Writeback:             0 kB
AnonPages:       1426568 kB
Mapped:           116060 kB
Shmem:            146448 kB
Slab:             137540 kB
SReclaimable:     112872 kB
SUnreclaim:        24668 kB
KernelStack:        3136 kB
PageTables:         8592 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     3021996 kB
Committed_AS:    2333124 kB
VmallocTotal:     122880 kB
VmallocUsed:       35100 kB
VmallocChunk:      40420 kB
HardwareCorrupted:     0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       4096 kB
DirectMap4k:       16376 kB
DirectMap4M:      892928 kB
#+end_src

\newpage

** MODIFICAR [3/6] Rendimiento de las implementaciones paralelas
#+LaTeX: \label{rendImpl}

En esta sección se presentan los resultados en tiempo de ejecución de los métodos que se ejecutan de forma diferente con cada implementación. Todas las pruebas de la sección se han realizado con la máquina descrita en la sección \ref{rendMaqCUDA}.

*** MODIFICAR Acumulación de resultados
#+LaTeX: \label{rendImplAcumulEnt}

****  en función de las entradas
En la figura \ref{grafImplCalculateEnt}, puede apreciarse que la implementación con instrucciones SSE2 de ensamblador para utilizar el coprocesador XMM es muy superior a las demás. En ésta los tipos de Buffer BIT y SIGN (neuronas binarias y bipolares) obtienen tiempos similares al tipo FLOAT. 

Las implementaciones CUDA obtienen resultados parecidos, aunque todas ellas obtienen peores tiempos para BIT y SIGN, esto se debe a que los pesos en estos casos son de un sólo byte y los algoritmos desarrollados están optimizados para accesos coalescentes a datos float de 4 bytes (la aruitectura CUDA está en general más preparada para este tipo de datos, aunque se podrían modificar los algoritmos BIT y SIGN para adaptarse mejor a la misma).

Los tiempos de la implementación de referencia C para el tipo FLOAT se encuentran cercanos a los de las implementaciones CUDA para BIT y SIGN, es decir, no mucho peor que las implementaciones CUDA para float. Sin embargo, los peores tiempos de la gráfica son con mucha diferencia los de esta implementación para BIT y SIGN.

Esta prueba se ha hecho usando el tamaño de las entradas como coordenada X, variándolo de 100 en 100 desde 100 hasta 1000. La coordenada Y es el tiempo en milisegundos. Los pesos y otros valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. El tamaño de la capa de salida es 100 la mitad de las veces y 150 la otra mitad, haciendose la media. Se permutan los tipos de neuronas y los tipos de implementación para obtener cada curva. Cada combinación se repite 5000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método de acumulación de resultados (Connection::calculateAndAddTo) variando el tamaño de la capa de entrada.
#+LABEL:      grafImplCalculateEnt
#+ATTR_LaTeX: width=\textwidth
[[./img/impl_calculate_inputSize.png]]
\newpage

**** HACER Acumulación de resultados en función de las salidas
#+LaTeX: \label{rendImplAcumulSal}

#+CAPTION:    Método de acumulación de resultados (Connection::calculateAndAddTo) variando el tamaño de la capa de salida.
#+LABEL:      grafImplCalculateSal
#+ATTR_LaTeX: width=\textwidth
[[./img/impl_calculate_outputSize.png]]
\newpage
**** HACER Zoom CUDA con bloques ent
**** HACER Zoom CUDA con bloques sal
*** MODIFICAR Activación
#+LaTeX: \label{rendImplActiv}

De la figura \ref{grafImplActivation} se extraen varias conclusiones. El rendimiento de la activación FLOAT es idéntico para las implementaciones C y SSE2.
Era de esperar pues no se han hecho optimizaciones para la versión SSE2. Para los tipos BIT y SIGN con estas mismas implementaciones se mejora ligeramente el rendimiento. SSE2 tarda un poco más que que la C en BIT y SIGN porque en la implementación SSE2 de BIT y SIGN no se almacenan los bits en el orden normal, si no en la representación especial de SSE2 (ver seción \ref{disenoParalXMMbyte}), para que el método de acumulación de resultados pueda ser óptimo. 

La implemetación CUDA es la más rápida para FLOAT, con un tiempo que se mantiene constante para los tamaños de salida probados. La implementación CUDA de BIT y SIGN produce los peores resultados con bastante diferencia. De nuevo, el algoritmo se adaptó directamente de la versión float sin modificarlo sustancialmente, lo que se requeriría para un rendimiento óptimo.

Esta prueba se ha hecho utilizando la función de activación IDENTITY (ninguna). El tamaño de las salidas se representa en la como coordenada X, variándolo de 100 en 100 desde 100 hasta 1000. La coordenada Y es el tiempo en milisegundos. Los valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. Se permutan los tipos de neuronas y los tipos de implementación para obtener cada curva. Cada combinación se repite 90000 veces para hacer la media reducir el ruido.

#+CAPTION:    Método que ejecuta la función de activación para las salidas de una capa (Connection::activation).
#+LABEL:      grafImplActivation
#+ATTR_LaTeX: width=\textwidth
[[./img/impl_activation.png]]
\newpage

**** HACER Activación CUDA

TODO cambiando tamaño de bloques 
FLOAT, BIT y SIGN por separado que si no no se ve nada

\newpage
*** MODIFICAR Funciones de activación

Si en la figura \ref{grafImplActivation} se comparaba el rendimiento de los distintos tipos de neurona (BIT, SIGN y FLOAT con la función de activación IDENTITY), en la figura \ref{grafImplActivationFunc} se comparan los rendimientos de los distintos tipos de funciones para el tipo de neurona FLOAT. Con el tipo de neurona BIT, la función de activación siempre es BINARY_STEP y con el tipo SIGN la activación siempre es BIPOLAR_STEP, por ello podemos excluir estos tipos de neurona de esta gráfica (esas activaciones se mantienen, pero para usando la implementación FLOAT internamente). Además, la implementación SSE2 de la activación para FLOAT es práctiacamente idéntica a la de C y como vimos en la gráfica \ref{grafImplActivation} produce los mismo resultados de rendimiento, por lo que tampoco se muestran los resultados de la activación SSE2 en este caso. Se muestran, entonces, los diferentes funciones de activación para las neuronas FLOAt y para las implementaciones C y CUDA.

Los rendimientos de la implementación CUDA son todos muy buenos y casi constantes, por lo que es difícil compararlos.

Para la implementación C, se puede ver como, en general, la función más lenta es la sigmoide bipolar, seguida muy de cerca por la sigmoide. La tangente hiperbólica sólo es ligeramente más lenta que el resto. Los tiempos de las funciones de activación de salto binario y bipolar son muy parecidos (ligeramente mayor para el caso salto binario) son muy similares al de la función identidad (que no hace nada).

Esta prueba se ha hecho utilizando los parámetros que se describen a continuación. El tamaño de las salidas se representa en la como coordenada X, variándolo de 2000 en 2000 desde 2000 hasta 20000. La coordenada Y es el tiempo en milisegundos. Los valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. Se permutan los tipos de funciones de activación para FLOAT y los tipos de implementación para para obtener cada curva. Cada combinación se repite 50000 veces para hacer la media reducir el ruido.

#+CAPTION:    Método que ejecuta la función de activación para las salidas de una capa (Connection::activation) con tipo de Buffer FLOAT y las diferentes funciones de activación implementadas.
#+LABEL:      grafImplActivationFunc
#+ATTR_LaTeX: width=\textwidth
[[./img/Activation_functions.png]]
\newpage
**** HACER Funciones de activación CUDA

Simplemente un detalle de la anterior sin meter la implementación C
*** REVISAR Mapeos entre Interface y Buffer

Estos métodos se usan para recibir entradas y sacar salidas al exterior de la red. Las interfaces sirven para independizar la red de la implementación concreta escogida. Cada implementación debe mapear correctamente desde la representación genérica (Interfaz) hacia su propia representación interna (figura \ref{grafImplCopyFrom}) de los datos y viversa (figura \ref{grafImplCopyTo}).

Como se especifica en el apartado \ref{disenoRedes}, los Buffers pueden almacenar los estados de capas de neuronas, pero también los pesos de las conexiones. Por ello, además de los tipos FLOAT, BIT y SIGN se incluye el tipo BYTE, que es el que utilizan los pesos para conexiones con capas de entrada BIT ó SIGN. Sin embargo, los tipos BIT y SIGN comparten métodos por lo que se ignora el SIGN. De nuevo, todas las implementaciones CUDA comparten las mismas funciones y no se contemplan por separado.

Esta prueba se ha hecho usando el tamaño del buffer como coordenada X, variándolo desde 512 hasta 8192 con incrementos de 512. La coordenada Y es el tiempo en milisegundos. Los elementos a ser copiados se inicializan aleatoriamente y con valores pertenecientes al intervalo [-20, 20] (1 ó 0 para los BITS). Se permutan los tipos de neuronas FLOAT y BIT con los tipos de implementación C, SSE2 y CUDA para obtener cada curva. Cada combinación se repite 50000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método que implementa la copia a un Buffer con una implementación determinada desde un Buffer genérico de la clase Interfaz (Buffer::copyFromInterface).
#+LABEL:      grafImplCopyFrom
#+ATTR_LaTeX: width=\textwidth
[[./img/impl_copyFromInterface.png]]

Para el caso en que se copian datos desde la interfaz hacia el Buffer (gráfica \ref{grafImplCopyFrom}), los peores tiempos son con mucha diferencia para el tipo BIT con la implementación SSE2. Igual que en la sección \ref{rendImplActiv}, esto se debe a la colocación especial de los bits para este algoritmo.

La implemetación CUDA no es tan lenta como se podría haber esperado. Dentro de esta implementación, el tipo FLOAT es el más lento, después el tipo BYTE y después el BIT. Esto se explica fácilmente, pues los tipo más grandes ocupan también más espacio en memoria y son más datos los que tienen que ser movidos.
Salvo la excepción ya citada de SSE2 para BIT, esto se cumple en general para todas las implementaciones. Las versiones C y SSE2 tienen tiempos similares para los tipos FLOAT y BYTE.

#+CAPTION:    Método que implementa la copia de un Buffer con una implementación determinada a un Buffer genérico de la clase Interfaz (Buffer::copyToInterface).
#+LABEL:      grafImplCopyTo
#+ATTR_LaTeX: width=\textwidth
[[./img/impl_copyToInterface.png]]

La gráfica \ref{grafImplCopyTo} se ha generado utilizando los mismos parémetros iguales a la ya comentada y también muestra unos resultados similares, aunque la versión CUDA es algo más lenta. Mover datos desde la memoria del dispositivo (GPU) hacia la memoria del huesped (CPU) es más costoso que hacerlo en el sentido contrario.

\newpage

*** REVISAR Cruce
#+LaTeX: \label{rendImplCruce}

Esta prueba se ha hecho usando el tamaño de las entradas como coordenada X, variándolo desde 512 hasta 8192 con incrementos de 512. La coordenada Y es el tiempo en milisegundos. Los pesos y otros valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. Los vectores de bits que determinan qué pesos tendrán que cruzarse también se determinan aleatoriamente con 1 ó 0. El tamaño de la capa de salida es 128. Se permutan los tipos de neuronas FLOAT y BIT con los tipos de implementación C, SSE2, =CUDA_REDUC0= (que requiere ordenación previa del vector de bits) y CUDA para obtener cada curva. Cada combinación se repite 500 veces para hacer la media y reducir el ruido. La razón por la que se omite el tipo de neurona bipolar (SIGN) es que el cruce para BIT y para SIGN es idéntico, los dos se hacen sobre pesos de un byte en lugar de pesos de 4 bytes (float). Se ha aprovechado la implementación CUDA_REDUC0 para ponerle un algoritmo de cruce CUDA diferente y poder compararlos.

#+CAPTION:    Método que implementa el operador de crossover para dos conexiones dadas (Connection::crossover).
#+LABEL:      grafImplCrossover
#+ATTR_LaTeX: width=\textwidth
[[./img/impl_crossover.png]]

Las implementaciones para la CPU son bastane más lentas, siendo SSE2 algo más lenta por al almacenamiento especial de los pesos que tiene, que usa bloques de pesos en vez de pesos individuales. Las dos implementaciones CUDA son mucho más rápidas con tiempos prácticamente idénticos entre sí. Parece ser que la reordenación del vector de bits se compensa con no tener que usar la memoria compartida en absoluto, como se discutió en la sección \ref{disenoParalCUDAcruza}. Aunque no se muestran en esta gráfica, los resultado obtenidos para el algoritmo marcado como =CUDA_REDUC0= en esta gráfica producía resultados similares con diferentes números de hilos por bloque.

\newpage
*** REVISAR Mutación y Olvido

Esta prueba se ha hecho usando el tamaño de las entradas como coordenada X, variándolo desde 256 hasta 4096 con incrementos de 256. La coordenada Y es el tiempo en milisegundos. Los pesos y mutaciones se determinan aleatoriamente y con valores pertenecientes al intervalo [-20, 20]. El tamaño de la capa de salida es 128. Se permutan los tipos de neuronas FLOAT y BIT con los tipos de implementación C, SSE2 y CUDA para obtener cada curva. Cada combinación se repite 90000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método que implementa el operador genético de mutación (Connection::mutate).
#+LABEL:      grafImplMutation
#+ATTR_LaTeX: scale=0.15
[[./img/impl_mutate.png]]

De la figura \ref{grafImplMutation} se puede deducir que el operador de mutación tiene un coste mínimo y constante respecto al tamaño de la conexión (da igual que sea en función de las entradas o las salidas) para todas las implementaciones. La implementación CUDA es más lenta para los dos tipos de neuronas, siendo para esta implementación algo más lenta la versión BIT (que accede a bytes en vez de a floats). Para CUDA sólo se contempla una implementación, pues todas comparten la misma función para la mutación.

#+CAPTION:    Método que implementa el operador genético de olvido (Connection::reset).
#+LABEL:      grafImplReset
#+ATTR_LaTeX: scale=0.15
[[./img/impl_reset.png]]

La gráfica \ref{grafImplReset} se obtiene con unos parámetros iguales a los descritos para la mutación. Los resultados y conclusiones también coinciden. El operador de olvido tiene un coste constante y mínimo.

\newpage
** MODIFICAR Rendimiento de los diferentes operadores genéticos
#+LaTeX: \label{rendOperadores}

Esta sección compara empíricamente el rendimiento computacional de los algoritmos de selección descritos en el apartado \ref{disenoGeneSel} del diseño de los algortimos genéticos.

En esta sección se cronometran los operadores genéticos descritos en el capitulo \ref{disenoGene}. Algunos ya se habían analizado en la sección \ref{rendImpl}, pero a un nivel más bajo, del que dependen las implementaciones. Ahora lo que nos interesa es principalmente el coste de los operadores dependiendo de las diferentes formas de usarlo a más alto nivel. Nos abstraemos por ello de la implementación paralela concreta y usamos la implementación C de referencia para todas estas pruebas. Todas las pruebas de esta sección se realizan usando la máquina descrita en \ref{rendMaqLaptop}.

*** Selección

Esta sección compara empriricamente el rendimiento computacional de los algoritmos de selección descritos en el apartado \ref{disenoGeneSel} del diseño de los algortimos genéticos.

Esta prueba se ha realizado usando el número de individuos a seleccionar como coordenada X, variándolo con incrementos de 50 desde 50 hasta 300. La coordenada Y es el tiempo en milisegundos. Se permutan dos tamaños totales de población (400 y 500) con los tipos de selección ROULETTE_WHEEL, RANKING, TOURNAMENT y TRUNCATION para obtener las diferentes curvas. Para el tipo de selección TOURNAMENT, se cosideran los tamaños de torneo 5, 15 y 25. Cada combinación se repite 50 veces para hacer la media y reducir el ruido. Para esta prueba se ha creado una tarea (hereda de la clase Task) especial que asigna el fitness a los individuos aleatoriamente y que genera individuos vacíos. Así se minimizan todos los tiempos que no tienen que ver con la selección. Por tanto, a esta prueba no le afecta en absoluto la implementación utilizada.

#+CAPTION:    Rendimiento de los distintos operadores de selección en función del número de individuos a seleccionar. Cada operador se prueba con tamaños totales de población 400 y 500. Además, para la selección por torneo se muestran los resultados con varios tamaños de torneo.
#+LABEL:      rendGenSelect
#+ATTR_LaTeX: width=\textwidth
[[./img/Population_Selection.png]]

La selección por ranking es la más costosa y además la que más depende del tamaño total de la población. 

La selección por torneo es algo menos costosa y depende más del tamaño del torneo que del tamaño total de la población. Aunque cuando se tienen que seleccionar muchos individuos un tamaño de población más grande también penaliza ligeramente a la selcción por torneo.

La selección de ruleta también tiene un coste muy bajo y poco dependiente del número de individuos a seleccionar en comparación con los esquemas ranking y torneo. Aunque tampoco se aprecia en la gráfica, la selección por ruleta sí resulta penalizada con un tamaño de población mayor.

A pesar que que tampoco se aprecia bien en la figura \ref{rendGenSelect}, la selección de truncado tiene un coste mínimo y prácticamente constante tanto del tamaño total de la población como del número de individuos a seleccionar. 

\newpage
*** Cruce

Esta sección compara empíricamente el rendimiento computacional de los algoritmos de cruce descritos en los apartados \ref{disenoGeneCruz} y \ref{disenoGeneNiv} del diseño de los algortimos genéticos.

Esta prueba se ha realizado usando un tamaño como coordenada X, variándolo con incrementos de 50 desde 50 hasta 300. Este tamaño es el tamaño de las capas entradas y el de las internas. La coordenada Y es el tiempo en milisegundos. Se usan dos capas de entrada a la red (iguales). Los pesos se inicializan aleatoriamente y con valores pertenecientes al intervalo [-20, 20]. El número de capas internas es 1 la mitad de las veces y 2 la otra mitad, haciendose la media. Se usa el tipo de neurona FLOAT la mitad de las veces y BIT la otra mitad, haciéndose la media. Para obtener cada curva, se usan los tipos de cruce UNIFORM, MULTIPOINT y PROPORTIONAL. Es primero se prueba con las probabilidad de cruce de 0.2 y 0.4. El segundo tipo se prueba con 1 y 6 puntos de cruce. Para el algortimo PROPORTIONAL se usa siempre un fitness de 1 para un individuo y 1.5 para el otro. Cada combinación se repite 50 veces para hacer la media y reducir el ruido.

Como muestra la figura \ref{rendGenCruza} el nivel de cruce más costoso es el propio peso, el más barato es el nivel de capa completa y los niveles de neurona tienen rendimientos intermedios y parecidos independientemente de cómo se interprete qué pesos pertenecen a una neurona (si los pesos de entrada a la neurona o los pesos de salida de la misma). 

#+CAPTION:    Rendimiento de los distintos operadores genéticos de cruce en función del tamaño de las capas de neuronas internas.
#+LABEL:      rendGenCruza
#+ATTR_LaTeX: width=\textwidth
[[./img/Individual_crossover.png]]

Como norma general, observamos que el cruce unifrome es siempre más costoso cuanto más se acerque la probabilidad de cruce al 50% y que el cruce multi-punto siempre es más lento cuantos más puntos de corte tenga, lo cuál, por otra parte, no es en modo alguno sorprendente. Sin embargo, el cruce multi-punto no parece muy sensible al número de puntos de corte, aunque afecta más cuanto mayor sea el tamaño de la red-individuo, más afecta el número de cortes al rendimiento. El cruce proporcional siempre es ligeramente más lento que el unifrome con probabilidad 0.4 (aunque con tiempos muy cercanos). La razón es que la probabilidad de cruce para este algoritmo se calcula con la expresión =fitness / (fitness + other_fitness)= y con los parámetros escogidos siempre resulta en ~1 / (1 + 1.5) = 0.4~.

Para el cruce a nivel de peso, el más lento por tener los genes más pequeños y numerosos, los algoritmos más lentos son el uniforme con una probabilidad del 40% y el proporcional, después el uniforme con probabilidad 0.2 y los más rápidos son los muulti-punto.

Con el cruce a nivel de capa, ocurre lo contrario: el uniforme con probabilidad 0.2 se mantiene en el medio, pero ahora los algoritmos más rápidos son el uniforme con una probabilidad del 40% y el proporcional, mientras que los multi-punto son más lentos.

El uniforme con una probabilidad del 20% es igual de rápido para el nivel de neurona independientemente de si se la trata de forma invertida o no. Es el más rápido para este nivel de cruce. Para el resto de esquemas de cruce, el nivel de neurona es más lento que el de neurona invertida.

Para en nivel de neurona sin invertir, el multi-punto con un sólo corte es el más lento y todos los demás, el multi-punto con 6 cortes, el uniforme 40% y el proporcional obtienen tiempos iguales.

Para el nivel de neurona invertida, los algoritmos más lentos son el uniforme con una probabilidad del 40% y el proporcional, después el multi-punto con un sólo corte, después el multi-punto de 6 cortes y el más rápido, como ya se ha dicho, es el uniforme con una probabilidad de 0.2.

\newpage
*** Mutación
#+LaTeX: \label{rendOperadoresMut}

Esta sección compara empíricamente el rendimiento computacional de los modos de mutación descritos en el apartado \ref{disenoGeneMut} del diseño de los algortimos genéticos.

Esta prueba se ha realizado usando un tamaño como coordenada X, variándolo con incrementos de 50 desde 50 hasta 300. Este tamaño es el tamaño de las capas entradas y el de las internas. La coordenada Y es el tiempo en milisegundos. Se usan dos capas de entrada a la red (iguales). Los pesos se inicializan aleatoriamente y con valores pertenecientes al intervalo [-20, 20]. El número de capas internas es 1 la mitad de las veces y 2 la otra mitad, haciendose la media. Se usa el tipo de neurona FLOAT la mitad de las veces y BIT la otra mitad, haciéndose la media. Para obtener cada curva, se permutan los algortimos PER_INDIVIDUAL y PROBABILISTIC con 4 probabilidades: 0.1, 0.2, 0.3 y 0.4. Para el PROBABILISTIC la probabilidad se toma directamente como parámetro. Para el algoritmo PER_INDIVIDUAL, el número de mutuaciones se calcula multiplicando el número total de genes del individuo por la probabilidad. Así, se espera que los dos algoritmos obtengan un número similar de mutaciones por individuo para la misma probabilidad parámetro.

Según la figura \ref{rendGenMutation}, ambos tipos de mutación son más costosos cuánto mayor es la probabilidad, tal y cómo era de esperar, pues más mutaciones son más escrituras. Además, la mutación probabilística es en general más costosa que la mutación determinística (un número constante de mutaciones por individuo, PER_INDIVIDUAL) salvo cuando la probabilidad parámetro es muy grande (muchas mutaciones por individuo). Aunque la interfaz determinística es más rápida, también se hace más lenta comparativamente cuanto mayor es la probabilidad. La explicación es que la opción probabilística tiene un coste fijo elevado (un número aleatorio por gen) mientras que en la determinística el coste sube con el número de mutaciones (varios números aleatorios por cada mutación a realizar).

#+CAPTION:    Rendimiento de los distintos operadores genéticos de mutación para diferentes probabilidades. Para el operador no probabilístico, se ha calculado el número de mutaciones por individuo multiplicando la probabilidad por el número de genes (pesos y umbrales), para poder compararlo en igualdad con el operador probabilístico en cuanto al número de escrituras en los pesos.
#+LABEL:      rendGenMutation
#+ATTR_LaTeX: width=\textwidth
[[./img/Individual_mutate.png]]
\newpage

*** Olvido
#+LaTeX: \label{rendOperadoresOlv}

Esta sección compara empíricamente el rendimiento computacional de los modos de olvido descritos en el apartado \ref{disenoGeneMut} del diseño de los algortimos genéticos. Esta prueba utiliza unos parámetros semejantes a la anterior (sección \ref{rendOperadoresMut}).
El nuevo operador de olvido/reset presenta otra vez unos resultados similares a los de la mutación y se pueden extraer las mismas conclusiones como muestra la figura \ref{rendGenOlvido}.

#+CAPTION:    Rendimiento de los distintas interfaces del nuevo operador genético de olvido para diferentes probabilidades. Para el operador no probabilístico, se ha calculado el número de mutaciones por individuo multiplicando la probabilidad por el número de genes (pesos y umbrales), para poder compararlo en igualdad con el operador probabilístico en cuanto al número de escrituras en los pesos.
#+LABEL:      rendGenOlvido
#+ATTR_LaTeX: width=\textwidth
[[./img/Individual_reset.png]]
\newpage

* MODIFICAR Resultados: Aprendizaje
#+LaTeX: \label{aprendizaje}

En esta sección se muestran resultados de aprendizaje en términos del mejor fitness de la población en cada generación. Nos abstraemos por ello de la implementación concreta y usamos la SSE2 para todas las pruebas del capítulo. Se elige esa implementación por ser la más rápida usando la máquina descrita en \ref{rendMaqLaptop} y porque no se miden tiempos sino generaciones, por lo que la implementación paralela no va a afectar al resultado.

** Comparación entre las distintas funciones de activación
#+LaTeX: \label{aprendFunc}

A continuación observamos cómo influyen las diferentes funciones de activación para las tareas implementadas.

*** OR

Para la tarea Or, no se observan diferencias sustanciales con diferentes funciones de activación al mirar la figura \ref{aprenFuncOr}.

#+CAPTION:    Aprendizaje de la tarea lógica Or con las distintas funciones de activación.
#+LABEL:      aprenFuncOr
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_OR.png]]

\newpage
*** AND

Según la gráfica \ref{aprenFuncAnd} la función de activación Sigmoide bipolar es ligeramente superior al resto para la tarea lógica AND.

#+CAPTION:    Aprendizaje de la tarea lógica And las distintas funciones de activación.
#+LABEL:      aprenFuncAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_AND.png]]

\newpage
*** XOR

Para la tarea XOR - cuyas redes neuronales tienen 2 capas - la figura \ref{aprenFuncXor} muestra diferencias de en el aprendizaje dependiendo de la función de activación. La función identidad es superior a la mayoría y la de escalón binario inferior. La sigmoide empiza como la mayoría, pero se va haciendo peor según avanza el aprendizaje.

#+CAPTION:    Aprendizaje de la tarea lógica Xor las distintas funciones de activación.
#+LABEL:      aprenFuncXor
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_XOR.png]]

\newpage
*** Reversi

Para la tarea Reversi igual que pasaba con la tarea Or, no se observan diferencias sustanciales con diferentes funciones de activación a pesar de que en este caso también se tienen varias capas en la red neuronal (figura \ref{aprenFuncReversi}).

#+CAPTION:    Aprendizaje de la tarea Reversi las distintas funciones de activación.
#+LABEL:      aprenFuncReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_REVERSI.png]]

\newpage

** Comparación entre redes discretas y lineales
#+LaTeX: \label{aprendDiscretLineales}

A continuación compararemos los posibles efectos beneficiosos o perjudiciales al aprendizaje que se pueden obtener a partir de renunciar a las funciones derivables en favor de neuronas de tipo BIT (salida 0 ó 1) ó SIGN (salida -1 ó 1) y de pesos discretos y más reducidos (1 Byte en lugar de los 4 bytes de un float). También se incluyen las funciones de activación BINARY_STEP y BIPOLAR_STEP para FLOAT para ver el caso en que los pesos siguen siendo igual de grandes y las neuronas se comportan como BIT o SIGN (para poder analizar el efecto de los pesos discretos y menores por separado).

*** OR

Para la tarea Or, la función IDENTITY es la mejor opción con muy poca diferencia (figura \ref{aprenDiscretOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretOr
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_OR.png]]

\newpage
*** AND

Para la tarea And, también con poca diferencia, las funciones bipolares son la opción superior independientemente de si los pesos son almacenados como Floats o cómo Bytes (figura \ref{aprenDiscretAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_AND.png]]

\newpage
*** XOR

De nuevo observamos más diferencias en la tarea Xor (figura \ref{aprenDiscretXor}). Como en AND, la opciones bipolares son superiores durante todo el aprendizaje. Seguidas de la activación linear, que se va haciendo menos efectiva conforme avanza el aprendizaje, cuando es superada por las versiones binarias. Tanto como para las binarias como para las bipolares, se aprecia una diferencia mínima en favor de las versiones cuyos pesos se alamacenan como floats, pero que no compensan las grandes diferencias de rendimiento comentadas en la sección TODO cambiar referencia\ref{rendImplAcumulEnt}.

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretXor
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_XOR.png]]

\newpage
*** Reversi

Para la tarea Reversi todos los tipos de neuronas obtienen resultados bastante semejantes en la gráfica \ref{aprenDiscretReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_REVERSI.png]]

\newpage
** Comparación de operadores genéticos
#+LaTeX: \label{aprendOperGen}

Para estudiar el aprendizaje con los distintos operadores genéticos, se han probado cada una de las 4 tareas implementadas y se han generado gráficas de cada una de ellas por separado. Se ha utilizado un tamaño de población de 8 individuos, de los cuales se seleccionan 4 en cada iteración, que se cruzan, se mutan se prueban los nuevos individuos generados para insertarlos ordenadamente en la población anterior, quedando de nuevo 8. De esta manera, se desechan los 4 peores de entre los 8 que había y los 4 nuevos generados. En caso de empate se favorece a los nuevos, siguiendo el criterio de búsqueda neutral (TODO falta referencia bibliográfica aquí). Cada población utiliza un esquema de selección, cruce y mutación concretos, pero para cada gráfica se hace la media de todos los esquemas posibles (salvo para el operador concreto que se esté estudiando en cada gráfica). El operador de olvido sólo se usa al generar sus propias gráficas, haciendo la media con el todas las posibilidades en el resto de operadores.

*** Selección

Como esquemas de selección se utilizan los operadores de truncado (se selecciona directamente a los mejores), ruleta (con valor base=1), ranking (con valores por defecto base=9 step=6) y torneo (con tamaños de torneo 2 y 4).

**** OR

(figura \ref{aprenSelectOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos esquemas de selección.
#+LABEL:      aprenSelectOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_OR.png]]

\newpage
**** AND

(figura \ref{aprenSelectAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos esquemas de selección.
#+LABEL:      aprenSelectAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_AND.png]]

\newpage
**** XOR

(figura \ref{aprenSelectXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos esquemas de selección.
#+LABEL:      aprenSelectXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenSelectReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos esquemas de selección.
#+LABEL:      aprenSelectReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_REVERSI.png]]


\newpage
*** Cruce

**** OR

(figura \ref{aprenCrossOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos esquemas y niveles de cruce.
#+LABEL:      aprenCrossOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_OR.png]]

\newpage
**** AND

(figura \ref{aprenCrossAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos esquemas y niveles de cruce.
#+LABEL:      aprenCrossAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_AND.png]]

\newpage
**** XOR

(figura \ref{aprenCrossXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos esquemas y niveles de cruce.
#+LABEL:      aprenCrossXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenCrossReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos esquemas y niveles de cruce.
#+LABEL:      aprenCrossReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_REVERSI.png]]

\newpage
*** Mutación

**** OR

(figura \ref{aprenMutOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos configuraciones de mutación.
#+LABEL:      aprenMutOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_OR.png]]

\newpage
**** AND

(figura \ref{aprenMutAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos configuraciones de mutación.
#+LABEL:      aprenMutAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_AND.png]]

\newpage
**** XOR

(figura \ref{aprenMutXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos configuraciones de mutación.
#+LABEL:      aprenMutXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenMutReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos configuraciones de mutación.
#+LABEL:      aprenMutReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_REVERSI.png]]

\newpage
*** Operador de olvido

**** OR

(figura \ref{aprenResetOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_OR.png]]

\newpage
**** AND

(figura \ref{aprenResetAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_AND.png]]

\newpage
**** XOR

(figura \ref{aprenResetXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenResetReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_REVERSI.png]]

\newpage
** Comparación de distintos tamaños de población número de individuos conservados por generación

\newpage
\newpage
* BORRAR [0/3] cosas pendientes
** HACER [0/13] General documentación
- [ ] Hacer Diseño
- [ ] Arreglar resul chorno genet ?
- [ ] Modificar Resultados : Rendimiento genético (pasar gráficas a milisegundos)
- [ ] Hacer API
- [ ] Factoria e implementaciones optimizadas
- [ ] Extensibilidad a partir de interfaces
- [ ] Guia de instalación
- [ ] Modificar Resultados : Rendimiento
- [ ] Modificar CUDA después de tener resultados
- [ ] Modificar Resultados : Aprendizaje
- [ ] Hacer conclusiones
- [ ] Doxygen ?
- [ ] Estándar Universidad
** HACER [0/1] Cosas que comentar
- [ ] En utilidades para pruebas o en otro sitio...
  explicar el error que hemos permitido para probar la corrección de cada implementación paralela comparandolas con la implementación secuencial C++
  Cogido del best practices de CUDA...
  7.3.2 Floating-Point Math Is Not Associative
  Each floating-point arithmetic operation involves a certain amount of rounding.
  Consequently, the order in which arithmetic operations are performed is important.
  If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal
  A+(B+C) as it is in symbolic math. When you parallelize computations, you
  potentially change the order of operations and therefore the parallel results might
  not match sequential results. This limitation is not specific to CUDA, but an
  inherent part of parallel computation.
** HACER [0/3] Org-mode/LaTeX
- [ ] Arreglar murl
- [ ] Código con colorines (minted)
  sudo su
  PATH=/usr/local/texlive/2012/bin/i386-linux:$PATH
  tlmgr update minted
  pdflatex -shell-escape preann-doc.tex
- [ ] Poner el plantuml dentro del documento principal ?
* HACER Conclusiones
#+LaTeX: \label{conclusiones}

\newpage

* Footnotes

#Bibliografía 
#+begin_latex

\cleardoublepage\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\begin{thebibliography}{9}
 
\bibitem{Hilera95} \textsc{Hilera, J.R., Martñinez, V.J.}: \emph{Redes Neuronales Artificiales. Fundamentos, Modelos y Aplicaciones}. Addison Wesley Iberoamericana, Madrid / Mexico (1995).

\bibitem{Koehn94} \textsc{Philipp Koehn}: \emph{Combining genetic algorithms and neural networks: The encoding problem}. The University of Tennessee, Knoxville (1994).

\bibitem{BackSchwefel93} \textsc{Th. Bäck and H.-P Schwefel}: \emph{An overview of evolutionary algorithms for parameter optimization}. Evolutionary Computation, Vol.1, No.1, pp.1–23. (1993).

\bibitem{Holland75} \textsc{J. H. Holland}: \emph{Adaptation in natural and artificial systems}. Univ Mich Press, Ann Arbor (1975).

\bibitem{Koza92} \textsc{J. R. Koza}: \emph{Genetic Programming: On the programming of computers by means of natural evolution}. MIT Press, Massachusetts (1992).

\bibitem{Kim92} \textsc{Kim, H.} (1992): \emph{Mathematical Findings on Artificial Neural Nets and Their Physical Meaning}. Technical Report. Computer Science

\bibitem{Funahashi89} \textsc{Funahashi, K.I.} (1989) : \emph{On the approximate realization of continuous mappings by neural networks}. En Neural Networks, 2. pp. 183-192.

\bibitem{Hornik89} \textsc{Hornik, K., Stichcombe, M. White, H.} (1989) : \emph{Multilayer feedforward networks are universal approximators}. En Neural Network, 2(5): 359 - 366

\bibitem{Walker95} \textsc{Walker, T.} (1995): \emph{Generating neural networks with genetic algorithms using a marker based encoding}. MSc Thesis. University of Edinburgh.

\bibitem{Wilson94} \textsc{Wilson, E.} (1994): \emph{Backpropagation Learning for System with Discrete-Valued Functions}. En Proceedings of the World Congress on Neural Networks. Vol 3, pp. 332-339.


\bibitem{Sutton86} \textsc{Sutton, R.S.} (1986): \emph{Two problems with Backpropagation and other steepest descent learning procedures for networks}. En Proc. of 8th Annual Conf. Of the Cognitive Science Society, pp 823-831. Lawrence Erlbaum Associates, Hillsdale, NJ.

\bibitem{Krose93} \textsc{Kröse, B., Van der Smagt, P.} (1993): \emph{An Introduction to Neural Networks}. University of Amsterdam.

\bibitem{Kolen91} \textsc{Kolen, J., Pollack, J.} (1991): \emph{Back Propagation is Sensitive to Initial Conditions}. En Advances in Neural Information Processing Systems. R. P. Lippmann, J. E. Moody, D. S. Touretzky (eds). Vol. 3, pp. 860-867. Morgan Kaufmann Publishers, Inc.

\bibitem{Liu2004} \textsc{Liu, A., Liu, Z., Niu, Z., Wang, C.} (2004): \emph{Evolving Neural Network using Real coded Genetic Algorithm (GA) for Multispectral Image}.

\bibitem{Branke95} \textsc{Branke, J.} (1995): \emph{Evolutionary Algorithms for Neural Network Design and Training}. En Proceedings of the First Nordic Workshop on Genetic

\bibitem{Bertona2005} \textsc{Luis Federico Bertona}: \emph{Entrenamiento de redes neuronales basado en algoritmos evolutivos}. Faculad de ingeniería, Universidad de Buenos Aires (2005).

\bibitem{Mitchell96} \textsc{Mitchell Melanie} (1996): \emph{An Introduction to Genetic Algorithms}. Massachusetts Institute of Technology.

\bibitem{Whitley89} \textsc{Darrell Whitley} (1989): \emph{The GENITOR Algortihm and Selection Pressure: Why Rank-based allocation of reproductive trials is best}. Colorado State University.

\bibitem{Mitchell2000} \textsc{Mitchell A. Potter, Kenneth A. De Jong} (2000): \emph{Cooperative Coevolution: An Architecture for Evolving Coadapted Subcomponents}. MIT Press.

\bibitem{Yao99} \textsc{Xin Yao}: \emph{Evolving Artificial Neural Networks}. School of Computer Science, The University of Birmingham (1999).

\bibitem{GomezMiikkulainen2003} \textsc{Faustino J. Gómez y Risto Miikkulainen}: \emph{Robust Non-Linear Control through Neuroevolution}. Deparment of Computer Science, The University of Texas (2003).

\bibitem{Weise2009} \textsc{Thomas Weise} (2009): \emph{Global Optimization Algorithms: Theory and Application}.

\bibitem{Montana89} \textsc{David J. Montana, Lawrence Davis} (1989): \emph{Training Feedforward Neural Networks Using Genetic Algorithms}.

\bibitem{Schaffer92} \textsc{Schaffer, J.D.; Whitley, D.; Eshelman, L.J} (1992): \emph{Combinations of genetic algorithms and neural networks: a survey of the state of the art}.

\bibitem{Schmidhuber97} \textsc{Sepp Hochreiter, Jürgen Schmidhuber} (1997): \emph{Long Short-Term Memory}. Neural Computation.

\bibitem{Stanley2002} \textsc{Stanley, K. O., and Miikkulainen, R.} (2002): \emph{Evolving neural networks through augmenting topologies. Evolutionary Computation}. University of Texas.


%%%%%%%%% 24 \bibitem{} \textsc{} (): \emph{}. .
%%%%%%%%% TODO, ordenar los que están por debajo

%%%%%%%% Algoritmos genéticos


\bibitem{DeJong88} \textsc{Kenneth De Jong}: \emph{Learning with genetic algorithms: An overview}. Mach Learning, vol. 3, pp. 121-138 (1988).

\bibitem{Goldberg89} \textsc{Goldberg, D.E.}: \emph{Genetic Algorithms in Search, Optimization and Machine Learning}. Addison-Wesley, Reading (1989).

\bibitem{Fogel95} \textsc{D. B. Fogel}: \emph{Evolutionary computation. Toward a new philosophy of machine intelligence}. IEEE Press, Piscataway, NJ (1995).

\bibitem{Michalewicz96} \textsc{Z. Michalewicz}: \emph{Genetic algorithms + data structures = evolution programs}. Third, Revised and Extended Edition, 3 ed: Springer (1996).


\bibitem{Merelo2012} \textsc{Juan J. Merelo Guervós}: \emph{Informática evolutiva: Algoritmos genéticos}. Documento técnico, Universidad de Granada, 2012. http://geneura.ugr.es/~jmerelo/ie/ags.htm

%%%%%%%% CUDA

\bibitem{Davis2001} \textsc{Christopher Edward Davis} (2001): \emph{Graphics Processing Unit Computation of Neural Networks}. University of New Mexico.

\bibitem{progGuide2009} \textsc{NVIDIA corporation}: \emph{CUDA Programming guide v2.3.1}. NVIDIA Developer Technology, (2009).

\bibitem{Harris2007} \textsc{Mark Harris}: \emph{Optimizing Parallel Reduction in CUDA}. NVIDIA Developer Technology, (2007).

\bibitem{bestPract2009} \textsc{NVIDIA corporation}: \emph{CUDA Best practices guide}. NVIDIA Developer Technology, (2009).

\bibitem{Farber2008} \textsc{Rob Farber}: \emph{CUDA: Supercomputing for the Masses}. Dr Dobbs (2008)

\murl{http}{www.drdobbs.com/parallel/cuda-supercomputing-for-the-masses-part/207200659}.

%%%% Juegos

\bibitem{Chaslot2010} \textsc{Guillaume Maurice Jean-Bernard Chaslot}: \emph{Monte-Carlo Tree Search}. Tésis doctoral, Universiteit Maastricht, Maastricht, Holanda (2010).

\bibitem{Syed03} \textsc{Syed, Omar; Syed, Aamir}: \emph{Arimaa – a New Game Designed to be Difficult for Computers}. International Computer Games Association Journal (2003).

\end{thebibliography}
#+end_latex

[fn:cudaGPGPU] CUDA forma parte de lo que se conoce como GPGPU, que consiste en utilizar unidades de procesamiento gráfico (GPUs) para cálculos generales que no tienen por qué ser gráficos.

[fn:cudaSuperComp]
\newline
http://gpgpu.org/2010/11/17/gpus-in-3-of-5-fastest-supercomputers
\newline
http://blogs.nvidia.com/2011/11/gpu-supercomputers-show-exponential-growth-in-top500-list/

[fn:juegEstratAbst]
\newline
http://en.wikipedia.org/wiki/Abstract_strategy_game

