#+TITLE:       Entrenamiento por refuerzo de redes neuronales mediante algoritmos gen\'eticos
#+AUTHOR:      Jorge Tim\'on Morillo-Velarde
#+EMAIL:       jtimonmv@gmail.com
#+KEYWORDS:    Redes neuronales, algoritmos gen\'eticos, redes neuronales evolutivas, aprendizaje por refuerzo, CUDA, entornos artificiales, juegos multi-agente.
#+LANGUAGE:    es
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[margin=2.5cm,includefoot]{geometry}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{pict2e}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{chngcntr}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{import}
#+LATEX_HEADER: \hypersetup{
#+LATEX_HEADER:     colorlinks,%
#+LATEX_HEADER:     citecolor=green,%
#+LATEX_HEADER:     filecolor=black,%
#+LATEX_HEADER:     linkcolor=blue,%
#+LATEX_HEADER:     urlcolor=blue
#+LATEX_HEADER: }
#+LaTeX_HEADER: \newcommand{\murl}[2]{\url{#1://#2}}
#+OPTIONS:     toc:nil H:5
#+BIND: org-export-latex-title-command ""

#+TODO: HACER MODIFICAR | REVISAR HECHO

# definiciones propias
#+begin_latex

\setcounter{secnumdepth}{5}
\counterwithin{figure}{section}
\setcounter{tocdepth}{5}

\newcommand{\mail}[1][jtimonmv@gmail.com]{%
     \href{mailto:#1} {#1}
}

\newcommand{\definicion}[1]{%
	\textbullet \bfseries{ #1 :}
}

\newenvironment{listaDefiniciones}%
%ordenes al inicio
{
\begin{list}{}%
     {  \setlength{\itemsep}{0.5ex}
	\setlength{\parsep}{0.5ex}
	\setlength{\partopsep}{0.5ex}
	\setlength{\topsep}{\dimexpr 2\itemsep}
	\setlength{\listparindent}{\dimexpr \parindent}
	\renewcommand*{\makelabel}[1]{\definicion{##1}}
	}
}
%ordenes al final
{
\end{list}
}%

#+end_latex

# Título, abstract e índice
#+begin_latex


\begin{titlepage}

\title{Entrenamiento por refuerzo de redes neuronales mediante algoritmos gen\'eticos}
\newline
\newline
\newline

\author{
\\ Autor:\\
\\ Jorge Tim\'on Morillo-Velarde \\ \mail
\\ \\ \\ \\
\\ Tutores del proyecto:\\ 
\\ Rosa M. P\'erez Utrero \\ \mail[rosapere@unex.es]
\\ 
\\ Juan A. G\'omez Pulido \\ \mail[jangomez@unex.es]
\\ \\ \\ \\
}

\date{\today}
\maketitle

\newpage
\begin{abstract}

En este trabajo se estudia un m\'etodo alternativo para el entrenamiento de redes neuronales con conexi\'on hacia delante. Se utiliza un algoritmo gen\'etico para ajustar los pesos de la red neuronal. Se eval\'ua el uso de diferentes tipos de neuronas (con salida real o binaria) para comparar sus rendimientos utilizando diferentes implementaciones paralelas (para el coprocesador XMM y para la arquitectura CUDA). Se prueban variaciones de los operadores gen\'eticos y se mide su efectividad en el entrenamiento. Se enfrenta el algoritmo a diferentes tipos de problemas de aprendizaje por refuerzo y se medita sobre la idoneidad del mismo para cada problema.
\\

\textbf{Palabras clave:} Redes neuronales, algoritmos gen\'eticos, redes neuronales evolutivas, aprendizaje por refuerzo, CUDA, entornos artificiales, juegos multi-agente.

\end{abstract}

\newpage

\tableofcontents

\newpage
#+end_latex

* MODIFICAR Introducci\'on
#+LaTeX: \label{intro}

 El /aprendizaje autom\'atico/ es una rama de la inteligencia artificial que trata de construir sistemas inform\'aticos que optimicen un criterio de rendimiento utilizando datos o experiencia previa. Dentro del aprendizaje autom\'atico, las t\'ecnicas se clasifican en base a su entrenamiento como supervisado o no supervisado. En el primero, al sistema se le suministran ejemplos de entradas con sus correspondientes salidas deseadas. En el entrenamiento no supervisado, no se tienen a priori ejemplos de c\'omo deber\'ia comportarse el sistema. El aprendizaje por refuerzo es un caso especial de aprendizaje supervisado en que la salida deseada exacta es desconocida. Se basa s\'olo en informaci\'on sobre si la salida actual es correcta o no. Al agente que se quiere que aprenda se le provee informaci\'on sobre lo bien o lo mal que est\'a actuando. Esta se\~nal de recompensa puede serle indicada bien cada vez que el agente act\'ua, bien al final de una prueba completa durante la que realiza varias acciones.

 Los algoritmos que tienen su origen en la observaci\'on de la naturaleza se denominan bio-inspirados. Entre ellos se encuentran las redes neuronales y los algoritmos gen\'eticos. 

 Las redes neuronales son modelos que intentan reproducir el comportamiento del cerebro humano [Hilera y Mart\'inez, 1995]. Una red neuronal consiste en un conjunto de elementos de procesamiento, llamados neuronas, los cuales se conectan entre s\'i [Koehn, 1994]. Las conexiones entre las neuronas tienen pesos asociados cuyos valores determinar\'an el comportamiento de la red. Existen algoritmos para determinar el valor de los pesos de una red mediante un entrenamiento supervisado, cabe destacar el de retro-propagaci\'on del error.

 Los algoritmos evolutivos, dentro de los cuales los algoritmos gen\'eticos son los m\'as conocidos, son una familia de modelos computacionales inspirados en la evoluci\'on y la supervivencia del m\'as apto [B\"ach, et. al., 1991; \"Omer, 1995; Whitley, 2001]. Se utilizan fundamentalmente en la resoluci\'on de problemas de b\'usqueda y de optimizaci\'on [Holland, 1975]. Buscan una soluci\'on del problema reproduciendo gen\'eticamente una poblaci\'on de individuos a lo largo de una serie de generaciones [Koza, 1997]. El aprendizaje es formulado como un problema de optimizaci\'on, en el que cada individuo de la poblaci\'on es una posible soluci\'on.

 Dada una topolog\'ia de red fija, el entrenamiento de una red neuronal puede ser visto como un proceso de optimizaci\'on cuyo objetivo es encontrar un conjunto de pesos que minimice el error que produce la red sobre el conjunto de ejemplos en el entrenamiento supervisado, o que maximice la recompensa en el aprendizaje por refuerzo.

 En nuestro caso, el agente utilizar\'a una red neuronal para decidir sus acciones (salida de la red) a partir de los datos que pueda recoger de su entorno (entrada a la red). Se utiliza un algoritmo gen\'etico para decidir los pesos adecuados para la red, utilizando una poblaci\'on de agentes con redes diferentes. Se utiliza la recompensa acumulada por el individuo en una o varias pruebas para medir su adaptaci\'on al medio. Se utilizan estas medidas y la poblaci\'on actual para construir la siguiente generaci\'on. Cuando alg\'un individuo demuestra estar lo suficientemente adaptado al medio para cumplir con las expectativas del entrenamiento (o cuando se supera un l\'imite prefijado de iteraciones), \'este finaliza.

 En todo este proceso, El algoritmo que m\'as se ejecuta es el que calcula la salida de la red a partir de la entrada. Este algoritmo se puede llamar varias veces por cada prueba. Adem\'as, las pruebas pueden repetirse varias veces por individuo y generaci\'on para disminuir el ruido producido por los factores aleatorios que pueda haber en las pruebas. Por ello, esta funci\'on se ha optimizado mediante su paralelizaci\'on en dos arquitecturas ampliamente extendidas y econ\'omicas: el coprocesador XMM, presente en todos los PCs de reciente fabricaci\'on y la arquitectura CUDA[Nota al pie: CUDA forma parte de lo que se conoce como GPGPU, que consiste en utilizar unidades de procesamiento gr\'afico (GPUs) para c\'alculos generales que no tienen por qu\'e ser gr\'aficos], compatible con la mayor\'ia de tarjetas gr\'aficas NVIDIA a partir de la serie 8000. Adem\'as, para incrementar a\'un m\'as el rendimiento, se estudia la viabilidad de entrenar redes con una versi\'on m\'inima de neurona con activaci\'on de tipo escal\'on y con pesos con tama\~no de un byte (en la secci\'on \ref{disenoParal} se describe con m\'as detalle). Llamaremos a las redes que utilizan estas estructuras redes discretas.

 Los problemas en los que aplicamos nuestro desarrollo, aunque no son realistas, se consideran interesantes para la rob\'otica y/o para la inteligencia artificial. Los tipos de problemas tienen que ver con la clasificaci\'on, los juegos considerados como deportes mentales, el control autom\'atico, la toma de decisiones en tiempo real y la colaboraci\'on de m\'ultiples agentes en tiempo real; pudiendo estar relacionados estos \'ultimos con la
[[http://es.wikipedia.org/wiki/Vida_artificial][vida artificial]]. En la mayor\'ia de los problemas, el agente se enfrenta a pruebas gen\'ericas que pueden tener factores aleatorios y/o agentes directamente programados. En algunos problemas, sin embargo, los agentes de la poblaci\'on pueden enfrentarse entre ellos para obtener una valoraci\'on.

 En el presente documento (que contiene la documentaci\'on asociada al proyecto fin de carrera titulado \flqq Entrenamiento por refuerzo de redes neuronales mediante algoritmos gen\'eticos\frqq, desarrollado por Jorge Tim\'on Morillo-Velarde para la consecuci\'on del t\'itulo de ingenier\'ia inform\'atica), primero comentaremos los fundamentos te\'oricos precisos para su comprensi\'on y su correcta ubicaci\'on dentro del dominio de la neuro-evoluci\'on, diferenci\'andolo de otros desarrollos en \'este \'area. Despu\'es, justificaremos las principales decisiones tomadas durante el desarrollo y explicaremos en detalle algunas partes de su implementaci\'on. Los rendimientos obtenidos con las diferentes implementaciones paralelas y opciones en el algortimo genético se muestran en el capítulo \ref{rendimiento}.En el cap\'itulo \ref{experimentacion}, se describen los experimentos realizados y se justifica la elecci\'on de los mismos. El cap\'itulo de resultados de aprendizaje \ref{aprendizaje} expone los datos emp\'iricos recogidos en los experimentos y razona unas someras conclusiones que luego se completan en el cap\'itulo \ref{conclusiones}.

\newpage
* MODIFICAR [0/4] Base te\'orica
#+LaTeX: \label{baseTeorica}

Tanto las redes neuronales como los algoritmos gen\'eticos est\'an inspirados en la naturaleza y han sido utilizados desde largo tiempo atr\'as. Las redes neuronales est\'an inspiradas en el funcionamiento del cerebro, como un sistema de procesamiento de informaci\'on distribuido. Por su parte, los algoritmos gen\'eticos se basan en la teor\'ia de la evoluci\'on de Darwin, por la que la evoluci\'on se produce a trav\'es de dos principios b\'asicos: los individuos que no se adaptan suficientemente al medio perecen, mientras que los que s\'i lo hacen transmiten sus genes con cierta variabilidad. Esta variabilidad puede proceder de dos fuentes: la cruza de genes entre individuos \'o la mutaci\'on directa de genes.

La combinaci\'on de estas dos t\'ecnicas es algo relativamente reciente. Algunos - principalmente anglosajones - han coincidido en llamar a esta s\'intesis [[http://en.wikipedia.org/wiki/Neuroevolution][neuro-evoluci\'on]], otros se refieren a ella como [[http://laral.istc.cnr.it/nolfi/papers/HBTNN-A.pdf][evoluci\'on de redes neuronales artificiales]], pero - en general - la mayor\'ia de los que la usan recurren a nombres que definen con m\'as precisi\'on la t\'ecnica concreta que utilizan; dadas las m\'ultiples posibilidades para combinar ambos m\'etodos. Aceptaremos el t\'ermino neuro-evoluci\'on para el resto del texto, aunque sin renunciar a los otros t\'erminos que hayan podido ser utilizados como redes neuronales evolutivas.

A continuaci\'on explicaremos m\'as detalladamente las bases te\'oricas de las tres t\'ecnicas: redes neuronales, algoritmos gen\'eticos y neuro-evoluci\'on. Nos centraremos principalmente en los algoritmos y estructuras que m\'as se asemejan a los implementamos en nuestra librer\'ia.

** MODIFICAR Redes neuronales
#+LaTeX: \label{basTeoRedes}

Las redes neuronales constan de un conjunto de elementos de procesamiento - conocidos como nodos o neuronas - interconectados entre s\'i. Pueden ser descritas mediante un grafo dirigido en el que cada neurona  \(i\) usa una funci\'on de activaci\'on de la forma:

\begin{equation}\label{eqSalidaNeu}
  y_i=f_i(\sum_{j=1}^n (w_{ij} \cdot x_j - \theta_i)).
\end{equation}

donde \(y_i\) es la salida de la neurona \(i\), \(x_j\) es la entrada n\'umero \(j\) a la misma, \(w_{ij}\) es el peso de la conexi\'on entre los nodos \(i\) y \(j\), \(\theta_i\) es el umbral de activaci\'on (o Bias) y \(f_i\) es una funci\'on que puede ser, o no, lineal.

#+CAPTION:    Red neuronal \emph{feed-forward}.
#+LABEL:      figFeedForward
#+ATTR_LaTeX: trim= 0.5cm 22cm 10cm 0cm, clip, width=15cm
[[./img/feed-forward.jpg]]

 Las redes neuronales artificiales pueden clasificarse como \emph{feed-forward} (con propagaci\'on hacia delante) o recurrentes dependiendo de su conectividad. Una red es \emph{feed-forward} (figura \ref{figFeedForward}) si existe un m\'etodo de numeraci\'on de las neuronas que cumpla que no existan conexiones desde un nodo hacia otro nodo con un n\'umero m\'as peque\~no que el de nodo de origen. Una red es recurrente (figura \ref{figRecurrente}) si no existe un m\'etodo de numeraci\'on que cumpla tal condici\'on. Para simplificar nuestro trabajo, nos centraremos en las redes \emph{feed-forward}.

#+CAPTION:    Red neuronal recurrente.
#+LABEL:      figRecurrente
#+ATTR_LaTeX: scale=0.35
[[./img/recurrente.jpg]]

 El aprendizaje de las redes neuronales se consigue habitualmente usando ejemplos: suelen tener un entrenamiento supervisado. Se basa en la comparaci\'on directa entre la salida de la red y la salida correcta o deseada. Normalmente se formula el entrenamiento como la minimizaci\'on de una funci\'on de error como el sumatorio del cuadrado del error de la salida respecto de la salida deseada para todos los datos disponibles (que constan de pares de entradas con sus correspondientes salidas deseadas). Un algoritmo de optimizaci\'on basado en el descenso del gradiente como la regla delta generalizada (tambi\'en conocido como algoritmo backpropagation) puede ser usado despu\'es iterativamente para ajustar los pesos y as\'i minimizar el error.

 En nuestro caso, utilizamos aprendizaje por refuerzo y no necesitamos una colecci\'on de ejemplos (aunque se podr\'ia utilizar para calcular el refuerzo). Los pesos los ajustar\'a un algoritmo gen\'etico. La estructura de la red se definir\'a de forma previa para cada problema y s\'olo evolucionar\'an los pesos (y umbrales).

** MODIFICAR Algoritmos gen\'eticos
#+LaTeX: \label{basTeoGenet}

Los algoritmos gen\'eticos son m\'etodos sistem\'aticos para la resoluci\'on de problemas de b\'usqueda y optimizaci\'on que aplican a \'estos los principios de la evoluci\'on biol\'ogica: selecci\'on basada en la poblaci\'on, reproducci\'on sexual y mutaci\'on.

 Los algoritmos gen\'eticos son m\'etodos de optimizaci\'on, que tratan de resolver el conjunto de problemas formulados como: hallar (xi,...,xn) tales que F(xi,...,xn) sea m\'aximo. En un algoritmo gen\'etico, tras parametrizar el problema en una serie de variables (xi,...,xn), se codifican en un cromosoma. Todos los operadores utilizados por un algoritmo gen\'etico se aplicar\'an sobre estos cromosomas, o sobre poblaciones de ellos. En el algoritmo gen\'etico va impl\'icito el m\'etodo para resolver el problema; son s\'olo par\'ametros de tal m\'etodo los que est\'an codificados - a diferencia de otros algoritmos evolutivos como la programaci\'on gen\'etica. Hay que tener en cuenta que un algoritmo gen\'etico es independiente del problema, lo cual lo hace un algoritmo robusto, por ser \'util para cualquier problema, pero a la vez d\'ebil, pues no est\'a especializado en ninguno.

 Las soluciones codificadas en un cromosoma compiten para ver cu\'al constituye la mejor soluci\'on (aunque no necesariamente la mejor de todas las soluciones posibles). El ambiente, constituido por las otras camaradas soluciones, ejercer\'a una presi\'on selectiva sobre la poblaci\'on, de forma que s\'olo los mejor adaptados (aquellos que resuelvan mejor el problema) sobrevivan o leguen su material gen\'etico a las siguientes generaciones, igual que en la evoluci\'on de las especies. La diversidad gen\'etica se introduce mediante mutaciones y reproducci\'on sexual. En la Naturaleza lo \'unico que hay que optimizar es la supervivencia, y eso significa a su vez maximizar diversos factores y minimizar otros. Un algoritmo gen\'etico, sin embargo, se usar\'a para optimizar habitualmente para optimizar s\'olo una funci\'on, no diversas funciones relacionadas entre s\'i simult\'aneamente. Este tipo de optimizaci\'on, denominada optimizaci\'on multimodal, tambi\'en se suele abordar con un algoritmo gen\'etico especializado.

 Por lo tanto, un algoritmo gen\'etico consiste en lo siguiente: hallar de qu\'e par\'ametros depende el problema, codificarlos en un cromosoma, y se aplican los m\'etodos de la evoluci\'on: selecci\'on y reproducci\'on sexual con intercambio de informaci\'on y alteraciones que generan diversidad. En las siguientes secciones se ver\'an cada uno de los aspectos de un algoritmo gen\'etico.

 Mediante los operadores de selecci\'on, se eligen los individuos que ser\'an progenitores de la siguiente generaci\'on (o directamente formar\'an parte de ella). Con los operadores de cruza, se generan nuevos individuos mezclando los cromosomas de varios individuos (normalmente, dos). Por \'ultimo, los operadores de mutaci\'on a\~naden cambios aleatorios a los individuos. La funci\'on de fitness nos da una aproximaci\'on de la adaptaci\'on del individuo al medio y \'esta es utilizada por los operadores de selecci\'on.

 En nuestro caso, el cromosoma de cada individuo lo forman los pesos de la red que utiliza ese individuo. Para calcular el fitness del individuo, se construir\'a la red con los pesos del cromosoma y se realizar\'an varias pruebas (para reducir el ruido generado por los posibles factores aleatorios de \'estas) sobre el individuo, sumando las recompensas de todas y obteniendo el citado fitness.

*** Algoritmo genético estándar y variaciones
#+LaTeX: \label{basTeoGenetEstan}

*** Operadores de selección
#+LaTeX: \label{basTeoGenetSel}

*** Operadores de cruza
#+LaTeX: \label{basTeoGenetCruz}

** MODIFICAR Fortalezas y deficiencias
#+LaTeX: \label{baseTeoricaFort}

Tanto las redes neuronales como los algoritmos gen\'eticos tienen fortalezas que nuestro m\'etodo aprovecha y debilidades que se pueden, en parte, minimizar por la combinaci\'on de ambos m\'etodos.

*** Redes neuronales
#+LaTeX: \label{baseTeoricaFortRedes}

Las redes neuronales con conexi\'on hacia delante en general son un importante m\'etodo de aproximaci\'on de funciones [Kim, 1992]. El perceptr\'on multicapa es un tipo de red neuronal con conexiones hacia delante. La topolog\'ia de un perceptr\'on multicapa esta definida por un conjunto de capas ocultas, una capa de entrada y una de salida. No existen restricciones sobre la funci\'on de activaci\'on aunque en general se suelen utilizar funciones sigmoideas. Existen demostraciones te\'oricas [Funahashi, 1989] de que un perceptr\'on multicapa cuya funci\'on de activaci\'on sea no constante, acotada y mon\'otona creciente es un aproximador universal de funciones. En [Hornik et alt, 1989] se llega a un resultado similar utilizando funciones de activaci\'on sigmoideas, no necesariamente continuas. Esto es un punto muy fuerte de las redes neuronales. 

Adem\'as, constituyen buena una herramienta para la construcci\'on de agentes pues s\'olo hay que codificar las entradas y las salidas de la red como las del agente y el tiempo de ejecuci\'on de la red s\'olo depende de la topolog\'ia de \'esta (para una topolog\'ia dada, es constante).

Algunas deficiencias del algoritmo back-propagation son su baja adaptabilidad, la alta dependencia de los par\'ametros del algoritmo, el estancamiento en m\'inimos locales, la posibilidad de par\'alisis y la alta dependencia de las condiciones iniciales.

\begin{listaDefiniciones}

\item [Adaptabilidad] El algoritmo tiene como premisa la utilizaci\'on de una funci\'on de activaci\'on derivable [Walker, 1995]. Al hacer uso de la derivada de la funci\'on de activaci\'on, es condici\'on necesaria para la aplicaci\'on del algoritmo que la misma sea continua y derivable en todo el dominio de aplicaci\'on [Wilson, 1994]. Esto impide la utilizaci\'on del m\'etodo en otras topolog\'ias donde la funci\'on de activaci\'on presenta discontinuidades.

Este problema suele encontrarse en varios m\'etodos de entrenamiento, los cuales son desarrollados para una determinada topolog\'ia y sus resultados, en general, no son extensibles directamente a otras topolog\'ias. Es necesario adaptar los m\'etodos para aplicarlos a otras topolog\'ias.

\item [Dependencia de par\'ametros del algoritmo] Los algoritmos de gradiente descendente hacen uso de una tasa de aprendizaje que idealmente deber\'ia ser infinitesimal. De esta manera, mediante peque\~nos ajustes de los pesos sin\'apticos el algoritmo converge hacia un m\'inimo. El uso de tasas de aprendizaje muy peque\~nas hace que el algoritmo tenga una convergencia estable hacia un m\'inimo, aunque el tiempo necesario para alcanzarlo puede llegar a ser muy alto. Como consecuencia de lo dicho anteriormente, y con el objetivo de disminuir el tiempo de convergencia del algoritmo, en la pr\'actica se suelen utilizar tasas de aprendizajes mayores a las te\'oricas. El aumento de la tasa de aprendizaje disminuye el tiempo de convergencia, pero tiene un efecto contraproducente: el algoritmo comienza a oscilar en torno a un m\'inimo, disminuyendo la probabilidad de alcanzarlo. El efecto de oscilaci\'on puede reducirse mediante la adici\'on de una tasa de momento, como se describi\'o en el cap\'itulo 3, pero no puede eliminarse.

El algoritmo backpropagation es muy dependiente de los par\'ametros mencionados previamente. Dependiendo de la selecci\'on de par\'ametros realizadas el resultado de la aplicaci\'on del algoritmo ser\'a exitosa o no [Liu et alt, 2004]. Peque\~nas variaciones sobre los par\'ametros del algoritmo pueden conducir a resultados diferentes. El principal problema es que no existe un m\'etodo general que permita establecer el valor de estos par\'ametros [Branke, 1995]. Los par\'ametros que aseguran la convergencia para un determinado problema pueden no ser aplicables a otro problema. De esta manera, la selecci\'on de los par\'ametros del algoritmo se realiza en base a la experiencia del dise\~nador, y se realiza un refinamiento de los mismos mediante mecanismos de prueba y error. Esto produce un aumento en el tiempo total de dise\~no y entrenamiento de la red.

\item [M\'inimos locales] La superficie que define la funci\'on de error E (ecuaci\'on 8) en base a los par\'ametros de la red neuronal es compleja y esta llena de valles y colinas. Debido a la utilizaci\'on del gradiente para encontrar el m\'inimo de dicha funci\'on de error se corre el riesgo de que el proceso de entrenamiento quede atrapado en un m\'inimo local [Sutton, 1986]. Esta situaci\'on no es deseable, fundamentalmente si dicho m\'inimo esta localizado lejos del m\'inimo global.

Existen algunos mecanismos para evitar que esto suceda. Una posible soluci\'on para evitar que el entrenamiento quede atrapado en un m\'inimo local es aumentar el n\'umero de neuronas ocultas de la red. Este mecanismo puede ayudar en aquellos casos en los que la red tiene escaso poder de representaci\'on interna, y no es capaz de distinguir entre dos patrones diferentes, proporcionando una misma salida para ambos patrones. Al aumentar el n\'umero de neuronas ocultas la red posee mayor cantidad de par\'ametros libres y puede conseguir una mejor representaci\'on interna.

Otros mecanismos que ayudan a disminuir los efectos de este problema son la adici\'on de una tasa de momento al proceso de entrenamiento, utilizar una tasa de aprendizaje decreciente a lo largo del proceso, partir de otras configuraciones iniciales de la red, a\~nadir ruido al m\'etodo de gradiente, etc.

\item [Par\'alisis] El fen\'omeno de par\'alisis, tambi\'en conocido como saturaci\'on, se produce cuando la entrada total a una neurona de la red toma valores muy altos, ya sean positivos o negativos. Al utilizar funciones de activaci\'on sigmoidales, la funci\'on de activaci\'on posee dos as\'intotas horizontales. Si la entrada de la neurona alcanza un valor alto, la funci\'on de activaci\'on se satura y alcanza un valor de activaci\'on m\'aximo o m\'inimo.

Cuando la funci\'on de activaci\'on se satura su derivada tiende a hacerse nula, haciendo que los par\'ametros de la red permanezcan invariables y, como consecuencia, la suma de los errores locales permanece constante por un largo periodo de tiempo [Kr\"ose y van der Smagt, 1993]. Aunque esta situaci\'on se suele confundir con un m\'inimo local, pues el error permanece invariable, en este caso es posible que despu\'es de un cierto tiempo el error comience nuevamente a decrecer.

El fen\'omeno de par\'alisis del perceptr\'on multicapa ocurre fundamentalmente cuando los par\'ametros de la red toman valores muy altos. Un mecanismo para evitar esto consiste en partir de valores iniciales bajos.

\item [Condiciones iniciales] El conjunto de pesos iniciales de la red neuronal generalmente se selecciona de manera aleatoria. Sin embargo, el algoritmo backpropagation es muy dependiente de las condiciones iniciales seleccionadas [Kolen, 1991]. Peque\~nas variaciones realizadas sobre las condiciones iniciales pueden llevar a grandes diferencias en el tiempo de convergencia del algoritmo.
\end{listaDefiniciones}

 A esto hay que a\~nadir que los algoritmos de gradiente requieren entrenamiento supervisado (normalmente, no funcionan para el aprendizaje por refuerzo) y que las conexiones sean hacia delante (la retro-propagaci\'on del error no se puede aplicar en redes recurrentes). 

 Usando un algoritmo gen\'etico como m\'etodo de entrenamiento de la red, se solucionan algunos de estos problemas y otros se mitigan en cierto grado. Con el algoritmo gen\'etico, se puede usar el aprendizaje por refuerzo y se pueden entrenar redes recurrentes sin problema. No se tienen requerimientos para la funci\'on de activaci\'on, por lo que aumenta su adaptabilidad. Se cambia la dependencia de los par\'ametros de ese algoritmo y ahora depende de los par\'ametros del algoritmo gen\'etico, estos par\'ametros son m\'as flexibles y se pueden alterar en medio del entrenamiento. El algoritmo gen\'etico es mucho menos tendente a estancarse en m\'inimos locales porque no utiliza la informaci\'on del gradiente y porque explora varios puntos (tantos como individuos tenga la poblaci\'on) del espacio de b\'usqueda simult\'aneamente. El fen\'omeno de saturaci\'on se produce cuando una neurona alcanza un m\'aximo o un m\'inimo. En este caso, la derivada de la funci\'on de activaci\'on se hace nula, y los pesos de la red permanecen invariables. Como el m\'etodo propuesto no hace uso de la derivada de la funci\'on de activaci\'on, el efecto de este fen\'omeno es completamente eliminado. Los valores iniciales de los pesos tambi\'en pueden afectar al algoritmo gen\'etico, en especial si son muy altos (ya sean positivos o negativos), pero existen experimentos que permiten afirmar que el m\'etodo propuesto es menos dependiente de los valores iniciales que el algoritmo backpropagation \cite[Bertona2005]{Bertona2005}.

*** Algoritmos gen\'eticos
#+LaTeX: \label{baseTeoricaFortRedes}

Un algoritmo gen\'etico es independiente del problema, lo cual lo hace un algoritmo robusto, por ser \'util para cualquier problema, pero a la vez d\'ebil, pues no est\'a especializado en ninguno. Hay que elegir la codificaci\'on de los cromosomas para cada caso concreto. Sin embargo, con nuestro m\'etodo siempre c\'odificaremos los cromosomas de manera similar (con una red neuronal) y s\'olo ser\'a necesario definir la funci\'on de fitness, elegir la topolog\'ia de la red, codificar las entradas y las salidas. Aunque la codificaci\'on de \'estas pueda admitir varias posibilidades (y algunas puedan ser m\'as ventajosas que otras) la red debe aprender a interpretar las correctas relaciones entre entradas y salidas por s\'i misma.

** MODIFICAR Neuro-evoluci\'on
#+LaTeX: \label{basTeoNeuro}

 La evoluci\'on se ha aplicado las redes neuronales artificiales en tres niveles muy diferentes: a los pesos de las conexiones, la arquitectura de la red y a las reglas de aprendizaje. La evoluci\'on de los pesos de las conexiones introduce una aproximaci\'on global y adaptable al entrenamiento, especialmente para el aprendizaje por refuerzo o para el entrenamiento de redes recursivas, donde los m\'etodos basados en el gradiente experimentan grandes dificultades. La evoluci\'on de las arquitecturas permite a las redes neuronales adaptar su topolog\'ia a diferentes problemas sin intervenci\'on humana y con esto se consigue un dise\~no autom\'atico de redes neuronales, dado que tanto la arquitectura como los pesos pueden ser evolucionados. La evoluci\'on de las reglas de aprendizaje puede ser considerada como un proceso de \textquotedblleft aprender a aprender\textquotedblright en redes neuronales donde la adaptaci\'on de las reglas de aprendizaje se consigue mediante la evoluci\'on. Tambi\'en puede ser contemplada como un proceso de descubrimiento autom\'atico de nuevas reglas de aprendizaje. Nos centraremos en la evoluci\'on de los pesos de las conexiones, por ser la evoluci\'on que utilizaremos.

 La evoluci\'on de los pesos de las conexiones se puede realizar en el aprendizaje supervisado (con ejemplos) definiendo la funci\'on de fitness como el error global obtenido por la red (invirtiendo el signo), comparando las salidas de la red y la salida deseada para cada ejemplo. Tambi\'en puede utilizar para el aprendizaje por refuerzo definiendo una funci\'on de fitness distinta.

 En general, los pasos a seguir son dos: decidir la codificaci\'on de los pesos de las conexiones (si se har\'a mediante cadenas binarias o no) y la ejecuci\'on del algoritmo gen\'etico propiamente dicho. Para el primer paso, las opciones m\'as extendidas son la representaci\'on binaria y la representaci\'on con n\'umeros reales.

   El algoritmo gen\'etico can\'onico siempre usa cadenas de bits para codificar las diferentes soluciones. Por ello, algunos trabajos tempranos de evoluci\'on de los pesos de las conexiones siguen esta aproximaci\'on \cite[Yao99]{Yao99}. Las ventajas son la f\'acil aplicaci\'on de los operadores gen\'eticos y su posible implementaci\'on digital. Habr\'ia que elegir la representaci\'on de los n\'umeros reales. Aqu\'i hay un compromiso para la precisi\'on con que se quieran representar los n\'umeros reales. Si se usan muy pocos bits para representar cada conexi\'on, el entrenamiento puede fallar porque algunas combinaciones de pesos no se pueden aproximar con suficiente precisi\'on por valores discretos. Por otra parte, si se usan demasiados bits, los cromosomas que representen a redes neuronales grandes se volver\'an demasiado largos y la evoluci\'on en proceso resultar\'a muy ineficiente.

 Por su parte, en la representaci\'on con n\'umeros reales, los cromosomas se codifican como vectores de n\'umeros reales con tantos elementos como conexiones. Los operadores gen\'eticos no se pueden aplicar directamente sobre los bits y han de ser dise\~nados de nuevo. Esto puede ser una ventaja, pues, por ejemplo, el operador de mutaci\'on podr\'ia tener una distribuci\'on gaussiana (u otra funci\'on) en lugar de mutar un bit cualquiera sin tener en cuenta su peso en la construcci\'on del n\'umero.

\begin{figure}[t]
\begin{minipage}{0.45\textwidth}
    \includegraphics [width=7.20cm]{./img/grafo1.jpg}
  \caption {Red neuronal y su codificaci\'on binaria (asumiendo que se usan 4 bits para representar cada n\'umero real).}\label{figGrafo1}
\end{minipage}
\begin{minipage}{0.10\textwidth}
\hfill
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \includegraphics [width=7.20cm]{./img/grafo2.jpg}
  \caption{Red equivalente con codificaci\'on alternativa.}\label{figGrafo2}
\end{minipage}
\end{figure}

 Uno de los problemas a los que se enfrenta la evoluci\'on de redes neuronales es el problema de la permutaci\'on. Es causado por el mapeado "muchos-a-uno" desde la representaci\'on en el cromosoma a la red que es construida. Con dos cromosomas distintos se pueden generar redes equivalentes como se muestra en las figuras \ref{figGrafo1} y \ref{figGrafo2}. Se puede solucionar dando m\'as importancia al operador de mutaci\'on que al de cruza (que es el que sufre con este problema) o con otros m\'etodos matem\'aticos \cite[Gomez, Miikkulainen 2003]{GomezMiikkulainen2003}. 

\newpage
* MODIFICAR [0/2] An\'alisis del problema
#+LaTeX: \label{analisis}
** MODIFICAR Objetivos
#+LaTeX: \label{anaObjetivos}

Se probar\'a la librer\'ia en casos concretos con el fin de contestar a las siguientes cuestiones:

1) ?`Qu\'e ventajas en el rendimiento se pueden obtener gracias a la paralelizaci\'on?

2) ?`Se puede simplificar la estructura de las redes neuronales para mejorar la paralelizaci\'on? ?`Qu\'e efecto tienen las funciones de tipo escal\'on (que permiten codificar la salida de cada neurona como un bit en vez de como un n\'umero real) tanto en el rendimiento como en el aprendizaje? ?`Qu\'e efecto tiene la codificaci\'on de los pesos en estructuras discretas (en lugar de n\'umeros reales) tanto en el rendimiento como en el aprendizaje?

3) ?`Qu\'e operadores gen\'eticos resultan m\'as adecuados para el entrenamiento en diferentes problemas? ?`Qu\'e valores de los par\'ametros del algoritmo gen\'etico resultan m\'as adecuados para el entrenamiento en diferentes problemas?

4) ?`Para qu\'e tipo de problemas resulta m\'as adecuado el m\'etodo propuesto? 

\newpage

** MODIFICAR Especificaciones de la librería a implementar
#+LaTeX: \label{anaEspecLib}

En el presente proyecto se pretende construir una librer\'ia de programaci\'on en C++ para la utilizaci\'on de redes neuronales con entrenamiento mediante algoritmos gen\'eticos. Se quiere que sea lo m\'as flexible posible en cuanto a la estructura de la red, para poder, en un futuro, determinar la topolog\'ia tambi\'en de forma gen\'etica. Por tanto, con la librer\'ia implementada debe ser posible crear una cualquier red con una arquitectura arbitraria. Deben ser posibles conexiones recurrentes y conectar capas con tipos de datos diferentes (por ejemplo, que una capa cuya salida son n\'umeros en coma flotante debe poder usar como entrada una capa que tiene bits como salida).

Como los entrenamientos pueden ser costosos en tiempo de ejecuci\'on, la librer\'ia debe estar paralelizada internamente al menos para la ejecuci\'on de redes neuronales. Esta paralelizaci\'on debe poder aprovecharse por los sistemas m\'as extendidos para que pueda ser utilizada en proyectos que aprovechen la computaci\'on voluntaria.
* HACER Diseño general
#+LaTeX: \label{diseno}

Como se definió en la sección \ref{anaEspecLib}, la librería debe poder construir redes neuronales de cualquier topología y, al mismo tiempo, debe poder ser paralelizada usando diferentes tecnologías. Además, en la sección \ref{anaObjetivos} establecimos que las neuronas pueden ser de varios tipos (binarias, bipolares y reales). 

Para soportar las diferentes implementaciones y tipos de neuronas sin incrementar la complejidad de la API de la librería, se definirán clases abstractas como interfaces de las que luego heredarán las diferentes implementaciones. Para independizar completamente el manejo de estas clases de fachada [TODO bibliografía patrón diseño facade], las implementaciones concretas sólo serán visibles a una clase factoría que será el único método para instanciar las implementaciones siguiendo el patrón de diseño factoría [TODO bibliografía patrón diseño factory]. Para las diferentes implementaciones paralelas descritas en el capítulo \ref{disenoParal}, se crearán diferentes clases que extiendan de las fachadas. Para soportar los diferentes tipos de neuronas, estas clases paralelizadas se implementarán usando plantillas. Sólo serán utilizadas directamente las clases fachada y los métodos específicos de cada implementación concreta serán llamados utilizando la técnica que en el contexto de análisis, diseño y desarrollo orientado a objetos se denomina polimorfismo.

Primero se describirán las clases fachada y el resto de clases utilizadas para la implementación de las redes neuronales en la sección \ref{disenoRedes}. Las implementaciones concretas de las fachadas para la factoría se describirán con más detalle en la sección [TODO decidir dónde: diseño, paralelizaciones, extensibilidad]. En la sección \ref{disenoGenetic} especificaremos de forma general las clases destinadas a la implementación del algoritmo genético y cómo se relacionan con las clases de las redes neuronales. Finalmente, en la sección \ref{disenoLoop} se describen las utilidades destinadas a probar la librería y medir su eficiencia, tanto en términos de rendimiento computacional como en términos de aprendizaje. Este último componente debe poder generar gráficas comparativas y ser suficientemente extensible para adaptarse a las necesidades del proyecto. 

** HACER Estructura de las redes neuronales
#+LaTeX: \label{disenoRedes}

Describiremos clases utilizadas empezando desde el nivel más bajo hasta llegar a la clase NeuralNet que implementa una red neuronal completa.

*** HACER Interface: 
#+LaTeX: \label{disenoRedesInter}

Para unificar ciertas tareas comunes y tener una estructura de datos manipulable desde

*** HACER Buffer: 
#+LaTeX: \label{disenoRedesBuff}

Tanto las entradas, las salidas como los pesos se almacenana en estructuras lineales similares a vectores matemáticos. Pero la representación ínterna de estos datos puede variar bastante dependiendo de la técnica de paralelización utilizada y el tipo de neuronas. Por esta razón y para mantener una API limpia y un código legible y mantenible se implementa la clase Buffer. Esta clase abstracta implementa varios métodos comunes e impone a la clases hijas la implementación de ciertos métodos en los que se describirán las pecualiaridades de cada representación, sin exponerlas al código que utilice la propia clase Buffer. Como hemos dicho, a esas peculiaridades se accederá utilizando polimorfismo. 

Los métodos que Buffer ofrece son:

- El destructor: aunque nunca es recomendable llamarlo explícitamente, como para cualquier otra clase. Notesé que el constructor no es público y, por tanto, no se podrá instanciar esta clase directamente. Se deberá utilizar siempre la clase Factory
- copyFromInterface(Interface* interface): 
- void copyToInterface(Interface* interface)
- virtual void copyFrom(Buffer* buffer)
- virtual void copyTo(Buffer* buffer)
- virtual Buffer* clone()
- void* getDataPointer()
- unsigned getSize()
- Interface* toInterface()
- void save(FILE* stream)
- void load(FILE* stream)
- void print()
- float compareTo(Buffer* other)
- void random(float range)



- ImplementationType getImplementationType()
    virtual BufferType getBufferType() = 0;
    virtual void reset() = 0;
*** HACER Connection:
*** HACER Layer:
*** HACER InputLayer:
*** HACER NeuralNet:

#+LaTeX: \label{disenoNeural}

Esta es la clase que gestiona las redes neuronales al nivel más alto. Contiene un grafo dirigido con capas (Layer )
Al nivel más alto, la clase que utilizaremos es NeuralNet, que implementa una red neuronal concreta

** HACER Estructuras para algoritmos genéticos
#+LaTeX: \label{disenoGenetic}
** HACER Utilidades para la experimentación
#+LaTeX: \label{disenoLoop}
* REVISAR Dise\~no del algoritmo gen\'etico 
#+LaTeX: \label{disenoGene}
** Funcionamiento general
#+LaTeX: \label{disenoGeneFunc}

Como se vió en la sección \ref{basTeoGenetEstan} existen diferentes enfoques en cuanto a la gestión de la población de individuos. El algoritmo genético original adoptaba la política de reemplazo generacional [biblio], con el que la población completa es reemplazada en cada generación. En cambio, la política de estado estacionario [biblio], adoptada por varios algoritmos genéticos posteriores, reemplaza la población selectivamente. Es posible, por ejemplo, que mantener uno o varios miembros de la población por varias generaciones, siempre que estos mantengan su puntuación por encima de otros individuos de la población. Nuestra gestión de la población debe permitir ambas posibilidades de forma configurable.

Para ello, mantendremos a la población como una lista ordenada en la que se irán insertando (también ordenadamente) los nuevos individuos producidos. Si tras una inserción se tienen más individuos que el tamaño máximo, el peor individuo (sea el nuevo o no) será desechado. Si dos individuos comparten la misma puntuación al ser comparados durante una inserción, se le dará ventaja al nuevo individuo siguiendo el criterio de busqueda neutral [biblio]. Este comportamiento es el propio del estado estacionario. Para obtener el comportamiento generacional, así como diferentes híbridos entre las dos posibilidades, definiremos una variable configurable para la población. Tras generar a los individuos de la siguiente generación, el sistema mirará esta variable para saber cuantos de los antiguos individuos debe conservar para competir con los nuevos y simplemente elimina al resto. Si el numéro de individuos a preservar es 0, el comportamiento será el generacional puro. Si el número de individuos a preservar es igual al tamaño máximo de la población (o es un número negativo), no se eliminará a ningún individuo de la generación anterior y todos ellos tendrán la oportunidad de sobrevivir compitiendo con los de la nueva generación. Si el número es algo intermedio entre 0 y el tamaño máximo de la población, estaremos usando un híbrido entre las políticas de reemplazo generacional y la de estado estacionario.

En general, para cada nueva generación se realiza la siguiente secuencia de acciones:

1) Selección: se puede definir una cantidad independiente de individuos a seleccionar con cada operador de selección. De esta manera, se pueden utilizar varios operadores de selección simultaneamente y combinarlos de infinidad de formas. Se deben seleccionar un mínimo de dos progenitores en cada generación para que el siguiente fallo no resulte en error.

2) Cruza: una vez seleccionados los progenitores, se genera a partir de ellos la descendencia, los nuevos individuos. Los progenitores se van eligiendo aleatoriamente y si van marcando para no ser usados dos veces. Si se han seleccionado menos individuos de los que se quieren generar mediante cruza, cuando todos hayan sido usados una vez se desmarcarán para poder ser reutilizados y continuar con la generación de la descendencia mediante la cruza. Por tanto, el número de nuevos individuos por generación puede ser tanto mayor como menor al número de progenitores seleccionados. Además, como ocurría en la selección, varios operadores de cruza diferentes pueden combinarse también. En este caso, cada operador de cruza puede ser aplicado a un nivel de cruza diferente (ver sección \ref{disenoGeneNiv}) y cada una de estas combinaciones se le puede asignar un número independiente de individuos a generar por cruza. Por tanto, en este caso las posibilidades son aún más abundantes que para la selección.

3) Olvido: a cada uno de los individuos de la descendencia se le aplica el operador de olvido determinístico o probabilístico (o los dos, aunque no tenga mucho sentido) como se detalla en la sección \ref{disenoGeneMut}.

4) Mutación: de forma similar al paso anterior, sobre cada uno de los individuos de la descendencia se le aplica el operador de mutación determinístico o probabilístico (o los dos, aunque de nuevo no tenga mucho sentido) como se detalla en la sección \ref{disenoGeneMut}.

5) Preservación de individuos antiguos: como se ha comentado antes, se puede definir un número de individuos antiguos a conservar en cada generación. Se mirará la variable "individuos a preservar" para conservar a los mejores y se eliminarán los que sean peores. Si la variable contiene un cero, se estará aplicando la política de reemplazo generacional, pues en tal caso se eliminarían en este paso todos los individuos antiguos.

6) Se probarán e insertarán ordenadamente en la población los individuos de la descendencia. Puede que alguno no llegue a estar en la población como tal si no hay hueco para él. Nótese que se han podido generar más descendientes en el paso 2 de lo que se haya definido como el tamaño máximo de la población. Y, además, puede que estos individuos tengan que competir no sólo con los individuos de su generación, sino con los conservados en el paso 5.

Para generar la popblación inicial, se tomará un individuo de ejemplo del que se copiará la estructura de la red neuronal para generar individuos aleatorios (con pesos y umbrales aleatorios) que se irán insertando ordenadamente en la población (lo que implica evaluarlos) hasta completar el tamaño máximo de la población. El criterio que se ha elegido es el de maximizar el fitness. La tarea debe ser diseñada de tal forma que un individuo con un fitness mayor sea mejor que uno con fitness menor.

** Operadores de selección
#+LaTeX: \label{disenoGeneSel}
Los operadores de selección que se han implementado son los siguientes: ruleta, ranking, torneo y truncado.

*** Ruleta
#+LaTeX: \label{disenoGeneSelRule}

Este tipo de selección sólo admite individuos con fitness mayor que cero, si el peor individuo no cumple esta condición se lanczará un error.
Para la selección por ruleta lo primero que hay que hacer es sumar el fitness de todos los individuos (S).
Luego, por cada individuo a seleccionar por este método:

1) Se elige un número aleatorio del intevalo (0, S), que llamaremos E (de elegido).

2) Se recorre la población desde el mejor individuo. Si el fitness del individuo (más el fitness de los individuos anteriores) es mayor que E, se selecciona ese individuo. Si no, se pasa al siguiente, acumulando el fitness de este individuo para la siguiente comparación.

*** Ranking
#+LaTeX: \label{disenoGeneSelRank}

Para la selección por ranking se puntuan los individuos dependiendo de su posición en la población.
Tradicionalmente se asigna N (el máximo de la población) al mejor, N-1 al segundo mejor, y así sucesivamente hasta llegar al peor individuo al que se asigna un fitness de 1. En nuestro caso hemos querido que sea más configurable y hemos añadido dos variables configurables: el "salto para el ranking" y la "base para el ranking". El salto para el ranking es la diferencia de fitness entre un individuo y el siguiente, en el ejemplo anterior era 1, pero podemos aumentar la presión selectiva incrementando este número. La "base para el ranking" se suma al fitness de toda la población. Por ello, para utilizar el ranking tradicional, los valores por defecto son "salto para el ranking" = 1 y "base para el ranking" = 0.

Una vez tenemos estos fitness auxiliares, se realiza la selección siguiendo un método similar al de la ruleta, pero con estas puntuaciones en lugar de los fitness originales.

*** Por torneo
#+LaTeX: \label{disenoGeneSelTorn}

Para la selección por torneo se cuenta con una variable configurable "tamaño del torneo" que no puede ser menor que el tamaño máximo de la población. En caso contrario se generará un error. Para cada individuo a seleccionar por este método:

1) Se preseleccionan "tamaño del torneo" individuos de la población de forma totalmente aleatoria pero evitando que se repitan.

2) Se selecciona el individuo más apto de todos los que están en el torneo.

El tamaño típico y, por ello, el valor por defecto que hemos seleccionado para el tamaño del torneo es 2.

*** Elitísta o por truncado
#+LaTeX: \label{disenoGeneSelTrunc}

La selección elitista es la más sencilla de todas. Simplemente se cogen los N (donde N es el número de individuos a seleccionar por este método) más aptos desde el principio de la lista ordenada de la población.

** Operadores de cruza
#+LaTeX: \label{disenoGeneCruz}

Aunque aceptamos varias definiciones de gen, como se explica en la sección \ref{disenoGeneNiv}, en esta sección trataremos las formas en que se pueden cruzar dos individuos, produciendo dos descencientes con los genes de los progenitores combinados de forma complementaria (todos los genes de los progenitores irán a un descendiente o a otro, aunque puede que uno de los descendientes se deseche si sobra). 

Todos los esquemas de cruce se aplican primero sobre un vector de bits (cada bit representa un gen) y luego se aplica el crossover usando ese vector. Esto permite compartir una sóla interfaz para la cruza a bajo nivel. Dada la diversidad de implementaciones de las redes neuronales, la cantidad de código se multiplicaría con los distintos esquemas de cruza de forma que el código sería mucho más complicado de desarrollar y mantener. Esto permite extender nuestro algoritmo genético con nuevos esquemas de cruza sin necesidad de modificar las distintas implementaciones (C, SEE2, CUDA). 

También es posible crear una nueva implementación (por ejemplo, usando openCL) sin necesidad de implementar por separado cada uno de los esquemas de cruza. De otra manera, la complejidad del código crecería NxM con respecto al número de esquemas de cruza y de implementaciones paralelas. De esta manera, sólo hay que implementar N + M.

Además, los pesos pueden estar dispuestos de forma diferente en memoria dependiendo de la implementación, como sucede en el caso descrito en las secciones \ref{disenoParalCUDAinv} y \ref{disenoParalCUDAcruza}, en el que la matriz de pesos se almacena invertida en memoria. En ese caso, basta con invertir la matriz de bits interfaz, en lugar de reimplementar el algoritmo de cruza que comparte con otros algoritmos CUDA.

*** Uniforme
#+LaTeX: \label{disenoGeneCruzUni}

Para la cruza uniforme, se debe indicar un parámetro "probabilidad", que puede ser configurado independientemente para cada nivel de cruza.
Para generar el hijo A, por cada gen de los progenitores, se elige un número aleatorio en el intervalo (0, 1). Si el número es menor que la probabilidad, se cogerá el gen del progenitor B, en caso contrario, el del progenitor A. Para generar el hijo B, se utilizan los genes que no se hayan utilizado para el descendiente A.

La probabilidad por defecto para todos los niveles es 0.7.

*** Proporcional
#+LaTeX: \label{disenoGeneCruzProp}

Este modo de cruza funciona de forma similar al anterior, con la diferencia de que la probabilidad no es especificada por el usuario, sino que se calcula a partir de los fitness de los progenitores. Tradicionalmente, se usa la siguiente fórmula:

\begin{equation}\label{eqCruzProp}
  probabilidad = finessA / (fitnessA + fitnessB)
\end{equation}

Esta fórmula sólo admite finess positivos, pero en nuestro caso hemos admitido más casos.

1) Si ambos son positivos, se aplica la fórmula \ref{eqCruzProp}.

2) Si ambos fitness son iguales a cero, la probabilidad es 0.5.

3) Si fitnessA es positivo y fitnessB es menor o igual que cero, la probabilidad es 1.

4) Si fitnessA es menor o igual que cero y fitnessB es positivo, la probabilidad es 0.

5) Por último, si ambos son negativos, se aplica otra fórmula parecida a la primera (pero en este caso, cuanto menos negativo mejor):

\begin{equation}\label{eqCruzPropNeg}
  probabilidad = -finessB / -(fitnessA + fitnessB)
\end{equation}

Aunque contemplar estos casos especiales puede parecer una complicación innecesaria, nos permite que este tipo de cruza sea compatible con tareas que admiten fitness negativos en lugar de tener que lanzar un error.

*** Multi-punto
#+LaTeX: \label{disenoGeneCruzMulti}

En la literatura convencional, frecuentemente se mencionan la "cruza de un punto" o la "cruza de dos puntos", pero en realidad son casos concretos de la más general "cruza multipunto". Por ello, se ha decido implementar sólo esta última, creando un parámetro "número de puntos" que puede ser configurado independientemente para cada nivel de cruza. El número de puntos por defecto para todos los niveles es 1.

El funcionamiento general es el siguiente:

1) Se marcan aleatoriamente "número de puntos" genes, que serán como puntos de corte.

2) Desde el inicio, hasta el primer punto, se cogen los genes del progenitor A. A partir desde este punto de corte hasta el siguiente, se gogen los genes del progenitor B, luego de nuevo los del A y así sucesivamente hasta el final.

De esta manera, se va alternando el progenitor en cada punto. Como siempre, el decendiente B usará los genes que no haya usado el descendiente A.

** Niveles de cruza
#+LaTeX: \label{disenoGeneNiv}

*** Pesos y umbrales
#+LaTeX: \label{disenoGeneNivPes}

Este es el nivel de cruza más pesado y sensible de todos. Todas las capas se colocan una detrás de otra con sus pesos seguidos de sus umbrales. Cada peso o umbral es un gen.

*** Neurona
#+LaTeX: \label{disenoGeneNivNeu}

En este caso cada gen es una neurona, con todos sus pesos y con su umbral. Los pesos son los que se multiplican por las entradas a esta neurona.
Se colocan en orden todas las capas y todas las neuronas de cada capa.

*** Neurona invertida
#+LaTeX: \label{disenoGeneNivNeuInv}

Este caso es muy similar al anterior, pero se cambia la definición de lo que se considera una neurona. En este caso, junto con el umbral, forman parte del mismo gen los pesos que se multiplican por la salida de esta neurona, en lugar de los que utiliza esta neurona para calcular su estado. Esta representación ha sido también denominada "neurona en fregona" [biblio].

*** Capa
#+LaTeX: \label{disenoGeneNivCap}

Para el nivel de capa, cada capa, valga la redundancia, es considerada un gen. Una capa incluye todas sus neuronas con sus pesos y umbrales, entendiendo una neurona como se hace en el apartado \ref{disenoGeneNivNeu} y no como la neurona invertida.

Aunque intuitivamente se puede pensar que este tipo de cruza no será muy útil si las capas son muy pocas o muy grandes, se ha decido implementar también este nivel de cruza para comparar el aprendizaje.

** Mutación y olvido
#+LaTeX: \label{disenoGeneMut}

La forma en que se implementan el operador de mutación y el de olvido son muy similares. La principal diferencia es que mientras el operador de olvido o reset simplemente pone a cero el peso o umbral que toque, el de mutación le suma un número aleatorio del intervalo (-X, X), donde X es un parámetro configurable que llamaremos "rango de mutación", que por defecto toma el valor 1. En cierto sentido, se podría considerar al operador de olvido como un tipo especial demutación.

Por lo demás, los dos operadores tienen dos formas de ser empleados: probabilística y determinista.

*** Probabilística

Esta forma de mutación es la más habitual en los algoritmos genéticos. Se usa una probabilidad parámetro ("probabilidad de mutación" o "probabilidad de olvido", ambas 0 por defecto) para calcular con cada peso y umbral si será mutado o no. Se elige un número aleatorio entre 0 y 1 y si el número es menor que la probabilidad, se realiza la acción correspondiente. Si es mutación sumar al peso la mutación que se obtiene a partir del rango como se ha comentado anteriormente y si es olvido el peso se iguala directamente a cero.

*** Determinista

Para evitar repetir el calculo de la probabilidad tantas veces y mejorar el rendimiento, se ofrece esta otra modalidad de mutación, con la esperanza de que el aprendizaje no se vea afectado negativamente.

En este caso en lugar de determinar probabilisticamente y peso por peso si un peso debe mutar o no, se configura un número determinado de mutaciones (u olvidos) que se aplicarán a cada individuo. Las variables "número de mutaciones" y "número de olvidos" tienen ambas por defecto el valor 0. Sabiendo el número de mutaciones que se van a realizar, sólo queda determinar aleatoriamente qué pesos y/o umbrales concretos serán mutados (u olvidados).

Para activar cualquiera de las dos modalidades en cualquiera de los dos operadores, basta con dar un valor positivo a las variables "probabilidad de mutación", "probabilidad de olvido", "número de mutaciones" y "número de olvidos". Como es habitual, se pueden emplear simultaneamente las varias opciones. En este caso también puede no activarse ningún tipo de mutación ni de olvido.
* REVISAR Optimizaciones mediante paralelización
#+LaTeX: \label{disenoParal}
** Introducción

Tanto los algoritmos genéticos como las redes neuronales requieren cálculos que presentan paralelismos inherentes. Para este proyecto se ha escogido explotar exclusivamente los de las redes neuronales (aunque también se paraleliza el operador genético de cruza para GPGPU, como se describe en la sección \ref{disenoParalCUDAcruza}). Pero la implementación se podría extender para aprovechar también los de los algoritmos genéticos, por ejemplo, utilizando múltiples CPUs y GPUs, usando una CPU para cada individuo y administrando las GPUs según su disponibilidad. Esto requeriría cambios no triviales en el modo en que las poblaciones son procesadas cada generación si se quiere extender la librería en ese sentido. Nuestras paralelizaciones solamente usan una CPU. Se han optado por dos alternativas que se comparan.

Gracias al diseño modular por el que se ha optado, es posible añadir otras implementaciones paralelas de las redes neuronales (por ejemplo, usando el lenguaje OpenCL) tan sólo implementando unos pocos métodos en un par de clases que extiendan las clases Fachada (TODO nota al pie sobre el patrón de diseño) que contienen toda la parte susceptible de ser cambiada para obtener mejor rendimiento.

La primera alternativa implementada es la utilización del conjunto ampliado de instrucciones SSE2 para acceder al co-procesador XMM. Este co-procesador está presente en todos los computadores recientes de la familia x86 liderada por Intel, que es probablemente la arquitectura más extendida en el mundo. La arquitectura vectorial del co-procesador multimedia permite operar sobre varios datos similares al mismo tiempo. En la sección \ref{disenoParalXMM} se explica con más detalle la arquitectura del mismo y como se ha utilizado para paralelizar nuestro algoritmo.

La segunda paralelización obedece a una tendencia bastante más reciente y en alza conocida como GPGPU (General Purpose Graphic Processor Units), que consiste en utilizar las terjetas especializadas en procesar gráficos para procesar otros cálculos que posiblemente nada tengan que ver con los gráficos. Debido a la gran demanda proveniente de diseñadores gráficos y, sobre todo, aficionados a los videojuegos, estos dispositivos comenzaron a tener unas especificaciones que resultaban muy atractivas a gran variedad de investigadores como físicos o bioquímicos. Al principio los investigadores dependian de su ingenio para mapear sus problemas específicos a un algoritmo que usase primitivas gráficas, pero con el creciente interés de esta técnica, los fabricantes decidieron ampliar su mercado de consumidores creando lenguajes específicos para este fin mucho más amigables y con facilidades para la optimización. El lenguaje C CUDA de NVIDIA, con el que desarrollamos la paralelización descrita en la sección \ref{disenoParalCUDA} es un ejemplo de estos lenguajes. Más tarde las compañías decidieron crear un lenguaje común que sirviese para todas las GPUs sin importar la marca llamado OpenCL. Hoy en día muchas de los supercomputadores más potentes del mundo utilizan múltiples GPUs para obtener los altos rendimientos que requieren[fn:cudaSuperComp].

** Ensamblador con SSE2
#+LaTeX: \label{disenoParalXMM}
*** Introducción al coprocesador XMM
#+LaTeX: \label{disenoParalXMMintro}

Como ya se ha mencionado, el coprocesador XMM utiliza una arquitectura vectorial (SIMD, Single Instruction Multiple Data, figura \ref{SIMDexecutionModel}). Esto significa que tiene varias ALUs que pueden realizar la misma operación sobre múltiples datos en paralelo. Como veremos, la tecnología XMM parmite algunas cosas más como operaciones de reducción sobre el vector de datos. XMM es una extensión de MMX (que introducía el célebre procesador Pentium XMM) en la que se dobla el tamaño máximo de los vectores (de 64 a 128 bits) y se añaden algunas instrucciones. Este coprocesador es utilizado también para las operaciones habituales con números de doble precisión, por lo que alternar frecuentemente entre los dos usos puede resultar en serias penalizaciones al rendimiento.

#+CAPTION:    Modelo de ejecución SIMD. En nuestro caso el destino se almacena en el mismo registro de origen 1.
#+LABEL:      SIMDexecutionModel
#+ATTR_LaTeX: scale=0.4
[[./img/SIMD_Execution_Model.jpg]]

El tamaño de los registros-vectores depende del tipo de datos a procesar: se pueden tener 2 números en doble precisión, 4 números en coma flotante, 4 enteros (con o sin signo), 8 enteros cortos (short), 16 bytes, 128 bits para operaciones lógicas, etc. La figura \ref{XMMregister} lo ilustra con más detalle. 

#+CAPTION:    Posibles usos vectoriales de los 128 bits de un registro XMM.
#+LABEL:      XMMregister
#+ATTR_LaTeX: width=\textwidth
[[./img/XMMregisters.jpg]]

No es preciso indicar qué tipo de datos contiene cada registro vector, los datos de cada registro XMM serán interpretados de una manera u otra dependiendo de la operación que se aplique sobre ellos. El compilador o en este caso el programador es responsable de mantener la integridad de los mismos. Por ejemplo, la instrucción PADDB, sumará dos registros interpretándolos como Bytes idependientes, PADDW sumará palabras (2 Bytes) y PADDD los tomará como palabras dobles (4 Bytes, el tamaño del típico int de C). Si queremos saturación con o sin signo debemos utilizar instrucciones que lo indiquen como PADDSB (saturación con signo) o PADDUSB
 (saturación sin signo). ADDPS para números en coma flotante con precisión simple (4 bytes), etc. Las instrucciones para usar registros MMX pertenecen al conjunto extendido SSE y las que operan sobre registros XMM pertenecen a SSE2.

*** Operaciones vectoriales con números en coma flotante
#+LaTeX: \label{disenoParalXMMfloat}

La función desarrollada para XMM para optimizar los cálculos de una red neuronal o capa de tipo float (sin optimizar la activación) puede ser llamado desde C/C++ usando el siguiente prototipo:

#+begin_src c
    void XMMreal(float* bufferEntrada, unsigned numeroBloques,
                 float* pesos, float &resultado);
#+end_src

Para calcular el estado de una neurona de tipo float se escribirá en la variable de salida resultado (sobre la que se tendrá que aplicar posteriormente la activación), tomamos como entrada dos vectores y un entero. Los arrays son el buffer de entrada (la salida de una capa de tipo float) y otro con los pesos asociados a esa entrada para esta neurona de salida concreta. El entero nos indica el número de bloques de entrada que han de ser procesados. Como se trada de números flotantes en precisión simple, podemos operar con cuatro de ellos simultáneamente en el coprocesador XMM. Por tanto los bloques son de tamaño 4 y los ambos arrays deben reservar un tamaño en memoria que sea múltiplo de cuatro floats. Los números sobrantes también serán procesados, por lo que es preciso anular las entradas y/o los pesos para evitar que estos valores sobrantes no afecten al resultado final.

Internamente, se van recorriendo ambos vectores, multiplicándo los elementos y acumulando los resultados. El núcleo del bucle contiene estas dos instrucciones:

#+begin_src asm
 	MULPS XMM0, XMM1
	ADDPS XMM3, XMM0
#+end_src

La primera multiplica 4 entradas contenidas en XMM0 por sus pesos correspondientes contenidos en XMM1. La segunda instrucción va acumulando los resultados en XXM3. Al final sólo hay que sumar los 4 subtotales que hay en cada uno de los elementos de XMM3 y devolver el resultado en la variable resultado.

*** Operaciones vectoriales con Bytes
#+LaTeX: \label{disenoParalXMMbyte}

Para poder aprovechar al máximo las capacidades del coprocesador XMM, se decide implementar un tipo de capa con unas características concretas.
La primera es que el estado de las neuronas será almacenado en bits, ya se trate de neuronas binarias cuyos estados pertenecen al conjunto {0, 1} o de neuronas de tipo bipolar cuyos estados pueden ser {-1, 1}. Esto nos ahorrará mucho espacio en memoria y, sobre todo, muchas lecturas de memoria para procesar el mismo número de neuronas de entrada.

La segunda característica es que los pesos tendrán valores pertenecientes al conjunto de enteros [-128, 127] y, por tanto, cada peso ocupará un byte en memoria. Esto significa que, además de leer menos datos de memoria como ocurre con las entradas, podremos procesar los pesos de 16 en 16 (los bytes que caben en un registro XMM de 128 bits) en lugar de hacerlo de 4 en 4 como en la función anterior que operaba con números en copa flotante con precisión simple. El hecho de que los pesos puedan tomar menos valores nos permitirá además reducir el espacio de búsqueda en el algoritmo genético, pero a la vez impone mutaciones enteras y, por tanto, cambios más bruscos. Los resultados en términos de aprendizaje al comparar los dos tipos de pesos se encuentran en el apartado \ref{aprendDiscretLineales}.

Las funciones para las capas de tipo binario y las de tipo bipolar son muy similares, sus prototipos son:

#+begin_src c
    int XMMbinario(void* bufferEntrada, unsigned numeroBloques, unsigned char* pesos);
    int XMMbipolar(void* bufferEntrada, unsigned numeroBloques, unsigned char* pesos);
#+end_src

Se ha escogido en este caso devolver el resultado directamente en lugar de usar un parámetro de salida, pero la decisión no tiene consecuencias trascendentes. Se explicará primero como funciona internamente la primera de las funciones y luego, para la segunda, sólo se explicarán las partes que la hacen diferente. Para una mayor claridad a la hora de presentar porciones de código, usaremos nombres descriptivos (similares a nombres de variables en lenguajes de más alto nivel) en lugar de los nombres de los registros XMM que se han utilizado en el código real: XMM0, XMM1...XM7.

Como en el caso en coma flotante, las entradas y los pesos se procesarán por bloques y se deberán rellenar adecuadamente los pesos y entradas para evitar que se sumen cálculos no desados. Para el caso bipolar es imprescindible anular los pesos, no basta con anular las entradas pues los bits nulos serán interpretados por el algortimo como -1 en vez de como 0. Para las entradas, los bloques contendran 128 bits, cada uno representando a una neurona de entrada. 

Para los pesos, los bloques serán de 16 bytes, uno para cada peso. De este modo, por cada bloque de entrada completo se requerirán 8 bloques de pesos (8 * 16 = 128). El número de bloques que se recibe por parámetro se refiere al número de bloques de pesos. Así, si no se van a usar todas las neuronas de entrada en el último bloque, no hay que seguir leyendo pesos que se sabe que deben ser nulos para el funcionamiento correcto. Esos bloques sobrantes no han de procesarse, ni siquiera almacenarse en memoria. Lo importante es que dentro del bucle principal que recorre las entradas (que se irán almacenando en el registro XMMentrada), hay un sub-bucle que se ejecuta hasta ocho veces, una vez por cada 16 pesos que se requieran, que se irán almacenando en el registro XMMpesos.

Para acceder a los bits de un bloque de entrada de 16 en 16 (el número de pesos que se van a procesar en cada vuelta del bucle de pesos), usaremos el registro XMMmascara que tendrá un bit activo por cada uno de los 16 bytes del bloque. La máscará se inicializará por cada bloque de entrada con 16 bytes iguales a 128 (el primer bit activo y todos los demás nulos en binario) y luego se irá deplazando todo el registro una posición a la derecha por cada nueva lectura hacia XMMpesos que no suponga también una lectura en XMMentradas y, por tanto la inicialización de la máscara. Los 16 byes con un 128 vienen de una constante en memoria. Para evitar la penalización que supondría leer esta constante por cada 8 bloques de pesos leídos, se reservará el registro XMM128 de los 8 disponibles (con arquitecturas de 64 bits, el coprocesador XMM dispone de 16 registros en vez de 8) y que en todo momento contendrá dicha constante leída de memoria una sola vez al principio de la función. Para ello se usará la siguiente instrucción (la misma que se usa para leer entradas y pesos):

#+begin_src asm
	MOVDQU XMM128, [cte_mascara_en_mem];
#+end_src

Cuando se quiera inicializar la máscara simplemente se utilizará la siguiente instrucción, que copia el contenido de un registro a otro y es mucho menos costosa que la anterior:

#+begin_src asm
	MOVDQA XMMmascara, XMM128
#+end_src

Para no estropear la mascará, previamente se ha copiado su contenido a XMMaux, sobre el que se harán varias operaciones. Ahora para acceder a cada uno de los bits en la posición que toque de las ocho, bastará con hacer un AND lógico con el registro de entradas. 

#+begin_src asm
	PAND XMMaux, XMMentradas
#+end_src

Ahora dependiendo de si el byte tiene algún bit activo o no, se sumará o no el peso correspondiente. Esta colocación de los bits con respecto al orden en que se cogen los pesos no es igual a la del algoritmo equivalente implementado en C, por tanto la función de activación de los tipos binario y bipolar para la implementación SSE2 (aunque esté escrita en C), debe tener en cuenta la disposición especial de los bits de entrada que espera esta función. Lo mismo sucede para los métodos que copian vectores de bits desde los Buffer dependientes de la implementación a los vectores más generales de la clase Interface que usamos para acceder a las entradas y salidas de la red neuronal desde el exterior, independizando así el manejo de estos datos de la representación interna que pueda tener cada implementación, como ya se ha descrito en la sección \ref{diseno}.

¿Cómo llegamos a partir de lo que tenemos en XMMaux (cada byte tiene en bit activo o no, dependiendo del estado de la neurona de entrada procesada) y en XMMpesos a un registro en el que sólo se tengan los pesos que correspondan a neuronas activas y que tenga anulados los pesos que corresponden a neuronas inactivas? Son necesarios algunos trucos de bastante bajo nivel que son realmente la parte más interesante de las funciones. Primero ejecutaremos la siguiente instrucción:

#+begin_src asm
    PCMPEQB XMMaux, XMMnulo
#+end_src

PCMPEQB compara cada byte de ambos registros y, si son iguales, pone a 255 (todos los bits activos) del byte en el primer registro (XMMaux). Si son distintos, pone cero (todos los bits inactivos) en ese mismo byte. En nuestro caso lo estamos comparando con un registro en el que todos los bits son nulos. Por ello, los bytes de XMMaux que tuviesen un bit activo se anularán enteros (por ser distintos a cero) y los que no tuviesen ninguno activo tomarán el valor 255 (por haber sido iguales a cero). Pero nosotros queríamos justamente lo contrario, por lo que invertimos completamente XMMaux para obtener el resultado deseado.Para invertir un registro, ejecutamos XOR contra un registro que tenga todos los bits activos (XMM255):

#+begin_src asm
    PXOR XMMaux, XMM255
#+end_src

Para iniciar los registros XMMnulo y XMM255 no se requieren constantes en memoria. Basta con usar de nuevo instrucciones lógicas:

#+begin_src asm
    PXOR XMM255, XMM255
#+end_src

Como cualquier registro independientemente de su contenido inicial es "igual a sí mismo", la comparación activará el registro por completo.

#+begin_src asm
    PXOR XMMnulo, XMMnulo
#+end_src

Como XOR requiere uno y sólo uno de los bits de entrada activos para activar la salida y como de nuevo el registro es "igual a sí mismo", el registro se anulará todos sus bits.

Una vez que tenemos en XMMaux cada byte a 255 ó 0 dependiendo del estado del bit correspondiente a cada una de las 16 neuronas de entrada procesadas, podemos desechar los pesos que no deban sumarse con un simple AND:

#+begin_src asm
    PAND XMMaux, XMMpesos
#+end_src

En la figura \ref{mascaraBinariaXMM} se trata de ilustrar la forma de acceso a los bits individuales. 

#+CAPTION:    Ejemplo ilustrativo del acceso paralelo a los bits individuales.
#+LABEL:      mascaraBinariaXMM
#+ATTR_LaTeX: width=\textwidth
[[./img/ejemploXMM.jpg]]

Todavía tenemos que sumar los pesos entre sí y acumularlos. Este es el paso que consigue una mayor mejora en las optimizaciones binaria y bipolar con respecto a la flotante. Aunque no hay ninguna instrucción que nos permita sumar todos los bytes de un registro XMM directamente, existe otra que nos es muy útil porque hace una reducción similar. Se trata de PSADBW. Con registros MMX (de 64 bits en vez de 128), calcula la diferencia absoluta entre los bytes de cada registro operando y suma todas esas diferencias, dejando el resultado en los 4 bytes bajos del registro MMX. Con registros XMM, opera de forma similar pero dejando dos resultados: uno en la los 4 bytes bajos de los 8 bajos y otro en los 4 bajos  de los  8 altos. Es decir, duplica la operación. 

#+begin_src asm
    PSADBW XMMaux, XMMnulo
#+end_src

Si uno de los operandos es un registro nulo, la diferencia absoluta entre 0 y un número siempre es ese mismo número, por lo que simplemente sumara los bytes. Surge aquí un pequeño problema dado que suma los bytes sin tener en cuenta su signo, como si todos fueran positivos. Cómo queriamos los pesos pertenecientes a [­128, 127], debemos hacer algo al respecto.

Antes de ejecutar la instrucción anterior, ejecutaremos: 

#+begin_src asm
    PAND XMMaux128, XMMaux
#+end_src

Y así tendremos 128 en los bytes cuyos bits estaban activos. Después, en lugar de sólo una instrucción de reducción, ejecutamos:

#+begin_src asm
    PSADBW XMMaux, XMMnulo
    PSADBW XMMaux128, XMMnulo
    PSUBD XMMaux, XMM128
#+end_src

Esto equivale a restarle 128 a cada byte que fuesemos a sumar, porque se suman con PSADBW tantos 128 como pesos haya. Hay que tener en cuenta que los pesos pueden ser [­128, 127] pero no equivalen, por ejemplo, a los char de C++. En C++, los números se representan en complemento a dos mientras que en nuestra representación alternativa el 0 es el ­128, el 128 es el 0, el 129 el 1, etc. Realmente no es importante, siempre y cuando lo tengamos presente. Ya sólo queda sumar las dos partes. Después, se repite el proceso hasta completar los 8 bits por bytes, cargando cada vez 16 pesos nuevos. Luego se reinicia la máscara, se lee el siguiente bloquede entrada y se repite todo hasta que hayamos completado en número de bloques.

Al final, hay que sumar las dos partes (alta y baja) que se están acumulando en un registro XMM. Esto se omite, como la gestión del bucle, porque no tiene demasiado interés en lo que a nuestros esfuerzos de optimización se refiere.

La explicación que se ha dado se refería al algoritmo para neuronas binarias, que pueden tomar los valores {0, 1}. Para las neuronas bipolares que pueden tomar los valores {-1, 1}, el código es bastante similar, aunque ligeramnte más complicado. En este caso, todos los pesos se utilizan, simplemente unos cambian su signo y otros no. Ahora, cuando invertimos XMMaux, también conservamos el original y también lo operamos con AND con los pesos. El resultado son los pesos que tendrán que ser restados en vez de sumados. También lo operamos el registro auxiliar invertido con el XMM128, pues por cada peso restado se tendrá que sumar 128 (en vez de restarlo). Por ejemplo, 129 es sólo 1 en nuestra representación, por tanto, para restar 1 (restar un peso igual a uno), restamos 129 y sumamos 128.

Recordamos resumadiamente lo que hacíamos en el núecleo de la versión binaria para luego señalar las diferencias.

#+begin_src asm
	MOVDQA XMMaux, XMMmascara      ;copiamos la máscara en una mascara auxiliar

	PAND XMMaux, XMMentradas       ;obtenemos el valor del bit a procesar en cada byte
    PCMPEQB XMMaux, XMMnulo        ;si el bit estaba activo->se pone a 0 todo el byte, 
                                   ;si no-> se pone a 1 todo el byte (255)
	PCMPEQB XMM255, XMM255         ;ponemos 255 en todos los byes del registro XMM255
    PXOR XMMaux, XMM255            ;invertimos XMMaux 
                                   ;(ahora hay 255 en los bytes que tenian el bit que tocaba activo)

	MOVDQU XMMaux128, XMM128       ;128 en todos los bytes de XMMaux128
	PAND XMMaux128, XMMaux         ;128 sólo en los bytes que estaban activos

	MOVDQU XMMpesos, [ptrPesos]    ;leemos el bloque actual de pesos
	PAND XMMaux, XMMpesos          ;asi tenemos el peso de cada conexión 
                                   ;solamente en los bytes con el bit activo

	PSADBW XMMaux, XMMnulo         ;sumamos todos los bytes (los que estaban activos)
	PSADBW XMMaux128, XMMnulo      ;sumamos 128 por cada byte que estaba activo

	PADDD XMMacumulador, XMMaux    ;sumamos estos pesos a los ya sumados previamente
	PSUBD XMMacumulador, XMMaux128 ;sustraemos 128 por cada bit que estaba activo
#+end_src

En el caso bipolar se hacen más cálculos. Además de los dos primeros, como en el caso binario, para el caso bipolar se hacen los dos últimos cálculos descritos en esta lista:

1) Se suman todos los pesos de las neuronas activas
2) Se resta 128 por cada neurona activa
3) Se restan todos los pesos de las neuronas inactivas
4) Se suma 128 por cada neurona inactiva

Como ahora no tenemos que desechar ningún peso, sino sumar unos y restar otros, el código quedaría así:

#+begin_src asm
	MOVDQA XMMaux, XMMmascara      ;copiamos la máscara en una mascara auxiliar

	PAND XMMaux, XMMentradas       ;obtenemos el valor del bit a procesar en cada byte
    PCMPEQB XMMaux, XMMnulo        ;si el bit estaba activo->se pone a 0 todo el byte,
                                   ;si no-> se pone a 1 todo el byte (255)
	PCMPEQB XMMauxInv, XMMauxInv   ;ponemos 255 en todos los byes del registro XMMauxInv
    PXOR XMMauxInv, XMMaux         ;ponemos el inverso de XMMaux en XMMauxInv

	MOVDQU XMMaux128, XMM128       ;128 en todos los bytes de XMMaux128
	PAND XMMaux128, XMMauxInv      ;128 sólo en los bytes que estaban activos
	PSADBW XMMaux128, XMMnulo      ;sumamos 128 por cada byte que estaba activo
	PSUBD XMMacumulador, XMMaux128 ;sustraemos 128 por cada bit que estaba activo

	MOVDQU XMMaux128, XMM128       ;128 en todos los bytes de XMMaux128
	PAND XMMaux128, XMMaux         ;128 sólo en los bytes que estaban inactivos
	PSADBW XMMaux128, XMMnulo      ;sumamos 128 por cada byte que estaba activo
	PADDD XMMacumulador, XMMaux128 ;sumamos 128 por cada bit que estaba inactivo

	MOVDQU XMMpesos, [ptrPesos]    ;leemos el bloque actual de pesos
	PAND XMMauxInv, XMMpesos       ;asi tenemos el peso de cada conexión 
                                   ;solamente en los bytes con el bit activo
	PAND XMMaux, XMMpesos          ;asi tenemos el peso de cada conexión 
                                   ;solamente en los bytes con el bit inactivo

	PSADBW XMMauxInv, XMMnulo      ;sumamos todos los bytes (los que estaban activos)
	PSADBW XMMaux, XMMnulo         ;sumamos todos los bytes (los que estaban inactivos)

	PADDD XMMacumulador, XMMauxInv ;sumamos los pesos "positivos" al acumulador
	PSUBD XMMacumulador, XMMaux    ;sustraemos los pesos "negativos" al acumulador
#+end_src

Todavía se podrían mejorar las soluciones si contásemos con la arquitectura de 64 bits. En tal caso tendríamos 16 registros XMM en lugar de sólo 8, no habría que reusar tanto los registros y algunos trucos (como los de poner a 255 ó a 0 todo un registro) podrían realizarse solamente una vez al principio en vez de cada vez que necesitamos alguno de estos valores en un registro que usamos para multiples cosas. Hemos optado por la compilación para 32 bits por su mayor portabilidad. En los sistemas operativos de 64 bits se puede simular la arquitectura de 32 bits y ejecutar nuestra optimización. No sucede lo mismo al contrario: si hubiesemos optado por la implementación de 64 bits no podríamos ejecutar la optimización sobre un sistema operativo de 32 bits.

Como hemos dicho, la colocación de los bits para la implementación XMM debe adaptarse para que se puedan obtener los mismos resultados que con el algoritmo implementado en C. Pero además en el algoritmo C no se puede usar el tipo char para los pesos (hay que usar unsigned char) y hay que restarles 128 antes de operar con ellos. Esto podría ralentizar "injustamente" al algoritmo C, por lo que también se hicieron pruebas de rendimiento sin restar 128 y usando el tipo char para comparar los tiempos. Logicamente, esa implementación C no obtiene resultados equivalentes a los de la SSE2, pero tan sólo se pretendía comparar el rendimiento. Sorprendentemente, con esta implementación C se obtenían resultados aún peores. También se probó usadno el tipo unsigned char pero sin restar 128 y el rendimiento era de nuevo ligeramente peor. Por alguna razón que no alcanzamos a explicar, el algoritmo en C funciona más rápido si ha de restar 128 a cada peso. Por ello, dejamos de lado nuestra preocupación sobre la posible penalización causada por nuestra representación de pesos en bytes.

** GPGPU con CUDA
#+LaTeX: \label{disenoParalCUDA}

Debido a la insaciable demanda de mercado de gráficos 3D de alta definición y en tiempo real, las unidades de procesamiento gráfico (Graphic Processor Unit, GPU) han evolucionado en procesadores altamente paralelos y multihilo, con muchos núcleos, tremenda capacidad de computación y con gran ancho de banda de memoria. La técnica consistente en utilizar el este poder computacional para realizar trabajos de proposito general, que pueden no tener nada que ver con los gráficos se denomina GPGPU (General Purpose Graphic Processor Unit). Los pioneros de la técnica buscaban homorfismos entre los algoritmos que pretendían ejecutar y cálculos que las librerías gráficas realizan internamente. 

Gracias a los lenguajes de alto nivel especializados para GPGPU como C CUDA u OpenCL, ya no es necesario modelar tu problema utilizando conceptos puramente gráficos como superficies y texturas. Sin embargo, para poder aprovechar las máximas posibilidades de rendimiento es necesario conocer la arquitectura de las unidades de procesamiento gráfico y así como los cuellos de botella que potencialmente puedan perjudicar a la optimización de nuestro algoritmo.

Aunque otras arquitecturas gráficas puedan ser similares en muchos aspectos, describiremos los conceptos básicos de la arquitectura CUDA, que es la que hemos utilizado para paralelizar los cálculos de estado de las redes neuronales y que fue diseñada explícitamente para soportar GPGPU incluso desde lenguajes de alto nivel. En principio C, pero luego también otros leguajes como FORTRAN, C++ y OpenCL. Desde la serie NVIDIA GeForce 8000 todas las gráficas que ha producido NVIDIA obedecen a la arquitectura básica CUDA (excepto las específicas para dispositivos móviles, que siguen la aruitectura Tegra). Aunque tarjetas posteriores ofrecen nuevas capacidades y posibilidades de ajuste de los algoritmos, son retrocompatibles con respecto al código implementado para versiones anteriores.

Por simplicidad, los trozos de código mostrados tratarán exclusivamente la versión real (float) de las neuronas, sin mostrar ni explicar las complejidades adicionales de las versiones binaria y bipolar.

*** Modelo de programación
#+LaTeX: \label{disenoParalCUDAprog}

C para CUDA es una extensión de C que permite al programador difinir funciones, llamadas núcleos (kernels) que cuando son llamadas se ejecutan N veces por N hilos CUDA diferentes, en vez de una sola vez como las funciones C habituales\cite{progGuide2009}. Para definir un kernel se usa el especificador de declaración =__global__= y el número de hilos CUDA para cada llamada se especifica con la nueva sintaxis ~<<<...>>>~ del siguiente ejemplo:

#+begin_src c
// Definicion del Nucleo
__global__ void MiKernel(float* A, float* B, float* C)
{
    ...
}
int main()
{
    ...
    // Invocacion del Nucleo
    MiKernel<<<1, N>>>(A, B, C);
}
#+end_src

A cada uno de los hilos que ejecuta el kernel se le da un identificador de hilo único que es accesible desde el kernel con la variable interna threadIdx. El siguiente código de ejemplo suma dos vectores A y B de tamaño N y guarda el resultado en el vector C:

#+begin_src c
// Definicion del Nucleo
__global__ void SumaVectores(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}
int main()
{
    ...
    // Invocacion del Nucleo
    SumaVectores<<<1, N>>>(A, B, C);
}
#+end_src

Cada uno de los hilos que ejecuta SumaVectores() realiza la suma de un par de elementos diferente.

Por conveniencia, threadIdx es un vector de tres componenetes para que los hilos puedan ser identificados usando un índice de una, dos o tres dimensiones, formando bloques de hilos unidimensionales, bidimensioneales o tridimensionales. Como ejemplo, el siguiente código suma los elementos de las matrices A y B de tamaño NxN y almacena el resultado en la matriz C:

#+begin_src c
// Definicion del Nucleo
__global__ void SumarMatriz(float A[N][N], float B[N][N], float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}
int main()
{
    ...
    // Invocacion del Nucleo
    dim3 dimBlock(N, N);
    SumarMatriz<<<1, dimBlock>>>(A, B, C);
}
#+end_src

El índice del hilo y su ID se relacionan de manera directa: para un bloque unidimensional, son iguales; para un bloque bidimensional de tamaño (Dx, Dy), el ID del hilo en con índice (x, y) es (x + y Dx); para uno tridimensional de tamaño (Dx, Dy, Dz), el ID del hilo con índice (x, y, z) es (x + y Dx + z Dx Dy).

Los hilos dentro de un mismo bloque pueden cooperar entre ellos compartiendo datos a través de la memoria compartida y sincronizando su ejecución para coordinar el acceso a memoria. Para ser más precisos, uno puede especificar puntos de sincronización en el núcleo llamando a la función interna =__syncthreads()= que actua como una barrera que hace esperar a todos los hilos del bloque antes de que ninguno pueda seguir. 

Para una cooperación eficiente, se espera que la memoria compartida sea de baja latencia y cercana al núcleo del procesador, como una caché de primer nivel, también que =__syncthreads()= sea ligera y todos los hilos de un bloque deben estar en el mismo núcleo de procesamiento. Por ello el número de hilos por bloque está restringido por los recursos limitados de memoria de un núcleo de procesamiento. En GPUs actuales un bloque de hilos puede contener hasta 512 hilos.

Sin embargo, un kernel puede ser ejecutado por múltiples bloques de hilos similares, de forma que el número total de hilos sea igual al número de hilos por bloque multiplicado por el número de bloques. Estos múltiples bloques se organizan en grids unidimensionales o bidimensionales de bloques de hilos como se muestra en la figura \ref{figGridThreadBlocks}. 

#+CAPTION:    Grid de bloques de hilos.
#+LABEL:      figGridThreadBlocks
#+ATTR_LaTeX: scale=0.6
[[./img/gridBlockThreads.jpg]]

La dimensiones del grid se especifica con el primer parámetro específico del kernel entre la sintaxis ~<<<...>>>~. Cada bloque dentro del grid se puede identicar con un índice unidimensional o bidimensional a través de la variable interna blockIdx. Las dimensiones de el bloque de hilos es accesible desde el kernel usando la variable interna blockDim. El código de ejemplo anterior quedaría así:

#+begin_src c
// Definicion del Nucleo
__global__ void SumarMatriz(float A[N][N], float B[N][N], float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}
int main()
{
    ...
    // Invocacion del Nucleo
    dim3 dimBlock(16, 16);
    dim3 dimGrid((N + dimBlock.x – 1) / dimBlock.x,
                 (N + dimBlock.y – 1) / dimBlock.y);
    SumarMatriz<<<dimGrid, dimBlock>>>(A, B, C);
}
#+end_src

El bloque de tamaño 16x16 = 256 se ha cogido algo arbitrariamente, y el grid se crea con suficientes bloques para tener un hilo por elemento de la matriz como antes. 

Los bloques de hilos deben poder ejecutarse independientemente, en cualquier orden, en paralelo o en serie. Este requerimiento de independencia hace que los bloques de hilos se puedan planificar en cualquier orden en cualquier número de núcleos, permitiendo a los programadores escribir código que escala para cualquier número de núcleos. El número de bloques en un grid es típicamente dictado por el tamaño de los datos a procesar en lugar del número de procesadores en el sistema, al que puede superar ampliamente.

Además de la jerarquía para la organización de los hilos descrita existe una jerarquía de memorias a cuyos diferentes espacios pueden acceder los hilos CUDA como muestra la figura \ref{cudaMemory}. Cada hilo tiene una memoria privada local. Cada bloque de hilos tiene uuna memoria compartida visible para todos los hilos del bloque y cuyos datos se mantienen por lo que dure el procesamiento del bloque. Finalmente, todos los hilos de todos los bloques tienen acceso a la misma memoria global. 

#+CAPTION:    Modelo de jerarquías de memoria CUDA.
#+LABEL:      cudaMemory
#+ATTR_LaTeX: scale=0.7
[[./img/memDrDobb.jpg]]

También hay dos espacios de memoria adicionales de sólo lectura que accesibles por todos los hilos: los espacios de memoria constantes y texturas. La memoria global, la de constantes y la de texturas pueden ser optimizadas para diferentes usos de memoria y son consistentes entre llamadas a kernels de la misma aplicación.

El modelo de programación CUDA asume que los hilos se ejecutarán en un dispositivo físicamente separado que opera como coprocesador de un programa C anfitrión. Los kernels se ejecutan en la GPU y el resto del programa C se ejecuta en una CPU anfitrión. También asume que tanto el anfitrión como el dispositivo mantienen su propia DRAM, llamadas memoria anfitrión y memoria de dispositivo, respectivamente.

Por tanto, un programa gestiona los espacios de memoria global, de constantes y de texturas visibles a los kernels a través de llamadas a la librería CUDA runtime. Esto incluye reserva, liberación de memoria y transferencia entre las memorias anfitrión y de dispositivo.

Ademas, se puede hacer un sistema anfitrión con varias GPUs y hacer llamadas de kernels con diferentes datos y configuraciones a cada uno de los dispoitivos. En el presente proyecto, sin embargo, se prescinde de esa posibilidad, así como de usar las memorias de constantes y de texturas.

*** Arquitectura CUDA
#+LaTeX: \label{disenoParalCUDAarq}

La arquitectura CUDA se construye alrededor de un conjunto escalable de Multiprocesadoes de flujo (Streaming Multiprocessors, SMs). Cuando un programa CUDA en la CPU anfitrión invoca a un grid de kernel, los bloques del grid son enumerados y distribuidos a los multiprocesadores con capacidad de ejecución como se muestra en la figura \ref{automaticScalability}. Los hilos de un bloque se ejecutan concurrentemente en un mismo multiprocesador. Cuando los bloques terminan, nuevos bloques son lanzados en los Multiprocesadores que queden libres.

#+CAPTION:    Escalabilidad automática: un dispositivo con más mutliprocesadores ejecutará un grid automáticamente más rápido que un dispositivo con menos multiprocesadores.
#+LABEL:      automaticScalability
#+ATTR_LaTeX: scale=0.6
[[./img/automaticScalability.jpg]]

Un multiprocesador consta de 8 núcleos que son procesadores escalares (Scalar Processors, SP), dos uunidades de función especial para trascendentales, una unidad de intrucciones multi-hilo, y una memoria compartida interna. El multiprocesador crea, gestiona y ejecuta los hilos concurrentes en un hardware con sobrecarga de planificación de ejecución nula. Implementa la barrera de sincronización intrínseca =__syncthreads()= con una sola instrucción. La rápida barrera de sincronización junto con la ligera creación de hilos y la sobrecarga nula de planificación soporta eficientemente un paralelismo muy granulado, permitiendo, por ejemplo, una descomposición con baja granulidad de problemas asignando un hilo a cada elemento de datos (como un pixel en una imagen, una celda en un calculo basado en una rejilla [grid] o una neurona de salida en una capa de una red neuronal).

Para gestionar cientos de hilos corriendo varios programas diferentes, el multiprocesador emplea una nueva arquitectura llamada SIMT (Single-instruction, multiple-trhead; una instrucción, múltiples hilos). El multiprocesador mapea cada hilo en un núcleo de procesamiento escalar, y cada hilo escalar se ejecuta independientemente con su propia dirección de instrucción y registro de estado. El multiprocesador SIMT crea, gestiona, planifica y ejecuta hilos en grupos de 32 hilos paralelos llamados warps (Termino originado en Weaving, la primera tecnología de hilos paralelos). Los hilos individuales que componen un warp SIMT empiezan juntos en la misma dirección de programa, pero son libres de divergir y seguir una ejecución independiente.

Cuando un multiprocesador recibe uno o más bloques de hilos para ejecutar, los divide en warps que son programados por la unidad SIMT. El modo en que un bloque es dividido en warps es siempre el mismo; cada warp contiene hilos con IDs consecutivos y crecientes con el primer warp conteniendo el hilo 0. 

Por cada tiempo de ejecución de intrucción, la unidad SIMT selecciona un warp que esté preparado para ser ejecutado y les da la siguiente instrucción a los hilos activos del warp. El warp ejecuta una instrucción común cada vez, por lo que la máxima eficiencia se alcanza cuando los 32 hilos del warp siguen el mismo flujo de ejecución. Si los hilos que divergen debido a una bifurcación condicional dependiente de los datos, el warp ejecuta de forma seliarizada cada camino de ejecución, desactivando los hilos que no están en ese camuno. Cuando todos los caminos se completan, los hilos convergen de nuevo en el mismo flujo de ejecución. Estas bifucaciones ocurren sólo dentro de un mismo warp; los diferentes warps se ejecutan independientemente sin importar si estos están ejecutando caminos de código comunes o disjuntos.

La arquitectura SIMT se parece a las organizaciones vectoriales SIMD (Single Instruction, Multiple Data) en que una sola instrucción controla multiples elementos de procesamiento. Una diferencia clave entre es que las arquitecturas vectoriales SIMD exponen el tamaño del vector SIMD al software, mientras que las instrucciones SIMT especifican la ejecución y el comportamiento de las bifurcaciones para un solo hilo. A diferencia de las maquinas vectoriales SIMD, SIMT permite a los programadores escribir código a nivel de hilo para hilos independientes y escalares, así como código con paralelismo de datos para hilos coordinados. En lo que se refiere a la corrección del código, el programador puede basicamente ignorar el comportamiento SIMT; sin embargo, se pueden conseguir mejoras de rendimiento sustanciales teniendo en cuenta que el código puede hacer que los hilos de un warp se bifurquen y evitándolo en la medida de lo posible. En la práctica, esto es similar al rol de las lineas de cache en el código tradicional: el tamaño de las lineas de cache se puede ignorar de forma segura cuando se diseña para que el código sea correcto pero debe ser considerado en la estructura del código cuando se diseña para un rendimiento máximo. Las arquitecturas vectoriales, sin embargo, requieren que el software haga las cargas coalescentes y gestione las divergencias manualmente.

Como se muestra en la figura \ref{arqCUDAdetalle}, cada mutliprocesador tiene memoria interna de 4 tipos:

- Un conjunto de registros locales de 32 bits por procesador.
- Una cache de datos paralelos o memoria compartida que es compartida por todos los procesadores escalares.
- Una cache de constantes de sólo lectura que es compartida por todos los procesadores escalares y acelera las lecturas desde el espacio de memoria de constantes, que es una región de sólo lectura de la memoria del dispositivo.
- Una cache de texturas de sólo lectura que es compartida por todos los procesadores escalares y acelera las lecturas desde el espacio de memoria de texturas, que es una región de sólo lectura de la memoria del dispositivo; cada multiprocesador accede a la cache de texturas a través de la unidedad de texturas que implementa los diferentes modos de direccionamiento y filtrado de datos especiales para texturas.

#+CAPTION:    Un conjunto de multiprocesadores SIMT con memoria compartida en el chip.
#+LABEL:      arqCUDAdetalle
#+ATTR_LaTeX: scale=0.6
[[./img/arqCUDAdetalle.jpg]]

Los espacios de memoria locales y globales son regiones de lectura/escritura de la momoria del dispositivo y no son cacheados. 
El número de bloques que un multiprocesador puede procesar a la vez - lo que se denomina como número de bloques activos por multiprocesador - depende de cuantos registros por hilo y cuánta memoria compartida por bloque son necesarios para un kernel dado, debido a que los registros de un multiprocesador y la momoria compartida son divididos entre todos los hilos de los bloques activos. Si no hay suficientes registros o memoria compartida disponibles por multiprocesador para procesar al menos un bloque, el kernel no podrá ser lanzado. El número máximo de bloques activos por multiprocesador, así como él número máximo de warps activos y el número de hilos activos dependen del dispositivo CUDA concreto.

Si una instrucción no atómica ejecutada en un warp escribe en la misma localización global de momoria compartida para más de un hilo dentro de warp, el número de escrituras serializadas que suceden en esa localización y el orden en que ocurren no está definido, pero se garantiza que al menos una de las escrituras se realizará. Si una instrucción atómica ejecutada por un warp lee, modifica o escribe sobre la misma localización en momodia global para más de uno de los hilos del warp, todas las lecturas, modificaciones y escrituras sobre esa localización se realizan y son todas serializadas, epro el orden en que ocurren tampoco está definido.
*** Reducción paralelizada
#+LaTeX: \label{disenoParalCUDAreduc}

La primera aproximación para la implementación de nuestro algoritmo genérico de cálculo de estado de una capa de red neuronal sobre la arquitectura CUDA estaba claramente influída por la optimización SSE2 anterior. Aquella función de ensamblador calculaba el resultado para una neurona de salida. En este primer kernel CUDA haremos lo mismo: cada llamada al kernel nos servirá para calcular la salida de un neurona (de nuevo, dejando la función de activación aparte). De este modo, cada hilo hará una multiplicación entre una entrada y un peso y luego se sumarán todos los resultados usando una reducción paralela.

En la sección \ref{disenoParalXMMbyte} utilizamos una instrucción especial para hacer la reducción y poder sumar los 16 elementos de un bloque, pero no existe una instrucción similar en CUDA. En \ref{disenoParalXMMfloat} se acumulaban los resultados en un registro con cuatro floats que se sumaban al final. El modo en que se sumaban no era lo que más aceleraba al algortimo, pero nos da una pista de lo que debemos hacer. Primero se sumaban dos y dos en una sóla instrucción (tras dduplicar el registro y desplazarlo) y después los otros dos resultantes. Generalizando a cualquier número de elementos iniciales y sumando los elementos de dos en dos de forma paralela, vemos que lo que tenemos que usar es una reducción en árbol\cite[Harris2007]{Harris2007}.

Pero queremos que el algortimo sirva para un tamaño de entrada cualquiera y como hemos visto, dividiremos los datos en un grid también de tamaño arbitrario compuesto de bloques de hilos cuyo tamaño sí depende de la máquina concreta. Este tamaño lo podremos ajustar a cada dispositivo con pruebas, pero surge un problema. No existe un método de sincronización global entre los hilos de los diferentes bloques en CUDA, sólo uno para sincronizar los hilos de un mismo bloque. Para superarlo analizaremos dos posibilidades. 

La primera es tener dos kernels y ejecutar el segundo cuantas veces sea necesario. El primer kernel multiplicará cada entrada por su peso correspondiente y hará la primera reducción. Cada bloque de hilos almacenará el resultado parcial de la reducción que ha realizado en la memoria global del dispositivo. El segundo kernel hará sólo reducción, sin multiplicar entradas ni pesos. Éste tomará como entrada las salidas de la ejecución anterior y se llamará tantas veces como sea necesario hasta que se llame al kernel con un grid de un sólo bloque de hilos que pueda sumar todo en una sola variable. Aunque la sobrecarga de tiempo que el hardware tarda en lanzar un kernel es despreciable y la sobrecarga que generaremos mediante software para configurar correctamente las sucesivas llamadas a los diferentes kernels parece en principio asumible, vemos que esta aproximación tiene otra gran desventaja. En cada ejecución de bloque, tras el cálculo inicial de la entrada por el peso, la primera reducción la realizan tan sólo la mitad de los hilos, la otra mitad están parados casi desde el principio, lo que sin duda parece un desperdicio. Además, cuando queden pocos resultados parciales el número de bloques por grid será también pequeño y si el dispositivo tiene muchos multiprocesadores los estaremos también infrautilizando.

La segunda posibilidad a analizar es renunciar a hacer una llamada al kernel por cada neurona de salida y, en vez de ello, permitir que cada bloque se encargue de una neurona de salida, contando el grid con tantos bloques como neuronas de salida tenga la capa de la red que esté siendo procesada. Siempre que el número de multiprocesadores de un dispositivo sea inferior al número de neuronas de salida (algo bastante razonable) no debemos temer el último problema de infrautilización comentado para la primera posibilidad. Además, también se desvanece nuestra preocupación de que los hilos de cada bloque queden demasiado pronto inactivos. Todas las lecturas de entradas y pesos, y los cálculos y acumulaciones correspondientes se dividirán entre todos los hilos del bloque destinado a calcular el valor de esa salida. De esta manera mantendremos todos los hilos de un bloque ocupados hasta la reducción final de los resultados parciales de cada hilo.

Elegimos, por tanto, esta segunda posibilidad, que consiste en realidad en combinar la reducción paralela con la secuencial (en varios hilos). No por ello dejaremos de explorar las posibilidades de optimización para la reducción paralela que sí se hará para reunir todos los resultados parciales de los diferentes hilos de un mismo bloque. Antes de analizar esas posibles optimizaciones, veamos como sería nuestro kernel sin tener en cuenta esas optimizaciones en la reducción (y, como hemos dicho anteriormente, sin contemplar neuronas binarias ni bipolares). Nuestro kernel tomará por entradas el número de neuronas de entrada (=input_size=) de la capa, al vector con las neuronas de entrada (=inputPtr=) y la matriz con los pesos (=weighs=). Como parámetro de salida tendrá el vector con las neuronas de salida listas para que se les aplique la función de activación (=results=).
Este sería el código que implementa el kernel:

#+begin_src c
__global__ void ReductionKernel(float* inputPtr, unsigned input_size, 
                    float* weighs, float* results)
{
    extern __shared__ float sdata[];

    unsigned weighsOffset = (blockIdx.x * input_size);

    float result = 0;
    unsigned i = threadIdx.x;

    while (i < input_size) {
        result += inputPtr[i] * weighs[weighsOffset + i];
        i += blockDim.x;
    }
    __syncthreads();

    unsigned tid = threadIdx.x;
    sdata[tid] = result;
    __syncthreads();

    // Reducción paralela de los resultados parciales en memoria compartida
    // ...

    if (tid == 0) {
        results[blockIdx.x] += sdata[0];
    }
}
#+end_src

Los pesos están ordenados de forma que todos los que correspondan a una misma neurona de salida estén juntos. Por ello, cada bloque calcula un desplazamiento en la matriz de pesos (=weighsOffset=) multiplicando su id de bloque por el tamaño de las entradas para acceder a los pesos que corresponden a la neurona de salida que le toca calcular. Hasta que se acaben las entradas, cada hilo tomará una entrada y la multiplicará por su peso correspondiente, acumulando su resultado parcial en una variable local (=result=) que empezará a cero. El iterador =i= que es diferente para cada hilo del bloque se irá incrementando con el tamaño del bloque, para que no se repitan los accesos a una misma entrada ni un mismo bloque. Tras esto, cada hilo almacena su resultado parcial en la memoria compartida que es donde se realizará la reducción paralela y sólo uno de ellos escribirá el resultado todal desde la memoria compartida a la memoria global del dispositivo, donde se almacenan las salidas de la capa.

Detengamonos ahora a analizar la parte de la reducción paralelizada y sus posibles optimizaciones. Una primera aproximación sería un acceso entrelazado como el del siguiente código:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=1; s < blockDim.x; s *= 2) 
    {
        if (tid % (2*s) == 0) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    // ...
#+end_src

Esta aproximación, sin embargo tiene defectos. En primer lugar, el operador módulo (=%=) es muy lento. Pero eso no es lo peor. Como vimos en la sección \ref{disenoParalCUDAarq}, además de los bloques hay una unidad menor llamada warp compuesta de 32 hilos y si todos los hilos del warp no siguen el mismo flujo de ejecución sufrimos una penalización importante en redimiento. Como los hilos de un mismo warp tienen ids consecutivos, vemos que la condición =if= del interior del bucle producirá warps altamente divergentes, lo que a su vez es altamente ineficiente.
Podemos mantener el mismo acceso entrelazado pero desde hilos consecutivos reemplazando el código anterior con el siguiente, con indexación por saltos:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=1; s < blockDim.x; s *= 2) 
    {
        int index = 2 * s * tid;

        if (index < blockDim.x) 
        {
            sdata[index] += sdata[index + s];
        }
        __syncthreads();
    }
    // ...
}
#+end_src

Esta condición =if= no produce ejecuciones divergentes en un mismo warp tan a menudo, sólo en las fases finales de la reducción (los 32 últimos elementos). Sin embargo, nos encontramos con un nuevo problema y es que los bancos de la memoria copartida no están diseñados para acceder a ellos de esta manera. Si dos hilos acceden de forma conflictiva a posiciones de la memoria compartida que se encuentran en el mismo banco, se produce un conflicto de bancos y se accederá a las dos posiciones secuencialmente, perdiendo las ventajas del acceso paralelo. Una excepción a esto sería que todos los hilos accediesen simultáneamente al mismo elemento, entonces en vez de obtener un conflicto simplemente utilizará una instrucción de broadcast (el número de hilos que tienen que acceder simultaneamente para usar el broadcast dependen de la generación del dispositivo, cuanto más nuevos, más permisivos).

Hay 16 bancos en la memoria compartida (32 en generaciones más nuevas, Fermi), que están entrelazados con una granularidad de 32 bits. Así, las direcciones de memoria estarían situadas en bancos de la siguiente manera:

| Banco       | 0           | 1           | ... | 15              |
|-------------+-------------+-------------+-----+-----------------|
| Direcciones | 0  1  2  3  | 4  5  6  7  | ... | 60  61  62  63  |
| Direcciones | 64 65 66 67 | 68 69 70 71 | ... | 124 125 126 127 |
| ...         | ...         | ...         | ... | ...             |

Floats con posiciones consecutivas en la memoria compartida pertenecen a distintos bancos y si dos hilos del mismo medio-warp (halfwarp) acceden al mismo banco a la vez se producirá el conflicto. En nuestro caso, es el propio acceso entrelazado el que nos está causando los problemas, por lo que nos desharemos de él en la siguiente aproximación con acceso secuencial:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=blockDim.x/2; s>0; s>>=1) 
    {
        if (tid < s) 
        {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    // ...
}
#+end_src

Hemos invertido el bucle y basado la indexación en el id del hilo. Así, además de una condición que no produce divergencias dentro de los warps, tenemos que hilos consucutivos acceden a posiciones de memoria compartida consecutivas. De nuevo, podemos mejorar esta parte del algortimo.

Conforme avanza la reducción, tenemos cada vez menos hilos activos en el bloque y cuando la s es menor o igual a 32, sólo nos queda un warp. Las instrucciones dentro de un warp se calculan de forma síncrona y vectorial (SIMD). Esto significa que cuando ~s <= 32~ no necesitamos llamar a =__syncthreads()=. Tampoco necesitamos hacer la comprobación ~if(tid < s)~, pues no ahorrará ningún cálculo. Por ello podemos desenrollar (loop unrolling) las 6 últimas iteraciones del bucle:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    for(unsigned int s=blockDim.x/2; s>32; s>>=1) 
    {
        if (tid < s)
        {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid < 32)
    {
        sdata[tid] += sdata[tid + 32];
        sdata[tid] += sdata[tid + 16];
        sdata[tid] += sdata[tid +  8];
        sdata[tid] += sdata[tid +  4];
        sdata[tid] += sdata[tid +  2];
        sdata[tid] += sdata[tid +  1];
    }   
    // ...
}
#+end_src

Esto no sólo ahorra trabajo inútil en el último warp sino en todos ellos. Sin desenrollar, todos los warps siguen ejecutando las vueltas del bucle y las condiciones =if=.
NVIDIA dejó de mantener el modo emulado para el compilador nvcc a partir de la versión CUDA v2.3 (esta fué la última que lo incluía). Al tratarse de software privativo nadie pudo continuar con su mantenimiento y este modo ya no es útil en nuevas versiones. Sin embargo, dado que nuestro software está preparado para ser compilado en modo emulación usando la versión mencionada, estimamos oportuno precisar que el emulador no trata los warps de forma idéntica a los dispositivos y para obtener resultados correctos, hay que añadir unas indicaciones para el precompilador:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
#ifndef __DEVICE_EMULATION__
    if (tid < 32)
#endif
    {
        sdata[tid] += sdata[tid + 32]; EMUSYNC;
        sdata[tid] += sdata[tid + 16]; EMUSYNC;
        sdata[tid] += sdata[tid +  8]; EMUSYNC;
        sdata[tid] += sdata[tid +  4]; EMUSYNC;
        sdata[tid] += sdata[tid +  2]; EMUSYNC;
        sdata[tid] += sdata[tid +  1]; EMUSYNC;
    }
    // ...
}
#+end_src

El paso que hemos usado anteriormente consistente en desenrrollar bucles (loop unrolling), no es solamente útil para aprovechar las peculiaridades de la arquitectura CUDA. De forma general, esta técnica se usa para ahorrar la sobrecarga que suponen los cálculos que se ocupan de gestionar el bucle. Podemos obtener un mayor rendimiento desenrollando completamente el bucle. Como contrapartida obtendremos un código compilado más largo, pero centrándonos en el rendimiento no nos importa tener unos binarios más grandes.

Para poder hacer esto necesitaríamos saber el tamaño del bloque en tiempo de compilación y repetir el resto del código para los posibles tamños de bloque. Esta penosa tarea perjudicará además gravemente la legibilidad y mantenibilidad del código si no se cuenta con plantillas, una técnica que implementa el lenguaje C++ pero no el C original. Las primeras versiones de CUDA no soportaban C++, tan sólo C, sin embargo, las plantillas están disponibles desde el principio, también para C CUDA.

Así quedaría la parte de código dedicada a la reducción paralelizada con los bucles completamente desenrrollados usando plantillas:

#+begin_src c
    // ...
    // Reducción paralela de los resultados parciales en memoria compartida
    if (blockSize >= 512) { 
        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads();
    }
    if (blockSize >= 256) { 
        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); 
    }
    if (blockSize >= 128) { 
        if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); 
    }
    
#ifndef __DEVICE_EMULATION__
    if (tid < 32)
#endif
    {
        if (blockSize >=  64) { sdata[tid] += sdata[tid + 32]; EMUSYNC; }
        if (blockSize >=  32) { sdata[tid] += sdata[tid + 16]; EMUSYNC; }
        if (blockSize >=  16) { sdata[tid] += sdata[tid +  8]; EMUSYNC; }
        if (blockSize >=   8) { sdata[tid] += sdata[tid +  4]; EMUSYNC; }
        if (blockSize >=   4) { sdata[tid] += sdata[tid +  2]; EMUSYNC; }
        if (blockSize >=   2) { sdata[tid] += sdata[tid +  1]; EMUSYNC; }
    }
    // ...
}
#+end_src

Además, podemos sustituir la variable local =blockDim.x= por la constante =blockSize= cuando aparezca. El kernel completo final (sólo para neuronas reales) es este:

#+begin_src c
template <unsigned int blockSize>
__global__ void ReductionKernel(float* inputPtr, unsigned input_size, 
                    float* weighs, float* results)
{
    extern __shared__ float sdata[];

    unsigned weighsOffset = (blockIdx.x * input_size);

    float result = 0;
    unsigned i = threadIdx.x;

    while (i < input_size) {
        result += inputPtr[i] * weighs[weighsOffset + i];
        i += blockSize;
    }
    __syncthreads();

    unsigned tid = threadIdx.x;
    sdata[tid] = result;
    __syncthreads();

    // Reducción paralela de los resultados parciales en memoria compartida
    if (blockSize >= 512) { 
        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads();
    }
    if (blockSize >= 256) { 
        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); 
    }
    if (blockSize >= 128) { 
        if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); 
    }
    
#ifndef __DEVICE_EMULATION__
    if (tid < 32)
#endif
    {
        if (blockSize >=  64) { sdata[tid] += sdata[tid + 32]; EMUSYNC; }
        if (blockSize >=  32) { sdata[tid] += sdata[tid + 16]; EMUSYNC; }
        if (blockSize >=  16) { sdata[tid] += sdata[tid +  8]; EMUSYNC; }
        if (blockSize >=   8) { sdata[tid] += sdata[tid +  4]; EMUSYNC; }
        if (blockSize >=   4) { sdata[tid] += sdata[tid +  2]; EMUSYNC; }
        if (blockSize >=   2) { sdata[tid] += sdata[tid +  1]; EMUSYNC; }
    }

    if (tid == 0) {
        results[blockIdx.x] += sdata[0];
    }
}
#+end_src

Para no tener que decidir el tamaño del bloque en tiempo de ejecución y poder mantener =block_size= como parámetro, al llamar al kernel utilizamos una estructura =switch= con los 10 posibles valores:

#+begin_src c
unsigned grid_size = output_size;
unsigned shared_mem_size = block_size * sizeof(float);

switch (block_size) {
    case 512:
        ReductionKernel<512, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 256:
        ReductionKernel<256, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 128:
        ReductionKernel<128, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 64:
        ReductionKernel< 64, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 32:
        ReductionKernel< 32, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 16:
        ReductionKernel< 16, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 8:
        ReductionKernel<  8, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 4:
        ReductionKernel<  4, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 2:
        ReductionKernel<  2, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
    case 1:
        ReductionKernel<  1, BT_FLOAT><<< grid_size, block_size, shared_mem_size >>>
            (inputPtr, input_size, output_size, weighs, results); break;
}
#+end_src

*** Paralelizar las salidas
#+LaTeX: \label{disenoParalCUDAsal}

En la sección \ref{disenoParalCUDAreduc} hemos realizado varias optimizaciones, pero no hemos usado al máximo la memoria compartida cuya lantencia es aproximadamente de 100 a 150 veces menor que la de la memoria global\cite[Farber2008]{Farber2008}. Tan sólo usamos un float por cada hilo del bloque, esto es, como máximo usamos 512 floats ó 512x4 = 2048 Bytes, cuando la memoria cmpartida tiene un tamaño de 16 KB (48KB para dispositivos de capacidad de computación 2.0 o superiores). A esto hay que descontar lo que ocupen los parámetros del kernel, que también se ubicarán en esta memoria, pues todos los hilos de un bloque deben poder acceder a ellos. Por tanto, tiene sentido buscar otra aproximación distinta de la de la reducción con la que se aproveche al máximo este espacio de memoria que sabemos es más rápido.

En esta aproximación, cada hilo calculará de forma completa una neurona de salida, en vez de hacerlo cada bloque como en la versión de reducción. De nuevo, todos los bloques tendrán que leer todas las entradas, pero ahora los hilos de un mismo bloque podrán usar la memoria compartida para almacenar estas entradas que en este caso deberán leer todos los hilos en vez de dividirselas. 
En una primera fase, cargaremos las entradas en la memoria compartida:

#+begin_src c
__global__
void OutputsKernel(float* inputs, unsigned input_size, unsigned output_size, float* weighs,
                                float* results)
{
    extern __shared__ float sdata[];

    unsigned pos = threadIdx.x;
    while (pos < input_size) {

        sdata[pos] = inputs[pos];
        pos += blockDim.x;
    }
    __syncthreads();
    // ...
#+end_src

Una vez tengamos todas las entradas en memoria, cada hilo accederá a los pesos que le correspondan para ir multiplicandolos por las entradas en memoria compartida e ir acumulando los resultados:

#+begin_src c
    // ...
    unsigned outputNeuron = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned weighsOffset = outputNeuron * input_size;
    float result = 0;

    if (outputNeuron < output_size) {

        for (unsigned i = 0; i < input_size; i++) {
            result += sdata[i] * weighs[weighsOffset + i];
            __syncthreads();
        }

        results[outputNeuron] += result;
    }
}
#+end_src

Finalmente, cada hilo almacena el resultado en la la nuerona de salida que le corresponde.
Notesé que en este caso todos los hilos del bloque acceden a la misma posición de memoria compartida a la vez. No esperamos que se presenten de nuevo conflictos de bancos de memoria sino, por el contrario, aprovechar la posibilidad de las lecturas de tipo broadcast. Desafortunadamente, el acceso a la memoria global para leer los pesos no es coalescente lo que se estropean nuestras expectativas. El acceso coalescente permite al dispositivo leer secciones contiguas de memoria con una sóla operación. Para un acceso coalescente, hilos contiguos deben acceder a posiciones de memoria global contiguas. Sin embargo, el salto entre la posición que lee un hilo y el siguiente es igual al tamaño de la capa de entrada, como se aprecia en la asignación =weighsOffset = outputNeuron * input_size=.

Como los pesos no están siendo accedidos de forma coalescente, las lecturas a memoria global serán secuenciales y, por tanto, las de la memoria compartida pueden no suceder al mismo tiempo. Además de estropear el broadcast, las lecturas desde la memoria global serán mucho más lentas. Esta es probablemente la optimización más importante de un kernel y la primera a tener en cuenta, pues aprovechar al máximo el ancho de banda de la memoria es fundamental para obtener un buen rendimiento usando GPGPU\cite{bestPract2009}.

Otro problema de esta aproximación es que el tamaño máximo de la capa de entrada está limitado por el tamaño de la memoria compartida. Como hemos dicho, la memoria compartida tiene una capacidad de 16 KB (aunque pueda ser superior en dispositivos más modernos). Para aceptar entradas de cualquier tamaño tendríamos que modificar el kernel de forma que tomase por parámetro con un índice que indicase el númeo de entrada que toca leer y adaptar los índices de las entradas en consecuencia. Como el algoritmo no podrá ser óptimo debido al acceso no coalescente, no se implementa dicha modificación. La presente aproximación se mantiene y se implementa también para los tipos de neurona binaria y bipolar; simplemente para comparar su rendimiento. Pero en la siguiente sección \ref{disenoParalCUDAinv} se trata de adaptar este algoritmo para conseguir un acceso coalescente a los pesos que sí podrá tomar entradas de cualquier tamaño.

*** Matriz de pesos invertida
#+LaTeX: \label{disenoParalCUDAinv}

En la sección anterior \label{disenoParalCUDAsal} la causa del acceso no coalescente era que los todos pesos de cada neurona de salida estaban juntos. Para acceder al primer peso de dos neuronas de salida consecutivas, que es a lo que quieren acceder dos hilos consecutivos, aplicabamos un salto de =input_size=. Pero si los pesos estuviesen en memoria colocados de la forma en la que serán accedidos, tendríamos un acceso coalescente. Para ello, tan sólo tenemos que invertir la matriz de pesos. Así, los pesos que corresponden a una misma neurona de entrada serán los que están juntos en lugar de los de una misma neurona de salida.

Si estuviesemos considerando un algortimo genérico de multiplicación de un vector por una matriz, tendríamos que contabilizar el coste de invertir la matriz como parte del coste total de ejecución, pues la inversión se realizaría siempre antes de hacer la operación. Pero nuestro caso es diferente. Podemos tener los pesos siempre invertidos en memoria. Tan sólo cuando la red sea almacenada o leída desde disco (por compatibilidad con las otras implementaciones paralelas), deben ser invertidas las matrices su formato normal. Cuando una capa es generada aleatoriamente, ni siquiera es necesaria la inversión. Sin embargo el cáculo del estado de una capa de una red neuronal se ejecutará probablemente varias veces durante la evaluación de un individuo y la evaluación de los individuos se repetirá por muchas generaciones. Por ello, no tenemos en cuenta los costes de la inversión de la matriz de pesos al comparar este algoritmo con otros, pues es bastante irrelevante.

La inversión de la matriz, sin embargo, sí tiene costes en cuanto a desarrollo de software. Las operaciones de mutación, cruza y olvido tienen que tener en cuenta esta colocación especial. El caso más complicado sería la cruza, aunque con el diseño descrito en la sección \ref{disenoGeneCruz}, la inversión se podrá hacer sobre el vector de bits que actua como interfaz para los diferentes esquemas de cruza y las diferentes implementaciones paralelas, cuyas disposiciones de pesos en memoria, como sucede en este caso, pueden variar. 

Resuelto el problema de la inversión desde el diseño y fuera de nuestro código CUDA, veamos los cambios que se han de aplicar sobre el kernel anterior para conseguir el acceso coalescente y poder mejorar su rendimiento:

#+begin_src c
__global__
void InvertedKernel(float* inputs, unsigned input_size, float* weighs, float* results,
                                        unsigned output_size)
{
    extern __shared__ float sdata[];

    unsigned input_pos = threadIdx.x;
    while (input_pos < input_size) {

        sdata[input_pos] = inputs[input_pos];
        input_pos += blockDim.x;
    }
    __syncthreads();

    unsigned output_pos = blockIdx.x * blockDim.x + threadIdx.x;
    float result = 0;

    if (output_pos < output_size) {

        for (unsigned i = 0; i < input_size; i++) {
            result += sdata[i] * weighs[output_pos + (i * output_size)];
            __syncthreads();
        }
        results[output_pos] += result;
    }
}
#+end_src

El kernel resulta incluso más simple. Ahora no hay una variable, =weighsOffset= para el salto entre hilos contiguos, porque el salto es simplemente  =output_pos= que depende directamente del número de bloque y de hilo. Ahora en vez de avanzar una posición en cada vuelta de bucle se avanza output_size, que es lo que se lee en total en cada vuelta.

Quedaba también pendiente por resolver el problema de la limitación del tamaño de las entradas que la memoria compartida nos impone. Para la proximación en la que paralelizabamos las salidas sin invertir la matriz de peso, resolver esto era más complejo, dijimos que teníamos que utilizar un parámetro adicional de indice de entradas y adaptar los índices a los pesos con este nuevo parámetro. Luego llmaríamos al kernel cuantas veces fuese necesario cambiando el valor de ese parámetro. 

Ahora, sin embargo, tenemos los pesos ordenados por entradas. Podemos directamente cambiar los punteros de entradas y pesos en cada llamada y darle como parámetro =input_size= el número de entradas que vaya a procesar en esa llamada en lugar del tamaño total de las entradas. Así, llamaremos a este kernel de la siguiente manera:

#+begin_src c
unsigned grid_size = ((output_size - 1) / block_size) + 1;
unsigned shared_mem_size;

while (input_size > CUDA_MAX_SHARED_FLOATS) {

    shared_mem_size = CUDA_MAX_SHARED_FLOATS * sizeof(float);
    InvertedKernel<<< grid_size, block_size, shared_mem_size >>>(flInputPtr, flWeighs, results, CUDA_MAX_SHARED_FLOATS, output_size);
    flInputPtr += CUDA_MAX_SHARED_FLOATS;
    flWeighs += (CUDA_MAX_SHARED_FLOATS * output_size);
    input_size -= CUDA_MAX_SHARED_FLOATS;
}
shared_mem_size = input_size * sizeof(float);
InvertedKernel<<< grid_size, block_size, shared_mem_size >>>(flInputPtr, flWeighs, results, input_size, output_size);
#+end_src

La constante =CUDA_MAX_SHARED_FLOATS= nos dice las entradas que podremos almacenar en memoria compartida en cada llamada al kernel. Para que funcione en todos los dispositivos, consideraremos un tamaño de memoria compartida de 16 KB ó 16384 Bytes a la que hay que descontar los parámetros del kernel. Hay ya 16 Bytes de memoria compartida que están reservados para almacenar las variables internas blockIdx, blockDim y gridDim (threadIdx se almacena en un registro especial). Además, se pueden usar hasta 256 bytes  como parámetros propios a los kernels. Nuestros parámetros propios son 3 punteros y 2 enteros sin signo. Todos ellos ocupan 4 Bytes, por lo que en nuestro caso debemos reservar 20 bytes más para los parámetros del kernel. En total, tenemos 16384 - 16 - 20 = 16348, por lo que para la versión de neurona real (dividir entre los 4 bytes que ocupa un float) tenemos que el tamaño máximo de la capa  de entrada es 4087.
Para dispositivos de capacidad de computación 2.0 o superiores, se puede adaptar el valor de esta constante y así aprovechar los 48 KB en lugar de usar sólo 16 KB.

TODO pero los accesos coalescentes tiene que ser desde posiciones de memoria alineadas con 64 bytes, así es que 16348-64

Independientemente del valor de la constante, en cada llamada al kernel se incrementa el puntero a las entradas en ese número de posiciones. El puntero de pesos se incrementa más rápido, puesto que por cada entrada que se procesa, se leen los pesos de todas las neuronas de salida correspondientes a esa entrada. Mientras el número de entradas por procesar sea mayor a la constante, se seguirá llamando al kernel, indicándole la constante como el número de entradas que debe procesar. Para la última llamada se indican las entradas que queden y también se adapta el parámetro de memoria compartida para reservar sólo la que se va a usar.

*** Función de activación
#+LaTeX: \label{disenoParalCUDAactiv}

Una vez que hemos obtenido los resultados de las neuronas de salida usando alguno de los algoritmos anteriores para cada una de las múltiples capas que una capa puede tomar como entrada, podemos aplicar la función de activación para completar el trabajo de esa capa de neuronas. El kernel para la activación se implementa con el siguiente código:

#+begin_src c
__global__
void activationKernel(float* results, float* thresholds, float* output, unsigned output_sz,
                             FunctionType functionType)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < output_sz) {
        output[idx] = Func(results[idx] - thresholds[idx], functionType);
    }
}
#+end_src

Como vemos, es una tarea bastante más simple. Cada uno de los hilos representa a una neurona de salida y accede a un resultado y a un umbral, los que le corresponden. Notamos, sin embargo, que se está llamando a una funcion =Func= dentro del kernel. Aunque sea una simple función C, no podemos reutilizar la definición que usan el resto de implementaciones (C++ y SSE2), sino que tenemos que redefinirla dentro de nuestro código CUDA como una función especial que será ejecutada dentro del dispositivo. Para este tipo de funciones se utiliza el especificador de declaración =__device__=. Esta es nuestra función:

#+begin_src c
__device__
float Func(float number, FunctionType functionType)
{
    switch (functionType) {

        case FT_BINARY_STEP:
            if (number > 0) {
                return 1;
            } else {
                return 0;
            }
        case FT_BIPOLAR_STEP:
            if (number > 0) {
                return 1;
            } else {
                return -1;
            }
        case SIGMOID:
            return 1.0f / (1.0f - exp(-number));
        case FT_BIPOLAR_SIGMOID:
            return -1.0f + (2.0f / (1.0f + exp(-number)));
        case FT_HYPERBOLIC_TANGENT:
            return tanh(number);
        case FT_IDENTITY:
        default:
            return number;
    }
}
#+end_src

Como vimos en el apartado del diseño \ref{disenoRedes}, las diferentes funciones de activación soportadas tienen un valor enumerado asociado y se pueden añadir más modificando tan sólo el enumerado y esta función. No obstante es importante recordar que esta función está definida en dos sitios que tendrán que actualizarse: aquí para el código CUDA y en el exterior para las otras implementaciones.

*** MODIFICAR Operador de cruza
#+LaTeX: \label{disenoParalCUDAcruza}

Como se explicó en \ref{disenoGeneCruz} y en \ref{disenoParalCUDAinv}, la implementación del operador genético de cruza es independiente del esquema de cruza y de la disposición de los pesos en memoria de cada implementación paralela. Tan sólo se tiene que recorrer dos matrices de pesos como si fuesen dos vectores y intercambiar el valor de cada peso o no en función de si está activo el bit correspondiente en el vector de bits del mismo tamaño que también se recibirá como parámetro. Por tanto, todas las demás consideraciones han debido aplicarse ya de antemano sobre el vector de bits.

El vector de bits llegará al kernel como un array de enteros sin signo y se deberá acceder a los bits individuales usando máscaras y operaciones lógicas.
Esta sería una primera aproximación:

#+begin_src c
__global__
void crossoverKernel(float* buffer1, float* buffer2, unsigned* bitBuffer, unsigned size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {

        unsigned bit = bitBuffer[ idx / BITS_PER_UNSIGNED ];
        __syncthreads();
        unsigned mask = 0x80000000;
        mask >>= (idx % BITS_PER_UNSIGNED);
        
        if (bit & mask) {
            float aux = buffer1[idx];
            buffer1[idx] = buffer2[idx];
            buffer2[idx] = aux;
        }
    }
}
#+end_src

Es bastante simple, cada hilo coge la parte del bitBuffer que le corresponde y aplica sobre la máscara (que contiene inicialmente =100...00=) los deplazamientos hacia la derecha que correspondan para acceder a su bit. Si haciendo un =AND= lógico entre esa parte del vector de bits y la máscara para seleccionar el suyo se obtiene algún 1, se intercambian en los vactores por la posición del hilo. La constante BITS_PER_UNSIGNED es igual a 32.

Sin embargo, esta forma de acceso al vector de bits es altamente ineficiente. Los 32 hilos contiguos accederán a la misma posición del vector de bits. Esto no sólo implica que no se accederá coalescentemente y no se leerán 16 valores (medio warp, que es cómo se accede a la memoria global en condiciones óptimas) con una sola instrucción de lectura. Además, el acceso a ese mismo elemento por los 32 hilos se hará de forma secuencial. Esta penalización es intolerable.

Si leyesemos primero el vector de bits a memoria compartida quizá podríamos evitar todo esto. Podríamos también evitar los posibles conflictos de bancos de memoria compartida con un broadcast para un warp (justamente 32 hilos, como los bits en una palabra de 4 bytes) del bloque de bits compartido por ese warp. El mínimo número de hilos contiguos que deben acceder a la misma posición de memoria compartida para que se haga un broadcast en lugar de producir un conflicto de bancos de memoria es 16 (un halfwarp), por lo que 32 hilos nos servirán:

#+begin_src c
__global__
void crossoverKernel(float* buffer1, float* buffer2, unsigned* bitBuffer, unsigned size)
{
    extern __shared__ float sdata[];

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned bitBuffer_pos = idx / BITS_PER_UNSIGNED;
    unsigned bitblocks_per_block = blockDim.x / BITS_PER_UNSIGNED;

    if (threadIdx.x < bitblocks_per_block) {
        sdata[threadIdx.x] = bitBuffer[ bitBuffer_pos ];
    }
    __syncthreads();

    if (idx < size) {

        unsigned bit = sdata[ threadIdx.x / BITS_PER_UNSIGNED ];
        __syncthreads();

        unsigned mask = 0x80000000;
        mask >>= ( threadIdx.x % BITS_PER_UNSIGNED );
        
        if (bit & mask) {
            float aux = buffer1[idx];
            buffer1[idx] = buffer2[idx];
            buffer2[idx] = aux;
        }
    }
}
#+end_src

El último bloque de hilos también leerá bitblocks_per_block, por lo que habrá que hacer el array de bits artificialemente más grande para que esas lecturas puedan hacerse, aunque no se vayan a utilizar esos valores.

Hemos, mejorado mucho el acceso al vector de bits. No obstante, el número de lecturas de desde memoria global a memoria compartida es muy pequeño. Sólo la hacen los primeros hilos del bloque. Como mucho, cuando el bloque tenga 512 hilos, se harán 4 lecturas por bloque desde =bitBuffer=. Para hacer una lectura coalescente se necesite que cada 16 hilos del bloque accedan a 16 posiciones consecutivas (y que los medios warps no se también estén seguidos). Pero en este caso algunos medios warps no realizarán lectura alguna y los que los hagan, no leerán 16 valores. La lectura coalescente puede no utilizar los 16 valores leídos, 4 es el mínimo. Con un tamaño de bloque de 512 hilos, se harían 4 lecturas. Entonces siempre se tendrá que llamar al kernel con 512 hilos para hacer lecturas conjuntas. No poder ajustar el número de hilos por bloque es un trastorno importante. Los recursos de un multiprocesador se reparten entre los hilos del bloque. Por ejemplo, cuantos más hilos por bloque, menos registros tendrá disponibles cada hilo. Corremos el riesgo de que algunas variables locales tengan que almacenarse en memoria global en vez de en registros.

TODO ademaś de posiciones contiguas las posiciones de memoria deben estar alineadas
TODO Solución con memoria compartida para bitVector pero varias posiciones de pesos por hilo ??
Cada hilo un bloque de bits
Despues, entre todos los del bloque, pasar por todos los bloques de bits
TODO ¿poner nueva solución como la mejor?

Afortunadamente, existe una solución mejor. Si cada hilo se ocupase de los 32 bits de una posición del array =bitBuffer=, el acceso también podría ser coalescente ya además el valor se podría alamacenar directamente en un registro. Pero entonces dejaría de ser coalescente el acceso a los propios vectores de pesos, pues cada hilo querría acceder a 32 pesos contiguos. A no ser, que el vector de bits viniese ya colocado de la forma que queremos, para que una posición de =bitBuffer= no representase 32 pesos contiguos, sino 32 los 32 pesos a los que su hilo puede acceder de forma coalescente. Debemos aplicar una transformación adicional sobre le vector de bits (si es que ya ha tenido alguno, por ejemplo, por la inversión de las matriz de pesos) para adaptarlo a nuestro nuevo algoritmo antes de llamar al kernel. Podemos aprovechar el momento en que creemos un Buffer CUDA a partir de una Interfaz (ver secciones \ref{disenoRedesInter} y \ref{disenoRedesBuff}) para realizar esta transformación. 

Pero antes de ver cómo realizaríamos la transformación, vemos como sería nuestro nuevo kernel. Cada hilo accederá a BITS_PER_UNSIGNED elementos de los arrays de pesos, cada bloque accederá a blockDim.x * BITS_PER_UNSIGNED. El salto entre las posiciones de un bloque y las del siguiente multiplicará el tamaño del bloque por BITS_PER_UNSIGNED, pero los hilos de un mismo bloque sequirán accediendo a posiciones contiguas:

#+begin_src c
__global__
void crossoverKernel(type* buffer1, type* buffer2, unsigned* bitBuffer, unsigned size)
{
    unsigned weighPos = (blockIdx.x * blockDim.x * BITS_PER_UNSIGNED) + threadIdx.x;
    unsigned maxPosForThisBlock = device_min ( (blockIdx.x + 1) * blockDim.x * BITS_PER_UNSIGNED, size);

    unsigned bitsForTheThread, mask;
    if (weighPos < maxPosForThisBlock) {
        bitsForTheThread = bitBuffer[(blockIdx.x * blockDim.x) + threadIdx.x];
        mask = 0x80000000;
    }
    __syncthreads();

    while (weighPos < maxPosForThisBlock) {
        if (mask & bitsForTheThread) {
            type aux = buffer1[weighPos];
            buffer1[weighPos] = buffer2[weighPos];
            buffer2[weighPos] = aux;
        }
        __syncthreads();
        weighPos += blockDim.x;
        mask >>= 1;
    }
}
#+end_src

Vemos entonces que la posición que los bits deben tomar en el vector para corresponderse con los pesos en este kernel depende del tamaño del bloque con el que se vaya a lanzar el kernel. Además, las partes finales de algunos bloques de bits no serán tenidas en cuenta dependiendo del tamaño del vector y del bloque CUDA, cuando antes sólo se quedaban sin usar las del último bloque de 32 bits. Así es que el array de bits adaptado puede ser más grande que el original.

Este es el constructor especial de la clase CudaBuffer para construir el array de bits adecuado:

#+begin_src c
CudaBuffer(Interface* bitBuffer, unsigned block_size)
{
    if (bitBuffer->getBufferType() != BT_BIT) {
        std::string error = "The Buffer type must be BIT to use a BitBuffer CudaBuffer constructor.";
        throw error;
    }
    unsigned bitBufferSize = bitBuffer->getSize();
    unsigned maxWeighsPerBlock = BITS_PER_UNSIGNED * block_size;

    tSize = (bitBufferSize / maxWeighsPerBlock) * maxWeighsPerBlock;
    tSize += min(bitBufferSize % maxWeighsPerBlock, block_size) * BITS_PER_UNSIGNED;

    Interface interfaceOrderedByBlockSize = Interface(tSize, BT_BIT);

    unsigned bit = 0, thread = 0, block_offset = 0;
    for (unsigned i = 0; i < bitBufferSize; i++) {

        unsigned weighPos = (thread * BITS_PER_UNSIGNED) + bit + block_offset;
        thread++;
        interfaceOrderedByBlockSize.setElement(weighPos, bitBuffer->getElement(i));

        if (thread == block_size) {
            thread = 0;
            bit++;
            if (bit == BITS_PER_UNSIGNED) {
                bit = 0;
                block_offset += (block_size * BITS_PER_UNSIGNED);
            }
        }
    }
    unsigned byteSize = interfaceOrderedByBlockSize.getByteSize();
    data = cuda_malloc(byteSize);
    cuda_copyToDevice(data, interfaceOrderedByBlockSize.getDataPointer(), byteSize);
}
#+end_src

No es particularmente elegante, pues básicamente reproduce lo que tendŕía que hacer el kernel de forma serializada para cambiar las posiciones para el acceso coalescente. Pero sólo será llamado antes de realizar una cruza con dos redes CUDA. Aunque esta transforamción puede parecer una penalización muy importante, no lo es. Tengase en cuenta que esta copia de Interface al Buffer CUDA se tenía que hacer de todas maneras. De no hacer esto, lo único que no ahorraríamos es la conversión de la interfaz inicial que contiene el vector de bits a otra interfaz intermedia con las nuevas posiciones. Pero las 3 últimas líneas las ejecutaríamos igual si hubiesemos optado por la segunda solución y mover los datos de memoria huesped a memoria de dispositivo es muy costoso. El acceso al vector de bits mucho más eficiente durante la ejecución del kernel y la mayor libertad para elegir el número de hilos sin duda justifican esta conversión.

*** Otros operadores genéticos

Otros operadores genéticos como la mutación y el olvido también se lanzando kernels CUDA. Este es el kernel para la mutación:

#+begin_src c
__global__
void mutateKernel(float* buffer, unsigned pos, float mutation)
{
    if (threadIdx.x == 0) {
        buffer[pos] += mutation;
    }
}
#+end_src

Y este el de olvido:

#+begin_src c
__global__
void resetKernel(float* buffer, unsigned pos)
{
    if (threadIdx.x == 0) {
        buffer[pos] = 0;
    }
}
#+end_src

Sólo actuará un hilo por bloque, no parece que se optimice mucho. Sin embargo, no se trata tanto de optimizar estos kernels sino de mantener los pesos en memoria de dispositivo en todo momento. El ancho de banda máximo en la transferencia de entre la memoria de dispositivo y la GPU es mucho mayor (por ejemplo, 141 GBps en la NVIDIA GeForce GTX 280) que el ancho de banda en la transmisión de datos entre la memoria huesped y la del dispositivo (8 GBps para PCI Express ×16 de segunda generación). Por ello, para el mejor rendimiento general de la aplicación, es de una prioridad alta minimizar las transferencias de datos etre el husped (CPU) y el dispositivo (GPU), incluso si eso significa lanzar kernels en la GPU que no suponen ninguna mejora de rendimiento comparados con ejecutar lo mismo en la CPU\cite{bestPract2009}. 

* REVISAR Experimentaci\'on
#+LaTeX: \label{experimentacion}

En esta sección se describen los problemas para los que se entrenarán las redes neuronales y las utilidades implementadas para la experimentación.

** Tareas de clasificaci\'on
#+LaTeX: \label{experimentacionClasif}

Las tareas de clasificación son una aplicación común de las redes neuronales entrenadas con retropropagación del error. También podemos entrenar nuestras redes neuronales para aprender a desempeñar este tipo de tares utilizando algoritmos genéticos. En general, la clasificación consiste en agrupar conjuntos de entradas posibles en clases. Por ejemplo, las entradas {e1, e3, e5} pentenecen a la clase c1; las entradas {e2, e4} pertenecen a la clase c2; las {e6, e7} a la clase c3, etc. Cada entrada sólo puede pertenecer a una clase. La clasificación tiene muchas aplicaciones el como reconocimiento de patrones o la construcción de filtros.

Las tareas de clasificación que se han elegido son simples. Se trata de operaciones lógicas entre dos vectores binarios. Las operaciones escogidas son AND (Y lógico), OR (O lógico) y XOR (O lógico exclusivo). Es sabido que para poder desempeñar la tarea XOR son necesarias redes neuronales de más de una capa, es decir, con capas ocultas. Si bien AND y OR eran tareas que un perceptrón simple (red neuronal de una sola capa) podía aprender, no puede aprender, sin embargo la tarea XOR. Esta última tarea fué la primera para la que se entrenó un perceptrón multicapa utilizando el algortimo de retropropagación del error y se ha convertido en un Benchmark común para algunos algortimos de aprendizaje artificial [TODO referencia bilbiográfica].

Puede parecer poco intuitivo que el cálculo de estas operaciones lógicas constituyan una tarea de clasificación, por lo que pondremos unos ejemplos ilustrativos. A continuación se muestran las clasificaciones para las tareas AND, OR y XOR para vectores de un tamaño de 1 bit. Las entradas, por tanto, son dos vectores de 1 bit (v1 y v2). Como la salida será de un bit, en estos casos sólo existen dos clases (0 ó 1) para cada operación/clasificación.

| v1 | v2 | Clase (OR) | Clase (AND) | Clase (XOR) |
|----+----+------------+-------------+-------------|
|  0 |  0 |          0 |           0 |           0 |
|  1 |  0 |          1 |           0 |           1 |
|  0 |  1 |          1 |           0 |           1 |
|  1 |  1 |          1 |           1 |           0 |

Expresado de otra forma, si llamamos c0 a la clase 0 y c1 a la clase 1, para la clasificación OR la entrada {00} pertenece a c0 y las entradas {10, 01, 11} pertenecen a c1; para AND, {00, 10, 01} pertenecen a c0 y {11} a c1; para XOR {00, 11} pertenecen a c0 y {10, 01} pertenecen a c1.

Como se describió en el capítulo \ref{manualProgrInterf}, para que nuestro sistema pueda aprender una tarea nueva, sólo es necesario implementar una clase que herede de la interzaz Task, en este caso, la clase implementada es BinaryTask. El método más importante es test, que toma un individuo como parámetro, lo prueba y le asigna el fitness resultante. También es importante el método setInputs, con el que se conectan las variables internas de la tarea con las entradas de la red neuronal de un individuo. Por último, getExample devuelve un individuo construido cuya estructura es suficiente para aprender la tarea concreta para la que se quiere entrenar a la población.

La clase BinaryTask es bastante flexible respecto a cómo puede ser inicializada. Hay dos parámetros que son olbigatorios: un enumerado BinaryOperation que indica que tipo de operación será realizada (BO_OR, BO_AND ó BO_XOR) y el tamaño de los vectores de entrada, que es igual al tamaño del de salida.
Existe un tercer parámetro optativo numTests que hace referencia al número de pruebas para evaluar a un individuo. Si no se rellena, se probarán todas las combinaciones posibles entre los dos vectores de entrada; si se rellena, determinará el número de pruebas aleatorias que se realizarán para probar a cada individuo. Para las dos posibilidades, el individuo empieza con una puntuación igual al número de diferencias con las salidas esperadas que podría obtener cómo máximo y se irán restando las diferencias que se vayan encontrando. Así, la puntuación del inndividuo será mejor cuanto menor sea el número de diferencias sigueindo la siguiente fórmula: Fitness = Número máximo de diferencias posibles - número de diferencias obtenidas.
Las pruebas aleatorias consisten simplemente en dar valores aleatorios a los vectores, hacer que la red neuronal obtenga su salida, obtener la salida deseada realizando la operación lógica correspondiente y comparar las diferencias.

No rellenar el número de pruebas y dejar que se evalúen todas las posibilidades nos dará valores de fitness más precisos, pero puede hacer las pruebas muy lentas para tamaños de vectores más grandes.

Para que esta tarea pueda ser realizada por neuronas binarias, bipolares y reales, en lugar de comparar la salida de la neurona directamente con la salida deseada se usarán aproximaciones. Se entenderá que en la salida de las redes neuronales, cualquier valor mayor o igual a 0.5 es equivalente a un 1 y cualquier valor menor que 0.5 (por ejemplo -1 para una neurona bipolar; 0.1 ó -7 para una neurona real) es equivalente a un 0. Esto se podría implementar con una capa adicional, pero se ha preferido por simplicidad hacerlo directamente dentro de la clase BinaryTask.

** Juegos de estrategia abstractos
#+LaTeX: \label{experimentacionJuegos}

En la sección anterior \ref{experimentacionClasif} hemos visto ejemplos de tareas para las que se podían entrenar redes neuronales con el método tradicional de retropropagación del error. En esta sección nos dedicaremos a una tarea para la que no es tan fácil conocer las salidas deseadas. Los juegos de estrategia abstractos[fn:juegEstratAbst] son aquellos juegos de estrategia para los que se trata de minimizar el factor suerte y que carecen de trasfondo o ambientación. Casi todos entran dentro de las categorías de tablero, cartas o piezas (como el dominó). No tienen información oculta ni elementos no determinísticos y frecuentemente se juegan por dos jugadores en turnos alternativos.

Nos hemos centrado en los juegos de tablero y en concreto en el juego conocido como Othello o Reversi. Otros juegos de estrategía abstractos de tablero podrían ser las damas, el tres en raya, el ajedrez, el go, el arimaa, etc. El tres en raya y las damas, por ejemplo, son problemas completamente resueltos matemáticamente y en esos casos sí sería relativamente fácil emparejar todas las posibles entradas con sus salidas deseadas para poder así entrenar a una red neuronal mediante retropropagación, pero no son particularmente interesantes desde el punto de vista del aprendizaje artificial. En otros juegos, los algoritmos de poda alfa-beta con alguna heurística diseñada por expertos y ejecutados en computadores son ampliamente superiores a los jugadores profesionales de los mismos. Es el caso del Reversi y el ajedrez.

En otros juegos, el árbol de posibilidades crece tanto con cada nivel que la ventaja de una mayor lectura en profundidad que disfrutan las máquinas se desvanece y la intuición humana aún es superior al cálculo computacional, por increíble que pueda parecer. El juego del Go (cercado), a pesar de tener un tablero tan simple como el del Reversi y pocas reglas simples de enumerar entra en esta categoría. Es un juego asiático más antiguo que el ajedrez y muy célebre en oriente, en especial en China (weiqui), Korea (baduk) y Japón (igo). Por el momento, la mejor de las máquinas (que no usa poda alfa-beta sino métodos probabilísticos y altamente paralelizables como el algortimo de Monte Carlo\cite[Chaslot2010]{Chaslot2010}) no es capaz de ganar al peor de los jugadores profesionales.

El Arimaa es un juego diseñado recientemente con el objetivo específico de que los algoritmos habituales de búsqueda en profundidad no fuesen efectivos  \cite[Syed03]{Syed03}. Es parecido al ajedrez, con el mismo tablero y piezas, pero sin una configuración inicial preestablecida, con casillas especiales, turnos de dos pasos independientes, movimientos de cambiar de posición una pieza con la del contrario, etc.

Aunque nuestras redes también pueden ser entrenadas para dar una heurística para juegos como el Ajedrez y el Arimaa, hemos preferido optar por los juegos más simples de implementar con piezas de un sólo tipo (el Reversi y el Go), pues esto permitirá reutilizar más código y también puede ser interesante éstudiar el aprendizaje de redes bipolares en este tipo de juegos. Se ha  implementado un tablero que serviría para ambos juegos, pero sólo se ha implementado la tarea Reversi. Para la terea Go, extremadamente interesante, se recomienda utilizar algún jugador ya implementado mediante software libre como puede ser GnuGo o FueGo.

Este tipo de tareas se implementará de forma general haciendo que las redes neuronales actúen como una heurística. Esta heurística puede ser usada como la base de un algoritmo de poda alfa-beta con profundidad configurable o simplemente considerando solamente el conjunto de todos los movimientos legales inmediatos, que es lo que se ha hecho para el caso Reversi. Para evaluar las redes neuronales, en lugar de enfrentarlas entre sí, se ha preferido utilizar un adversario también automático pero no basado en redes neuronales. Así, cada red que quiera ser evaluada se enfrenta a este jugador una o varias veces y se acumulan los resultados para obtener el fitness.

El jugador que se ha implementado para Reversi es extremadamente simple, pero, aún así, es capáz de ganar al jugador humano casual. No utiliza poda  alfa-beta, sino simplemente evalua todos los movimientos legales inmediatos y elige el mejor, igual que lo harán los jugadores neuronales. La diferencia es que el oponente no utiliza una red neuronal como heurística. La heurística del oponente consiste simplemente en contar la puntuación que resultaría si se realizase un movimiento concreto, como si la partida acabase en ese momento. El oponente, por tanto, tiene acceso a la puntuación actual de cada movimiento hipotético y en eso basa su heurística. La red neuronal, sin embargo, no tiene acceso a esas puntuaciones: solamente toma como entrada el tablero resultante de cada movimiento hipotético y debe con eso dar una aproximación de lo bueno que es el movimiento. Para poder ganar a nuestro oponente tendrá que ser capáz, por lo menos, de ser capaz de calcular la puntuación de forma similar a su oponente. No obstante, la red neuronal actúa como una caja negra y no sabemos realmente en qué criterios internos se está basando. Lo que sí se podría hacer es construir un circuito lógico equivalente a la red neuronal para analizarlo y tratar de extraer conclusiones sobre la estrategia aprendida.

La tarea Reversi se implementa en la clase ReversiTask que también hereda de la clase abstracta Task y que utiliza la clase ReversiBoard que implementa las reglas de juego de Reversi y que, a su vez, hereda de la clase que implementa el el tablero genérico para juegos con piezas iguales pero de dos jugadores Board. Éste último siempre es de un tamaño cuadrado (las mismas casillas a lo largo que a lo ancho) pero el tablero de Reversi tiene, además, la restricción de ser como mínimo de un tamaño 4x4. Esto es así por que las cuatro piezas centrales (2x2) empiezan ya rellenas para que los juegadores tengan movimientos legales al inicio.

TODO ¿debo explicar las reglas del juego?
* HACER [0/3] Manual del programador
#+LaTeX: \label{manualProgr}
** HACER Guia de instalación
#+LaTeX: \label{manualProgrInstal}

sudo aptitude install make g++ nasm gnuplot
sudo aptitude install texlive-latex-recommended texlive-latex-extra

** HACER Uso general (API)
#+LaTeX: \label{manualProgrApi}
** HACER Extensibilidad a partir de interfaces
#+LaTeX: \label{manualProgrInterf}
\newpage
* REVISAR [2/3] Resultados: Rendimiento
#+LaTeX: \label{rendimiento}

En esta sección se analizan resultados de rendimiento en tiempo de ejecución.

** REVISAR [1/2] Características técnicas de la máquinas utilizadas
#+LaTeX: \label{rendMaq}

*** MODIFICAR Máquina con capacidad CUDA
#+LaTeX: \label{rendMaqCUDA}

Para las pruebas en las que no se necesitaba capacidad CUDA se ha utilizado otra máquina con peores características técnicas. Esta es la máquina que se ha utilizado para las pruebas del apartado \ref{rendOperadores} y el capítulo \ref{aprendizaje}. Se trata de un ordenador de mesa (desktop) con bastante potencia, con una CPU Phenom II X6 1100T y un dispositvo GPU NVIDIA GTX 570.

Para más detalles técnicos sobre la máquina, a continuación se adjunta la salida de los comandos ejecutados para obtener información sobre la misma.

- CPU

#+begin_src sh
$ cat /proc/cpuinfo
#+end_src

- Dispositivos (GPU)

#+begin_src sh
$ lspci -vv
#+end_src

- Memoria

#+begin_src sh
$ cat /proc/meminfo
#+end_src

TODO
After compilation, go to NVIDIA_GPU_Computing_SDK/C/bin/linux/release in
the user’s home directory and run deviceQuery

*** REVISAR Máquina sin capacidad CUDA
#+LaTeX: \label{rendMaqLaptop}

Para las pruebas en las que no se necesitaba capacidad CUDA se ha utilizado otra máquina con peores características técnicas. Esta es la máquina que se ha utilizado para las pruebas del apartado \ref{rendOperadores} y el capítulo \ref{aprendizaje} (aunque en ese no se miden rendimientos en tiempo sino aprendizaje a lo largo de generaciones). Se trata de un pequeño ordenador portatil (laptop) de la marca Asus con 2 núcleos (core 2 duo) y 4 GB de memoria (de los que parte son utilizados por la GPU Intel, que no cuenta con su propia memoria).

Para más detalles técnicos sobre la máquina, a continuación se adjunta la salida de los comandos ejecutados para obtener información sobre la misma.

- CPU

#+begin_src sh
$ cat /proc/cpuinfo
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 23
model name	: Genuine Intel(R) CPU           U7300  @ 1.30GHz
stepping	: 10
cpu MHz		: 800.000
cache size	: 3072 KB
physical id	: 0
siblings	: 2
core id		: 0
cpu cores	: 2
apicid		: 0
initial apicid	: 0
fdiv_bug	: no
hlt_bug		: no
f00f_bug	: no
coma_bug	: no
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc arch_perfmon pebs bts aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
bogomips	: 2677.65
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 23
model name	: Genuine Intel(R) CPU           U7300  @ 1.30GHz
stepping	: 10
cpu MHz		: 800.000
cache size	: 3072 KB
physical id	: 0
siblings	: 2
core id		: 1
cpu cores	: 2
apicid		: 1
initial apicid	: 1
fdiv_bug	: no
hlt_bug		: no
f00f_bug	: no
coma_bug	: no
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc arch_perfmon pebs bts aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
bogomips	: 2676.61
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

#+end_src

- Dispositivos (GPU)

#+begin_src sh
$ lspci -vv
00:00.0 Host bridge: Intel Corporation Mobile 4 Series Chipset Memory Controller Hub (rev 07)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort+ >SERR- <PERR- INTx-
	Latency: 0
	Capabilities: <access denied>
	Kernel driver in use: agpgart-intel
	Kernel modules: intel-agp

00:02.0 VGA compatible controller: Intel Corporation Mobile 4 Series Chipset Integrated Graphics Controller (rev 07)
	Subsystem: ASUSTeK Computer Inc. Device 1862
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 28
	Region 0: Memory at fe400000 (64-bit, non-prefetchable) [size=4M]
	Region 2: Memory at d0000000 (64-bit, prefetchable) [size=256M]
	Region 4: I/O ports at dc00 [size=8]
	Capabilities: <access denied>
	Kernel driver in use: i915
	Kernel modules: i915

00:02.1 Display controller: Intel Corporation Mobile 4 Series Chipset Integrated Graphics Controller (rev 07)
	Subsystem: ASUSTeK Computer Inc. Device 1862
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Region 0: Memory at fe800000 (64-bit, non-prefetchable) [size=1M]
	Capabilities: <access denied>

00:1a.0 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #4 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 16
	Region 4: I/O ports at d880 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1a.1 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #5 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin B routed to IRQ 21
	Region 4: I/O ports at d800 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1a.2 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #6 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin D routed to IRQ 19
	Region 4: I/O ports at d480 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1a.7 USB Controller: Intel Corporation 82801I (ICH9 Family) USB2 EHCI Controller #2 (rev 03) (prog-if 20)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin C routed to IRQ 18
	Region 0: Memory at fe9fbc00 (32-bit, non-prefetchable) [size=1K]
	Capabilities: <access denied>
	Kernel driver in use: ehci_hcd

00:1b.0 Audio device: Intel Corporation 82801I (ICH9 Family) HD Audio Controller (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1443
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Interrupt: pin A routed to IRQ 22
	Region 0: Memory at fe9f4000 (64-bit, non-prefetchable) [size=16K]
	Capabilities: <access denied>
	Kernel driver in use: HDA Intel
	Kernel modules: snd-hda-intel

00:1c.0 PCI bridge: Intel Corporation 82801I (ICH9 Family) PCI Express Port 1 (rev 03)
	Control: I/O- Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Bus: primary=00, secondary=01, subordinate=01, sec-latency=0
	Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>
	Kernel driver in use: pcieport
	Kernel modules: shpchp

00:1c.1 PCI bridge: Intel Corporation 82801I (ICH9 Family) PCI Express Port 2 (rev 03)
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Bus: primary=00, secondary=02, subordinate=02, sec-latency=0
	Memory behind bridge: fea00000-feafffff
	Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>
	Kernel driver in use: pcieport
	Kernel modules: shpchp

00:1c.5 PCI bridge: Intel Corporation 82801I (ICH9 Family) PCI Express Port 6 (rev 03)
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Bus: primary=00, secondary=03, subordinate=03, sec-latency=0
	I/O behind bridge: 0000e000-0000efff
	Memory behind bridge: feb00000-febfffff
	Prefetchable memory behind bridge: 00000000c0000000-00000000c01fffff
	Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>
	Kernel driver in use: pcieport
	Kernel modules: shpchp

00:1d.0 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #1 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 23
	Region 4: I/O ports at d400 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1d.1 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #2 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin B routed to IRQ 19
	Region 4: I/O ports at d080 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1d.2 USB Controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #3 (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin C routed to IRQ 18
	Region 4: I/O ports at d000 [size=32]
	Capabilities: <access denied>
	Kernel driver in use: uhci_hcd

00:1d.7 USB Controller: Intel Corporation 82801I (ICH9 Family) USB2 EHCI Controller #1 (rev 03) (prog-if 20)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 23
	Region 0: Memory at fe9fb800 (32-bit, non-prefetchable) [size=1K]
	Capabilities: <access denied>
	Kernel driver in use: ehci_hcd

00:1e.0 PCI bridge: Intel Corporation 82801 Mobile PCI Bridge (rev 93) (prog-if 01)
	Control: I/O- Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Bus: primary=00, secondary=04, subordinate=04, sec-latency=32
	Secondary status: 66MHz- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort+ <SERR- <PERR-
	BridgeCtl: Parity- SERR+ NoISA- VGA- MAbort- >Reset- FastB2B-
		PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
	Capabilities: <access denied>

00:1f.0 ISA bridge: Intel Corporation ICH9M-E LPC Interface Controller (rev 03)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Capabilities: <access denied>
	Kernel modules: iTCO_wdt

00:1f.2 SATA controller: Intel Corporation ICH9M/M-E SATA AHCI Controller (rev 03) (prog-if 01)
	Subsystem: ASUSTeK Computer Inc. Device 1c47
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz+ UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin B routed to IRQ 27
	Region 0: I/O ports at cc00 [size=8]
	Region 1: I/O ports at c880 [size=4]
	Region 2: I/O ports at c800 [size=8]
	Region 3: I/O ports at c480 [size=4]
	Region 4: I/O ports at c400 [size=32]
	Region 5: Memory at fe9fb000 (32-bit, non-prefetchable) [size=2K]
	Capabilities: <access denied>
	Kernel driver in use: ahci
	Kernel modules: ahci

02:00.0 Network controller: Atheros Communications Inc. AR9285 Wireless Network Adapter (PCI-Express) (rev 01)
	Subsystem: Device 1a3b:1089
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Interrupt: pin A routed to IRQ 17
	Region 0: Memory at feaf0000 (64-bit, non-prefetchable) [size=64K]
	Capabilities: <access denied>
	Kernel driver in use: ath9k
	Kernel modules: ath9k

03:00.0 Ethernet controller: Atheros Communications Atheros AR8132 / L1c Gigabit Ethernet Adapter (rev c0)
	Subsystem: ASUSTeK Computer Inc. Device 14e5
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0, Cache Line Size: 32 bytes
	Interrupt: pin A routed to IRQ 29
	Region 0: Memory at febc0000 (64-bit, non-prefetchable) [size=256K]
	Region 2: I/O ports at ec00 [size=128]
	Capabilities: <access denied>
	Kernel driver in use: atl1c
	Kernel modules: atl1c

#+end_src

- Memoria

#+begin_src sh
$ cat /proc/meminfo
MemTotal:        3062124 kB
MemFree:          209488 kB
Buffers:          293972 kB
Cached:           950716 kB
SwapCached:         8024 kB
Active:          1741556 kB
Inactive:         935852 kB
Active(anon):    1082592 kB
Inactive(anon):   496576 kB
Active(file):     658964 kB
Inactive(file):   439276 kB
Unevictable:          68 kB
Mlocked:              68 kB
HighTotal:       2201224 kB
HighFree:          95160 kB
LowTotal:         860900 kB
LowFree:          114328 kB
SwapTotal:       1490936 kB
SwapFree:        1436288 kB
Dirty:               136 kB
Writeback:             0 kB
AnonPages:       1426568 kB
Mapped:           116060 kB
Shmem:            146448 kB
Slab:             137540 kB
SReclaimable:     112872 kB
SUnreclaim:        24668 kB
KernelStack:        3136 kB
PageTables:         8592 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     3021996 kB
Committed_AS:    2333124 kB
VmallocTotal:     122880 kB
VmallocUsed:       35100 kB
VmallocChunk:      40420 kB
HardwareCorrupted:     0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       4096 kB
DirectMap4k:       16376 kB
DirectMap4M:      892928 kB
#+end_src

\newpage

** MODIFICAR [0/15] Rendimiento de las implementaciones paralelas
#+LaTeX: \label{rendImpl}

En esta sección se presentan los resultados en tiempo de ejecución de los métodos que se ejecutan de forma diferente con cada implementación. Todas las pruebas de la sección se han realizado con la máquina descrita en la sección \ref{rendMaqCUDA}.

*** MODIFICAR Acumulación de resultados
#+LaTeX: \label{rendImplAcumulEnt}

****  en función de las entradas
En la figura \ref{grafImplCalculateEnt}, puede apreciarse que la implementación con instrucciones SSE2 de ensamblador para utilizar el coprocesador XMM es muy superior a las demás. En ésta los tipos de Buffer BIT y SIGN (neuronas binarias y bipolares) obtienen tiempos similares al tipo FLOAT. 

Las implementaciones CUDA obtienen resultados parecidos, aunque todas ellas obtienen peores tiempos para BIT y SIGN, esto se debe a que los pesos en estos casos son de un sólo byte y los algoritmos desarrollados están optimizados para accesos coalescentes a datos float de 4 bytes (la aruitectura CUDA está en general más preparada para este tipo de datos, aunque se podrían modificar los algoritmos BIT y SIGN para adaptarse mejor a la misma).

Los tiempos de la implementación de referencia C para el tipo FLOAT se encuentran cercanos a los de las implementaciones CUDA para BIT y SIGN, es decir, no mucho peor que las implementaciones CUDA para float. Sin embargo, los peores tiempos de la gráfica son con mucha diferencia los de esta implementación para BIT y SIGN.

Esta prueba se ha hecho usando el tamaño de las entradas como coordenada X, variándolo de 100 en 100 desde 100 hasta 1000. La coordenada Y es el tiempo en segundos. Los pesos y otros valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. El tamaño de la capa de salida es 100 la mitad de las veces y 150 la otra mitad, haciendose la media. Se permutan los tipos de neuronas y los tipos de implementación para obtener cada curva. Cada combinación se repite 5000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método de acumulación de resultados (Connection::calculateAndAdd) variando el tamaño de la capa de entrada.
#+LABEL:      grafImplCalculateEnt
#+ATTR_LaTeX: width=\textwidth
[[./img/Connection_calculateAndAddTo.png]]
\newpage

**** HACER Acumulación de resultados en función de las salidas
#+LaTeX: \label{rendImplAcumulSal}

**** HACER Zoom CUDA con bloques ent
**** HACER Zoom CUDA con bloques sal
*** MODIFICAR Activación
#+LaTeX: \label{rendImplActiv}

De la figura \ref{grafImplActivation} se extraen varias conclusiones. El rendimiento de la activación FLOAT es idéntico para las implementaciones C y SSE2.
Era de esperar pues no se han hecho optimizaciones para la versión SSE2. Para los tipos BIT y SIGN con estas mismas implementaciones se mejora ligeramente el rendimiento. SSE2 tarda un poco más que que la C en BIT y SIGN porque en la implementación SSE2 de BIT y SIGN no se almacenan los bits en el orden normal, si no en la representación especial de SSE2 (ver seción \ref{disenoParalXMMbyte}), para que el método de acumulación de resultados pueda ser óptimo. 

La implemetación CUDA es la más rápida para FLOAT, con un tiempo que se mantiene constante para los tamaños de salida probados. La implementación CUDA de BIT y SIGN produce los peores resultados con bastante diferencia. De nuevo, el algoritmo se adaptó directamente de la versión float sin modificarlo sustancialmente, lo que se requeriría para un rendimiento óptimo.

Esta prueba se ha hecho utilizando la función de activación IDENTITY (ninguna). El tamaño de las salidas se representa en la como coordenada X, variándolo de 100 en 100 desde 100 hasta 1000. La coordenada Y es el tiempo en segundos. Los valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. Se permutan los tipos de neuronas y los tipos de implementación para obtener cada curva. Cada combinación se repite 90000 veces para hacer la media reducir el ruido.

#+CAPTION:    Método que ejecuta la función de activación para las salidas de una capa (Connection::activation).
#+LABEL:      grafImplActivation
#+ATTR_LaTeX: width=\textwidth
[[./img/Connection_activation.png]]
\newpage

**** HACER Activación CUDA

TODO cambiando tamaño de bloques 
FLOAT, BIT y SIGN por separado que si no no se ve nada

\newpage
*** MODIFICAR Funciones de activación

Si en la figura \ref{grafImplActivation} se comparaba el rendimiento de los distintos tipos de neurona (BIT, SIGN y FLOAT con la función de activación IDENTITY), en la figura \ref{grafImplActivationFunc} se comparan los rendimientos de los distintos tipos de funciones para el tipo de neurona FLOAT. Con el tipo de neurona BIT, la función de activación siempre es BINARY_STEP y con el tipo SIGN la activación siempre es BIPOLAR_STEP, por ello podemos excluir estos tipos de neurona de esta gráfica (esas activaciones se mantienen, pero para usando la implementación FLOAT internamente). Además, la implementación SSE2 de la activación para FLOAT es práctiacamente idéntica a la de C y como vimos en la gráfica \ref{grafImplActivation} produce los mismo resultados de rendimiento, por lo que tampoco se muestran los resultados de la activación SSE2 en este caso. Se muestran, entonces, los diferentes funciones de activación para las neuronas FLOAt y para las implementaciones C y CUDA.

Los rendimientos de la implementación CUDA son todos muy buenos y casi constantes, por lo que es difícil compararlos.

Para la implementación C, se puede ver como, en general, la función más lenta es la sigmoide bipolar, seguida muy de cerca por la sigmoide. La tangente hiperbólica sólo es ligeramente más lenta que el resto. Los tiempos de las funciones de activación de salto binario y bipolar son muy parecidos (ligeramente mayor para el caso salto binario) son muy similares al de la función identidad (que no hace nada).

Esta prueba se ha hecho utilizando los parámetros que se describen a continuación. El tamaño de las salidas se representa en la como coordenada X, variándolo de 2000 en 2000 desde 2000 hasta 20000. La coordenada Y es el tiempo en segundos. Los valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. Se permutan los tipos de funciones de activación para FLOAT y los tipos de implementación para para obtener cada curva. Cada combinación se repite 50000 veces para hacer la media reducir el ruido.

#+CAPTION:    Método que ejecuta la función de activación para las salidas de una capa (Connection::activation) con tipo de Buffer FLOAT y las diferentes funciones de activación implementadas.
#+LABEL:      grafImplActivationFunc
#+ATTR_LaTeX: width=\textwidth
[[./img/Activation_functions.png]]
\newpage
**** HACER Funciones de activación CUDA

Simplemente un detalle de la anterior sin meter la implementación C

*** MODIFICAR Cruce

El cruce para BIT y para SIGN es idéntico, los dos se hacen sobre pesos de un byte en lugar de pesos de 4 bytes (float), por lo que se omite el tipo de neurona bipolar (SIGN) en esta prueba. No hay una diferencia sustancial de rendimiento entre los diferentes tipos de neuronas (FLOAT y BIT) para las implementaciones C y SSE2. La cruza de SSE2 se ve ligeramente penalizada, Como en el método de la activación, aunque no se aprecia muy bien en la gráfica \ref{grafImplCrossover}. En este caso se debe al almacenamiento especial de los pesos para SSE2, que usa bloques de pesos en vez de pesos individuales.
La implementación CUDA es algo más lenta. Aunque tampoco hay mucha diferencia entre los tipos de neuronas para esta implementación, las de tipo BIT hacen el cruce más rápido.

Esta prueba se ha hecho usando el tamaño de las entradas como coordenada X, variándolo de 100 en 100 desde 100 hasta 1000. La coordenada Y es el tiempo en segundos. Los pesos y otros valores se inicializaban aleatoriamente y pertenecientes al intervalo [-20, 20]. Los vectores de bits que determinan qué pesos tendrán que cruzarse también se determinan aleatoriamente con 1 ó 0. El tamaño de la capa de salida es 100 la mitad de las veces y 150 la otra mitad, haciendose la media. Se permutan los tipos de neuronas FLOAT y SIGN con los tipos de implementación C, SSE2 y CUDA para obtener cada curva. Cada combinación se repite 5000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método que implementa el operador de crossover para dos conexiones dadas (Connection::crossover).
#+LABEL:      grafImplCrossover
#+ATTR_LaTeX: width=\textwidth
[[./img/Connection_crossover.png]]

TODO fichero Crossover_Impl
Los dos métodos CUDA (en el antiguo, variando el número de bloques).
TODO fichero Crossover_CUDA

\newpage

*** MODIFICAR Mutación

De la figura \ref{grafImplMutation} se puede deducir que el operador de mutación tiene un coste mínimo y constante respecto al tamaño de la conexión (da igual que sea en función de las entradas o las salidas) para todas las implementaciones. La implementación CUDA es más lenta para los dos tipos de neuronas, y los tiempo para estas dos son idénticos. Para CUDA sólo se contempla una implementación, pues todas comparten la misma función de mutación.

Esta prueba se ha hecho usando el tamaño de las entradas como coordenada X, variándolo de 100 en 100 desde 100 hasta 1000. La coordenada Y es el tiempo en segundos. Los pesos y mutaciones se determinan aleatoriamente y con valores pertenecientes al intervalo [-20, 20]. El tamaño de la capa de salida es 100 la mitad de las veces y 150 la otra mitad, haciendose la media. Se permutan los tipos de neuronas FLOAT y BIT con los tipos de implementación C, SSE2 y CUDA para obtener cada curva. Cada combinación se repite 90000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método que implementa el operador genético de mutación (Connection::mutate).
#+LABEL:      grafImplMutation
#+ATTR_LaTeX: width=\textwidth
[[./img/Connection_mutate.png]]
\newpage

*** MODIFICAR Olvido

La gráfica \ref{grafImplReset} se obtiene con unos parámetros iguales a los descritos en la sección \ref{grafImplMutation} y los resultados y conclusiones también coinciden. El operador de olvido tiene un coste constante y mínimo.

#+CAPTION:    Método que implementa el operador genético de olvido (Connection::reset).
#+LABEL:      grafImplReset
#+ATTR_LaTeX: width=\textwidth
[[./img/Connection_reset.png]]
\newpage

*** MODIFICAR Mapeos entre Interface y Buffer

Estos métodos se usan para recibir entradas y sacar salidas al exterior de la red. Las interfaces sirven para independizar la red de la implementación concreta escogida. Cada implementación debe mapear correctamente desde la representación genérica (Interfaz) hacia su propia representación interna (figura \ref{grafImplCopyFrom}) de los datos y viversa (figura \ref{grafImplCopyTo}).

Como se especifica en el apartado \ref{disenoRedes}, los Buffers pueden almacenar los estados de capas de neuronas, pero también los pesos de las conexiones. Por ello, además de los tipos FLOAT, BIT y SIGN se incluye el tipo BYTE, que es el que utilizan los pesos para conexiones con capas de entrada BIT ó SIGN. Sin embargo, los tipos BIT y SIGN comparten métodos por lo que se ignora el SIGN. De nuevo, todas las implementaciones CUDA comparten las mismas funciones y no se contemplan por separado.

Esta prueba se ha hecho usando el tamaño del buffer como coordenada X, variándolo con incrementos de 2000 desde 2000 hasta 20000. La coordenada Y es el tiempo en segundos. Los elementos a ser copiados se inicializan aleatoriamente y con valores pertenecientes al intervalo [-20, 20] (1 ó 0 para los BITS). Se permutan los tipos de neuronas FLOAT y BIT con los tipos de implementación C, SSE2 y CUDA para obtener cada curva. Cada combinación se repite 50000 veces para hacer la media y reducir el ruido.

#+CAPTION:    Método que implementa la copia a un Buffer con una implementación determinada desde un Buffer genérico de la clase Interfaz (Buffer::copyFromInterface).
#+LABEL:      grafImplCopyFrom
#+ATTR_LaTeX: width=\textwidth
[[./img/Buffer_copyFromInterface.png]]

Para el caso en que se copian datos desde la interfaz hacia el Buffer (gráfica \ref{grafImplCopyFrom}), los peores tiempos son con mucha diferencia para el tipo BIT con la implementación SSE2. Igual que en la sección \ref{rendImplActiv}, esto se debe a la colación especial de los bits para este algoritmo.

La implemetación CUDA no es tan lenta como se podría haber esperado. Dentro de esta implementación, el tipo FLOAT es el más lento, después el tipo BYTE y después el BIT. Esto se explica fácilmente, pues los tipo más grandes ocupan también más espacio en memoria y son más datos los que tienen que ser movidos.
Salvo la excepción ya citada de SSE2 para BIT, esto se cumple en general para todas las implementaciones.

Aunque no se aprecia en la gráfica, la versión CUDA para BIT es igual de rápida que las versiones C y SSE2 para FLOAT. Tampoco se observa bien, pero las versiones C y SSE2 para BYTE también obtienen tiempos similares.

#+CAPTION:    Método que implementa la copia de un Buffer con una implementación determinada a un Buffer genérico de la clase Interfaz (Buffer::copyToInterface).
#+LABEL:      grafImplCopyTo
#+ATTR_LaTeX: width=\textwidth
[[./img/Buffer_copyToInterface.png]]

La gráfica \ref{grafImplCopyTo} se ha generado utilizando los mismos parémetros iguales a la ya comentada y también muestra unos resultados similares. La única diferencia notable es que las versiones CUDA son algo más lentas y la versión CUDA para BIT ya no consigue igualar a la versiones C y SSE2 para FLOAT. Mover datos desde la memoria del dispositivo (GPU) hacia la memoria del huesped (CPU) es más costoso que hacerlo en el sentido contrario.

\newpage

** REVISAR Rendimiento de los diferentes operadores genéticos
#+LaTeX: \label{rendOperadores}

Esta sección compara empíricamente el rendimiento computacional de los algoritmos de selección descritos en el apartado \ref{disenoGeneSel} del diseño de los algortimos genéticos.

En esta sección se cronometran los operadores genéticos descritos en el capitulo \ref{disenoGene}. Algunos ya se habían analizado en la sección \ref{rendImpl}, pero a un nivel más bajo, del que dependen las implementaciones. Ahora lo que nos interesa es principalmente el coste de los operadores dependiendo de las diferentes formas de usarlo a más alto nivel. Nos abstraemos por ello de la implementación paralela concreta y usamos la implementación C de referencia para todas estas pruebas. Todas las pruebas de esta sección se realizan usando la máquina descrita en \ref{rendMaqLaptop}.

*** Selección

Esta sección compara empriricamente el rendimiento computacional de los algoritmos de selección descritos en el apartado \ref{disenoGeneSel} del diseño de los algortimos genéticos.

Esta prueba se ha realizado usando el número de individuos a seleccionar como coordenada X, variándolo con incrementos de 50 desde 50 hasta 300. La coordenada Y es el tiempo en segundos. Se permutan dos tamaños totales de población (400 y 500) con los tipos de selección ROULETTE_WHEEL, RANKING, TOURNAMENT y TRUNCATION para obtener las diferentes curvas. Para el tipo de selección TOURNAMENT, se cosideran los tamaños de torneo 5, 15 y 25. Cada combinación se repite 50 veces para hacer la media y reducir el ruido. Para esta prueba se ha creado una tarea (hereda de la clase Task) especial que asigna el fitness a los individuos aleatoriamente y que genera individuos vacíos. Así se minimizan todos los tiempos que no tienen que ver con la selección. Por tanto, a esta prueba no le afecta en absoluto la implementación utilizada.

#+CAPTION:    Rendimiento de los distintos operadores de selección en función del número de individuos a seleccionar. Cada operador se prueba con tamaños totales de población 400 y 500. Además, para la selección por torneo se muestran los resultados con varios tamaños de torneo.
#+LABEL:      rendGenSelect
#+ATTR_LaTeX: width=\textwidth
[[./img/Population_Selection.png]]

La selección por ranking es la más costosa y además la que más depende del tamaño total de la población. 

La selección por torneo es algo menos costosa y depende más del tamaño del torneo que del tamaño total de la población. Aunque cuando se tienen que seleccionar muchos individuos un tamaño de población más grande también penaliza ligeramente a la selcción por torneo.

La selección de ruleta también tiene un coste muy bajo y poco dependiente del número de individuos a seleccionar en comparación con los esquemas ranking y torneo. Aunque tampoco se aprecia en la gráfica, la selección por ruleta sí resulta penalizada con un tamaño de población mayor.

A pesar que que tampoco se aprecia bien en la figura \ref{rendGenSelect}, la selección de truncado tiene un coste mínimo y prácticamente constante tanto del tamaño total de la población como del número de individuos a seleccionar. 

\newpage

*** Cruce

Esta sección compara empíricamente el rendimiento computacional de los algoritmos de cruce descritos en los apartados \ref{disenoGeneCruz} y \ref{disenoGeneNiv} del diseño de los algortimos genéticos.

Esta prueba se ha realizado usando un tamaño como coordenada X, variándolo con incrementos de 50 desde 50 hasta 300. Este tamaño es el tamaño de las capas entradas y el de las internas. La coordenada Y es el tiempo en segundos. Se usan dos capas de entrada a la red (iguales). Los pesos se inicializan aleatoriamente y con valores pertenecientes al intervalo [-20, 20]. El número de capas internas es 1 la mitad de las veces y 2 la otra mitad, haciendose la media. Se usa el tipo de neurona FLOAT la mitad de las veces y BIT la otra mitad, haciéndose la media. Para obtener cada curva, se usan los tipos de cruce UNIFORM, MULTIPOINT y PROPORTIONAL. Es primero se prueba con las probabilidad de cruce de 0.2 y 0.4. El segundo tipo se prueba con 1 y 6 puntos de cruce. Para el algortimo PROPORTIONAL se usa siempre un fitness de 1 para un individuo y 1.5 para el otro. Cada combinación se repite 50 veces para hacer la media y reducir el ruido.

Como muestra la figura \ref{rendGenCruza} el nivel de cruza más costoso es el propio peso, el más barato es el nivel de capa completa y los niveles de neurona tienen rendimientos intermedios y parecidos independientemente de cómo se interprete qué pesos pertenecen a una neurona (si los pesos de entrada a la neurona o los pesos de salida de la misma). 

#+CAPTION:    Rendimiento de los distintos operadores genéticos de cruza en función del tamaño de las capas de neuronas internas.
#+LABEL:      rendGenCruza
#+ATTR_LaTeX: width=\textwidth
[[./img/Individual_crossover.png]]

Como norma general, observamos que el cruce unifrome es siempre más costoso cuanto más se acerque la probabilidad de cruce al 50% y que el cruce multi-punto siempre es más lento cuantos más puntos de corte tenga, lo cuál, por otra parte, no es en modo alguno sorprendente. Sin embargo, el cruce multi-punto no parece muy sensible al número de puntos de corte, aunque afecta más cuanto mayor sea el tamaño de la red-individuo, más afecta el número de cortes al rendimiento. El cruce proporcional siempre es ligeramente más lento que el unifrome con probabilidad 0.4 (aunque con tiempos muy cercanos). La razón es que la probabilidad de cruce para este algoritmo se calcula con la expresión =fitness / (fitness + other_fitness)= y con los parámetros escogidos siempre resulta en ~1 / (1 + 1.5) = 0.4~.

Para el cruce a nivel de peso, el más lento por tener los genes más pequeños y numerosos, los algoritmos más lentos son el uniforme con una probabilidad del 40% y el proporcional, después el uniforme con probabilidad 0.2 y los más rápidos son los muulti-punto.

Con el cruce a nivel de capa, ocurre lo contrario: el uniforme con probabilidad 0.2 se mantiene en el medio, pero ahora los algoritmos más rápidos son el uniforme con una probabilidad del 40% y el proporcional, mientras que los multi-punto son más lentos.

El uniforme con una probabilidad del 20% es igual de rápido para el nivel de neurona independientemente de si se la trata de forma invertida o no. Es el más rápido para este nivel de cruce. Para el resto de esquemas de cruza, el nivel de neurona es más lento que el de neurona invertida.

Para en nivel de neurona sin invertir, el multi-punto con un sólo corte es el más lento y todos los demás, el multi-punto con 6 cortes, el uniforme 40% y el proporcional obtienen tiempos iguales.

Para el nivel de neurona invertida, los algoritmos más lentos son el uniforme con una probabilidad del 40% y el proporcional, después el multi-punto con un sólo corte, después el multi-punto de 6 cortes y el más rápido, como ya se ha dicho, es el uniforme con una probabilidad de 0.2.

\newpage
*** Mutación
#+LaTeX: \label{rendOperadoresMut}

Esta sección compara empíricamente el rendimiento computacional de los modos de mutación descritos en el apartado \ref{disenoGeneMut} del diseño de los algortimos genéticos.

Esta prueba se ha realizado usando un tamaño como coordenada X, variándolo con incrementos de 50 desde 50 hasta 300. Este tamaño es el tamaño de las capas entradas y el de las internas. La coordenada Y es el tiempo en segundos. Se usan dos capas de entrada a la red (iguales). Los pesos se inicializan aleatoriamente y con valores pertenecientes al intervalo [-20, 20]. El número de capas internas es 1 la mitad de las veces y 2 la otra mitad, haciendose la media. Se usa el tipo de neurona FLOAT la mitad de las veces y BIT la otra mitad, haciéndose la media. Para obtener cada curva, se permutan los algortimos PER_INDIVIDUAL y PROBABILISTIC con 4 probabilidades: 0.1, 0.2, 0.3 y 0.4. Para el PROBABILISTIC la probabilidad se toma directamente como parámetro. Para el algoritmo PER_INDIVIDUAL, el número de mutuaciones se calcula multiplicando el número total de genes del individuo por la probabilidad. Así, se espera que los dos algoritmos obtengan un número similar de mutaciones por individuo para la misma probabilidad parámetro.

Según la figura \ref{rendGenMutation}, ambos tipos de mutación son más costosos cuánto mayor es la probabilidad, tal y cómo era de esperar, pues más mutaciones son más escrituras. Además, la mutación probabilística es en general más costosa que la mutación determinística (un número constante de mutaciones por individuo, PER_INDIVIDUAL) salvo cuando la probabilidad parámetro es muy grande (muchas mutaciones por individuo). Aunque la interfaz determinística es más rápida, también se hace más lenta comparativamente cuanto mayor es la probabilidad. La explicación es que la opción probabilística tiene un coste fijo elevado (un número aleatorio por gen) mientras que en la determinística el coste sube con el número de mutaciones (varios números aleatorios por cada mutación a realizar).

#+CAPTION:    Rendimiento de los distintos operadores genéticos de mutación para diferentes probabilidades. Para el operador no probabilístico, se ha calculado el número de mutaciones por individuo multiplicando la probabilidad por el número de genes (pesos y umbrales), para poder compararlo en igualdad con el operador probabilístico en cuanto al número de escrituras en los pesos.
#+LABEL:      rendGenMutation
#+ATTR_LaTeX: width=\textwidth
[[./img/Individual_mutate.png]]
\newpage

*** Olvido
#+LaTeX: \label{rendOperadoresOlv}

Esta sección compara empíricamente el rendimiento computacional de los modos de olvido descritos en el apartado \ref{disenoGeneMut} del diseño de los algortimos genéticos. Esta prueba utiliza unos parámetros semejantes a la anterior (sección \ref{rendOperadoresMut}).
El nuevo operador de olvido/reset presenta otra vez unos resultados similares a los de la mutación y se pueden extraer las mismas conclusiones como muestra la figura \ref{rendGenOlvido}.

#+CAPTION:    Rendimiento de los distintas interfaces del nuevo operador genético de olvido para diferentes probabilidades. Para el operador no probabilístico, se ha calculado el número de mutaciones por individuo multiplicando la probabilidad por el número de genes (pesos y umbrales), para poder compararlo en igualdad con el operador probabilístico en cuanto al número de escrituras en los pesos.
#+LABEL:      rendGenOlvido
#+ATTR_LaTeX: width=\textwidth
[[./img/Individual_reset.png]]
\newpage

* MODIFICAR Resultados: Aprendizaje
#+LaTeX: \label{aprendizaje}

En esta sección se muestran resultados de aprendizaje en términos del mejor fitness de la población en cada generación. Nos abstraemos por ello de la implementación concreta y usamos la SSE2 para todas las pruebas del capítulo. Se elige esa implementación por ser la más rápida usando la máquina descrita en \ref{rendMaqLaptop} y porque no se miden tiempos sino generaciones, por lo que la implementación paralela no va a afectar al resultado.

** Comparación entre las distintas funciones de activación
#+LaTeX: \label{aprendFunc}

A continuación observamos cómo influyen las diferentes funciones de activación para las tareas implementadas.

*** OR

Para la tarea Or, no se observan diferencias sustanciales con diferentes funciones de activación al mirar la figura \ref{aprenFuncOr}.

#+CAPTION:    Aprendizaje de la tarea lógica Or con las distintas funciones de activación.
#+LABEL:      aprenFuncOr
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_OR.png]]

\newpage
*** AND

Según la gráfica \ref{aprenFuncAnd} la función de activación Sigmoide bipolar es ligeramente superior al resto para la tarea lógica AND.

#+CAPTION:    Aprendizaje de la tarea lógica And las distintas funciones de activación.
#+LABEL:      aprenFuncAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_AND.png]]

\newpage
*** XOR

Para la tarea XOR - cuyas redes neuronales tienen 2 capas - la figura \ref{aprenFuncXor} muestra diferencias de en el aprendizaje dependiendo de la función de activación. La función identidad es superior a la mayoría y la de escalón binario inferior. La sigmoide empiza como la mayoría, pero se va haciendo peor según avanza el aprendizaje.

#+CAPTION:    Aprendizaje de la tarea lógica Xor las distintas funciones de activación.
#+LABEL:      aprenFuncXor
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_XOR.png]]

\newpage
*** Reversi

Para la tarea Reversi igual que pasaba con la tarea Or, no se observan diferencias sustanciales con diferentes funciones de activación a pesar de que en este caso también se tienen varias capas en la red neuronal (figura \ref{aprenFuncReversi}).

#+CAPTION:    Aprendizaje de la tarea Reversi las distintas funciones de activación.
#+LABEL:      aprenFuncReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/FunctionTypes_REVERSI.png]]

\newpage

** Comparaci\'on entre redes discretas y lineales
#+LaTeX: \label{aprendDiscretLineales}

A continuación compararemos los posibles efectos beneficiosos o perjudiciales al aprendizaje que se pueden obtener a partir de renunciar a las funciones derivables en favor de neuronas de tipo BIT (salida 0 ó 1) ó SIGN (salida -1 ó 1) y de pesos discretos y más reducidos (1 Byte en lugar de los 4 bytes de un float). También se incluyen las funciones de activación BINARY_STEP y BIPOLAR_STEP para FLOAT para ver el caso en que los pesos siguen siendo igual de grandes y las neuronas se comportan como BIT o SIGN (para poder analizar el efecto de los pesos discretos y menores por separado).

*** OR

Para la tarea Or, la función IDENTITY es la mejor opción con muy poca diferencia (figura \ref{aprenDiscretOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretOr
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_OR.png]]

\newpage
*** AND

Para la tarea And, también con poca diferencia, las funciones bipolares son la opción superior independientemente de si los pesos son almacenados como Floats o cómo Bytes (figura \ref{aprenDiscretAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_AND.png]]

\newpage
*** XOR

De nuevo observamos más diferencias en la tarea Xor (figura \ref{aprenDiscretXor}). Como en AND, la opciones bipolares son superiores durante todo el aprendizaje. Seguidas de la activación linear, que se va haciendo menos efectiva conforme avanza el aprendizaje, cuando es superada por las versiones binarias. Tanto como para las binarias como para las bipolares, se aprecia una diferencia mínima en favor de las versiones cuyos pesos se alamacenan como floats, pero que no compensan las grandes diferencias de rendimiento comentadas en la sección TODO cambiar referencia\ref{rendImplAcumulEnt}.

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretXor
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_XOR.png]]

\newpage
*** Reversi

Para la tarea Reversi todos los tipos de neuronas obtienen resultados bastante semejantes en la gráfica \ref{aprenDiscretReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos tipos de neuronas (Binarias, bipolares y lineales).
#+LABEL:      aprenDiscretReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/BufferTypes_REVERSI.png]]

\newpage
** Comparaci\'on de operadores gen\'eticos
#+LaTeX: \label{aprendOperGen}

Para estudiar el aprendizaje con los distintos operadores genéticos, se han probado cada una de las 4 tareas implementadas y se han generado gráficas de cada una de ellas por separado. Se ha utilizado un tamaño de población de 8 individuos, de los cuales se seleccionan 4 en cada iteración, que se cruzan, se mutan se prueban los nuevos individuos generados para insertarlos ordenadamente en la población anterior, quedando de nuevo 8. De esta manera, se desechan los 4 peores de entre los 8 que había y los 4 nuevos generados. En caso de empate se favorece a los nuevos, siguiendo el criterio de búsqueda neutral (TODO falta referencia bibliográfica aquí). Cada población utiliza un esquema de selección, cruza y mutación concretos, pero para cada gráfica se hace la media de todos los esquemas posibles (salvo para el operador concreto que se esté estudiando en cada gráfica). El operador de olvido sólo se usa al generar sus propias gráficas, haciendo la media con el todas las posibilidades en el resto de operadores.

*** Selección

Como esquemas de selección se utilizan los operadores de truncado (se selecciona directamente a los mejores), ruleta (con valor base=1), ranking (con valores por defecto base=9 step=6) y torneo (con tamaños de torneo 2 y 4).

**** OR

(figura \ref{aprenSelectOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos esquemas de selección.
#+LABEL:      aprenSelectOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_OR.png]]

\newpage
**** AND

(figura \ref{aprenSelectAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos esquemas de selección.
#+LABEL:      aprenSelectAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_AND.png]]

\newpage
**** XOR

(figura \ref{aprenSelectXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos esquemas de selección.
#+LABEL:      aprenSelectXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenSelectReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos esquemas de selección.
#+LABEL:      aprenSelectReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Selection_REVERSI.png]]


\newpage
*** Cruza

**** OR

(figura \ref{aprenCrossOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos esquemas y niveles de cruza.
#+LABEL:      aprenCrossOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_OR.png]]

\newpage
**** AND

(figura \ref{aprenCrossAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos esquemas y niveles de cruza.
#+LABEL:      aprenCrossAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_AND.png]]

\newpage
**** XOR

(figura \ref{aprenCrossXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos esquemas y niveles de cruza.
#+LABEL:      aprenCrossXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenCrossReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos esquemas y niveles de cruza.
#+LABEL:      aprenCrossReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Crossover_REVERSI.png]]

\newpage
*** Mutación

**** OR

(figura \ref{aprenMutOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos configuraciones de mutación.
#+LABEL:      aprenMutOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_OR.png]]

\newpage
**** AND

(figura \ref{aprenMutAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos configuraciones de mutación.
#+LABEL:      aprenMutAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_AND.png]]

\newpage
**** XOR

(figura \ref{aprenMutXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos configuraciones de mutación.
#+LABEL:      aprenMutXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenMutReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos configuraciones de mutación.
#+LABEL:      aprenMutReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Mutations_REVERSI.png]]

\newpage
*** Operador de olvido

**** OR

(figura \ref{aprenResetOr}).

#+CAPTION:    Aprendizaje de la tarea lógica Or con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetOr
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_OR.png]]

\newpage
**** AND

(figura \ref{aprenResetAnd}).

#+CAPTION:    Aprendizaje de la tarea lógica And con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetAnd
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_AND.png]]

\newpage
**** XOR

(figura \ref{aprenResetXor}). 

#+CAPTION:    Aprendizaje de la tarea lógica Xor con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetXor
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_XOR.png]]

\newpage
**** Reversi

 gráfica \ref{aprenResetReversi}.

#+CAPTION:    Aprendizaje de la tarea Reversi con los distintos configuraciones de olvido (reset).
#+LABEL:      aprenResetReversi
#+ATTR_LaTeX: width=\textwidth
[[./img/Reset_REVERSI.png]]

\newpage
** Comparación de distintos tamaños de población número de individuos conservados por generación

\newpage
* HACER Conclusiones
#+LaTeX: \label{conclusiones}

\newpage

#Bibliografía 
\cleardoublepage\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\begin{thebibliography}{9}
 
%\cite[texto]{etiqueta}

%
% No he puesto todas las referencias que tenía porque tenía demasiadas y no tengo muy claro qué tengo que hacer cuando unos se citan a otros. 
% Por ejemplo, he dejado muchas citas que Bertona hacía de otros y no sé si eso es correcto. Yao en una sóla frase referencia a muchísima gente y no sé muy bien qué debería hacer.
%Desde luego, he aprendido que la bibliografía es mejor prepararla sobre la marcha que al toda final.
%

%%%%%%%% Neuro Evolución

\bibitem{Yao99} \textsc{Xin Yao}: \emph{Evolving Artificial Neural Networks}. School of Computer Science, The University of Birmingham (1999).

\bibitem{GomezMiikkulainen2003} \textsc{Faustino J. G\'omez y Risto Miikkulainen}: \emph{Robust Non-Linear Control through Neuroevolution}. Deparment of Computer Science, The University of Texas (2003).

\bibitem{Bertona2005} \textsc{Luis Federico Bertona}: \emph{Entrenamiento de redes neuronales basado en algoritmos evolutivos}. Faculad de ingenier\'ia, Universidad de Buenos Aires (2005).

%%%%%%%% CUDA

%%%%%%%% TODO poner el manual de programación, el de mejores prácticas y la guia de referencia
%%%%%%%% TODO probar \textsl para el contenido del bibitem 

\bibitem{progGuide2009} \textsc{NVIDIA corporation}: \emph{CUDA Programming guide v2.3.1}. NVIDIA Developer Technology, (2009).

\bibitem{Harris2007} \textsc{Mark Harris}: \emph{Optimizing Parallel Reduction in CUDA}. NVIDIA Developer Technology, (2007).

\bibitem{bestPract2009} \textsc{NVIDIA corporation}: \emph{CUDA Best practices guide}. NVIDIA Developer Technology, (2009).

\bibitem{Farber2008} \textsc{Rob Farber}: \emph{CUDA: Supercomputing for the Masses}. Dr Dobbs (2008)

\murl{http}{www.drdobbs.com/parallel/cuda-supercomputing-for-the-masses-part/207200659}.

%%%% Juegos

\bibitem{Chaslot2010} \textsc{Guillaume Maurice Jean-Bernard Chaslot}: \emph{Monte-Carlo Tree Search}. Tésis doctoral, Universiteit Maastricht, Maastricht, Holanda (2010).

\bibitem{Syed03} \textsc{Syed, Omar; Syed, Aamir}: \emph{Arimaa – a New Game Designed to be Difficult for Computers}. International Computer Games Association Journal (2003).

\end{thebibliography}
* Footnotes

[fn:cudaSuperComp]
\newline
http://gpgpu.org/2010/11/17/gpus-in-3-of-5-fastest-supercomputers
\newline
http://blogs.nvidia.com/2011/11/gpu-supercomputers-show-exponential-growth-in-top500-list/

[fn:juegEstratAbst]
\newline
http://en.wikipedia.org/wiki/Abstract_strategy_game
